{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c471c1",
   "metadata": {},
   "source": [
    "Complete Time Series Event Labeling System\n",
    "==========================================\n",
    "\n",
    "Input:  [B, L] time series tensor\n",
    "Output: Rich event annotations at multiple scales\n",
    "\n",
    "Components:\n",
    "1. Multi-scale feature extraction\n",
    "2. Step-wise symbolic labels\n",
    "3. Trend segment detection  \n",
    "4. Peak/trough detection\n",
    "5. Volatility regime detection\n",
    "6. Change point detection\n",
    "7. Motif discovery\n",
    "8. Hierarchical event fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e2234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98c6d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. COMPREHENSIVE VOCABULARY (110+ labels)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary with hierarchical structure\"\"\"\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    UNK = 2\n",
    "    \n",
    "    # Step-level movement (3-12)\n",
    "    FLAT = 3\n",
    "    UP_TINY = 4\n",
    "    UP_SMALL = 5\n",
    "    UP_MEDIUM = 6\n",
    "    UP_LARGE = 7\n",
    "    UP_HUGE = 8\n",
    "    DOWN_TINY = 9\n",
    "    DOWN_SMALL = 10\n",
    "    DOWN_MEDIUM = 11\n",
    "    DOWN_LARGE = 12\n",
    "    DOWN_HUGE = 13\n",
    "    \n",
    "    # Spikes (14-19)\n",
    "    SPIKE_UP_WEAK = 14\n",
    "    SPIKE_UP_STRONG = 15\n",
    "    SPIKE_UP_EXTREME = 16\n",
    "    SPIKE_DOWN_WEAK = 17\n",
    "    SPIKE_DOWN_STRONG = 18\n",
    "    SPIKE_DOWN_EXTREME = 19\n",
    "    \n",
    "    # Trend segments (20-39)\n",
    "    UPTREND_SHORT_WEAK = 20\n",
    "    UPTREND_SHORT_MODERATE = 21\n",
    "    UPTREND_SHORT_STRONG = 22\n",
    "    UPTREND_MEDIUM_WEAK = 23\n",
    "    UPTREND_MEDIUM_MODERATE = 24\n",
    "    UPTREND_MEDIUM_STRONG = 25\n",
    "    UPTREND_LONG_WEAK = 26\n",
    "    UPTREND_LONG_MODERATE = 27\n",
    "    UPTREND_LONG_STRONG = 28\n",
    "    ACCELERATING_UP = 29\n",
    "    DECELERATING_UP = 30\n",
    "    \n",
    "    DOWNTREND_SHORT_WEAK = 31\n",
    "    DOWNTREND_SHORT_MODERATE = 32\n",
    "    DOWNTREND_SHORT_STRONG = 33\n",
    "    DOWNTREND_MEDIUM_WEAK = 34\n",
    "    DOWNTREND_MEDIUM_MODERATE = 35\n",
    "    DOWNTREND_MEDIUM_STRONG = 36\n",
    "    DOWNTREND_LONG_WEAK = 37\n",
    "    DOWNTREND_LONG_MODERATE = 38\n",
    "    DOWNTREND_LONG_STRONG = 39\n",
    "    ACCELERATING_DOWN = 40\n",
    "    DECELERATING_DOWN = 41\n",
    "    \n",
    "    # Flat/stable (42-46)\n",
    "    FLAT_SHORT = 42\n",
    "    FLAT_MEDIUM = 43\n",
    "    FLAT_LONG = 44\n",
    "    PLATEAU = 45\n",
    "    STABLE_REGIME = 46\n",
    "    \n",
    "    # Peaks and troughs (47-62)\n",
    "    LOCAL_PEAK_WEAK = 47\n",
    "    LOCAL_PEAK_MODERATE = 48\n",
    "    LOCAL_PEAK_STRONG = 49\n",
    "    SHARP_PEAK = 50\n",
    "    BROAD_PEAK = 51\n",
    "    DOUBLE_TOP = 52\n",
    "    TRIPLE_TOP = 53\n",
    "    ROUND_TOP = 54\n",
    "    \n",
    "    LOCAL_TROUGH_WEAK = 55\n",
    "    LOCAL_TROUGH_MODERATE = 56\n",
    "    LOCAL_TROUGH_STRONG = 57\n",
    "    SHARP_TROUGH = 58\n",
    "    BROAD_TROUGH = 59\n",
    "    DOUBLE_BOTTOM = 60\n",
    "    TRIPLE_BOTTOM = 61\n",
    "    ROUND_BOTTOM = 62\n",
    "    \n",
    "    # Volatility regimes (63-72)\n",
    "    LOW_VOLATILITY = 63\n",
    "    NORMAL_VOLATILITY = 64\n",
    "    ELEVATED_VOLATILITY = 65\n",
    "    HIGH_VOLATILITY = 66\n",
    "    VOLATILITY_SPIKE = 67\n",
    "    VOLATILITY_CLUSTER = 68\n",
    "    CALM_PERIOD = 69\n",
    "    TURBULENT_PERIOD = 70\n",
    "    VOLATILITY_TRANSITION_UP = 71\n",
    "    VOLATILITY_TRANSITION_DOWN = 72\n",
    "    \n",
    "    # Oscillations (73-80)\n",
    "    OSCILLATION_REGULAR_SMALL = 73\n",
    "    OSCILLATION_REGULAR_MEDIUM = 74\n",
    "    OSCILLATION_REGULAR_LARGE = 75\n",
    "    OSCILLATION_IRREGULAR = 76\n",
    "    HIGH_FREQUENCY_NOISE = 77\n",
    "    CYCLIC_PATTERN = 78\n",
    "    DAMPENED_OSCILLATION = 79\n",
    "    AMPLIFYING_OSCILLATION = 80\n",
    "    \n",
    "    # Change points & regime shifts (81-90)\n",
    "    MEAN_SHIFT_UP = 81\n",
    "    MEAN_SHIFT_DOWN = 82\n",
    "    VARIANCE_INCREASE = 83\n",
    "    VARIANCE_DECREASE = 84\n",
    "    LEVEL_SHIFT_UP = 85\n",
    "    LEVEL_SHIFT_DOWN = 86\n",
    "    STRUCTURAL_BREAK = 87\n",
    "    REGIME_CHANGE = 88\n",
    "    TREND_REVERSAL = 89\n",
    "    SUDDEN_CHANGE = 90\n",
    "    \n",
    "    # Structural patterns (91-105)\n",
    "    RALLY_THEN_DROP = 91\n",
    "    DROP_THEN_RALLY = 92\n",
    "    UP_THEN_SIDEWAYS = 93\n",
    "    DOWN_THEN_SIDEWAYS = 94\n",
    "    STAIRCASE_UP = 95\n",
    "    STAIRCASE_DOWN = 96\n",
    "    V_SHAPE_RECOVERY = 97\n",
    "    INVERTED_V_SHAPE = 98\n",
    "    W_PATTERN = 99\n",
    "    M_PATTERN = 100\n",
    "    CUP_AND_HANDLE = 101\n",
    "    HEAD_AND_SHOULDERS = 102\n",
    "    ROUNDING_BOTTOM = 103\n",
    "    ROUNDING_TOP = 104\n",
    "    CONSOLIDATION = 105\n",
    "    \n",
    "    # Anomalies (106-112)\n",
    "    OUTLIER_HIGH = 106\n",
    "    OUTLIER_LOW = 107\n",
    "    RARE_EVENT = 108\n",
    "    DISCORD = 109\n",
    "    ANOMALOUS_SEGMENT = 110\n",
    "    UNEXPECTED_BEHAVIOR = 111\n",
    "    DATA_QUALITY_ISSUE = 112\n",
    "    \n",
    "    # Macro regimes (113-120)\n",
    "    BULLISH_REGIME = 113\n",
    "    BEARISH_REGIME = 114\n",
    "    SIDEWAYS_REGIME = 115\n",
    "    VOLATILE_REGIME = 116\n",
    "    TRENDING_REGIME = 117\n",
    "    MEAN_REVERTING_REGIME = 118\n",
    "    MOMENTUM_REGIME = 119\n",
    "    TRANSITION_REGIME = 120\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 121\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bad1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. MULTI-SCALE FEATURE EXTRACTOR (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50, 100]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First and second derivatives\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        d2x = torch.diff(dx, dim=1)  # [B, L-2]\n",
    "        features['d2x'] = F.pad(d2x, (2, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            # Prepare for conv1d: [B, 1, L]\n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            \n",
    "            # Rolling mean using conv1d\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w  # [1, 1, w]\n",
    "            \n",
    "            # Pad to maintain length\n",
    "            padding = w - 1\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_mean_3d = F.conv1d(x_padded, kernel)  # [B, 1, L]\n",
    "            rolling_mean = rolling_mean_3d.squeeze(1)  # [B, L]\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std (volatility)\n",
    "            # Compute (x - mean)^2 for each point\n",
    "            x_centered = x_3d - rolling_mean.unsqueeze(1)  # [B, 1, L]\n",
    "            x_centered_padded = F.pad(x_centered, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_var_3d = F.conv1d(x_centered_padded ** 2, kernel)  # [B, 1, L]\n",
    "            rolling_var = rolling_var_3d.squeeze(1)  # [B, L]\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (linear regression in window)\n",
    "            slopes = self._rolling_slope(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "            \n",
    "            # Energy (sum of absolute changes)\n",
    "            dx_abs = torch.abs(features['dx']).unsqueeze(1)  # [B, 1, L]\n",
    "            dx_abs_padded = F.pad(dx_abs, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            energy_3d = F.conv1d(dx_abs_padded, torch.ones(1, 1, w, device=device))  # [B, 1, L]\n",
    "            features[f'energy_{w}'] = energy_3d.squeeze(1)  # [B, L]\n",
    "        \n",
    "        # Z-scores for anomaly detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _rolling_slope(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Compute rolling linear regression slope\"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        slopes = torch.zeros(B, L, device=device)\n",
    "        \n",
    "        # Create time indices for regression\n",
    "        t = torch.arange(window, dtype=x.dtype, device=device)\n",
    "        t_mean = t.mean()\n",
    "        t_var = ((t - t_mean) ** 2).sum()\n",
    "        \n",
    "        # Use rolling window\n",
    "        for i in range(L):\n",
    "            start = max(0, i - window + 1)\n",
    "            end = i + 1\n",
    "            actual_window = end - start\n",
    "            \n",
    "            if actual_window < 3:  # Need minimum points for regression\n",
    "                continue\n",
    "            \n",
    "            # Get window data\n",
    "            window_data = x[:, start:end]  # [B, actual_window]\n",
    "            \n",
    "            # Time indices for this window\n",
    "            t_win = torch.arange(actual_window, dtype=x.dtype, device=device)\n",
    "            t_win_mean = t_win.mean()\n",
    "            t_win_var = ((t_win - t_win_mean) ** 2).sum()\n",
    "            \n",
    "            # Compute slope\n",
    "            x_mean = window_data.mean(dim=1, keepdim=True)\n",
    "            cov = ((t_win - t_win_mean) * (window_data - x_mean)).sum(dim=1)\n",
    "            slopes[:, i] = cov / (t_win_var + 1e-8)\n",
    "        \n",
    "        return slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc465bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. STEP-WISE SYMBOLIC ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep with symbolic labels\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, L]\n",
    "        features: dict from MultiScaleFeatureExtractor\n",
    "        Returns: [B, L] step labels\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Compute global quantiles for thresholding\n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padding\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles: 20%, 40%, 60%, 80%, 95%\n",
    "        quantiles = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.20, 0.40, 0.60, 0.80, 0.95], device=device)\n",
    "        )\n",
    "        q20, q40, q60, q80, q95 = quantiles\n",
    "        \n",
    "        epsilon = 0.1 * q20  # Flat threshold\n",
    "        \n",
    "        # Initialize with PAD\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Process t >= 1\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q20), t] = VOCAB.UP_TINY\n",
    "            labels[up_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.UP_HUGE\n",
    "            \n",
    "            # Extreme spikes\n",
    "            labels[up_mask & (abs_diff > q95), t] = VOCAB.SPIKE_UP_EXTREME\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q20), t] = VOCAB.DOWN_TINY\n",
    "            labels[down_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.DOWN_HUGE\n",
    "            labels[down_mask & (abs_diff > q95), t] = VOCAB.SPIKE_DOWN_EXTREME\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee0488be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. TREND SEGMENT DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrendSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    direction: str  # 'up', 'down', 'flat'\n",
    "    slope: float\n",
    "    strength: str  # 'weak', 'moderate', 'strong'\n",
    "    label: int\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"Detect trend segments using piecewise linear approximation\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[TrendSegment]:\n",
    "        \"\"\"\n",
    "        Detect trends for a single sequence\n",
    "        x: [L] single sequence\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        # Use rolling slope features\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            # Fallback: compute simple slopes\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find runs of same sign\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        # Detect sign changes\n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 3:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            # Segment statistics\n",
    "            seg_slope = slopes[start:end].mean()\n",
    "            seg_std = slopes[start:end].std()\n",
    "            duration = end - start\n",
    "            \n",
    "            # Classify direction\n",
    "            if abs(seg_slope) < 0.01:\n",
    "                direction = 'flat'\n",
    "                label = self._classify_flat(duration)\n",
    "            elif seg_slope > 0:\n",
    "                direction = 'up'\n",
    "                strength, accel = self._classify_strength(seg_slope, slopes[start:end])\n",
    "                label = self._classify_uptrend(duration, strength, accel)\n",
    "            else:\n",
    "                direction = 'down'\n",
    "                strength, accel = self._classify_strength(abs(seg_slope), slopes[start:end])\n",
    "                label = self._classify_downtrend(duration, strength, accel)\n",
    "            \n",
    "            segments.append(TrendSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                direction=direction,\n",
    "                slope=seg_slope,\n",
    "                strength=strength if direction != 'flat' else 'n/a',\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _classify_strength(self, avg_slope: float, slopes: np.ndarray):\n",
    "        \"\"\"Classify trend strength and acceleration\"\"\"\n",
    "        if avg_slope < 0.05:\n",
    "            strength = 'weak'\n",
    "        elif avg_slope < 0.15:\n",
    "            strength = 'moderate'\n",
    "        else:\n",
    "            strength = 'strong'\n",
    "        \n",
    "        # Check acceleration\n",
    "        mid = len(slopes) // 2\n",
    "        slope_first_half = slopes[:mid].mean()\n",
    "        slope_second_half = slopes[mid:].mean()\n",
    "        \n",
    "        if abs(slope_second_half) > abs(slope_first_half) * 1.2:\n",
    "            accel = 'accelerating'\n",
    "        elif abs(slope_second_half) < abs(slope_first_half) * 0.8:\n",
    "            accel = 'decelerating'\n",
    "        else:\n",
    "            accel = 'steady'\n",
    "        \n",
    "        return strength, accel\n",
    "    \n",
    "    def _classify_uptrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for uptrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_UP\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_UP\n",
    "        \n",
    "        # Duration classification\n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        # Strength\n",
    "        str_type = strength.upper()\n",
    "        \n",
    "        # Map to vocabulary\n",
    "        label_name = f\"UPTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.UPTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_downtrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for downtrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_DOWN\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_DOWN\n",
    "        \n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        str_type = strength.upper()\n",
    "        label_name = f\"DOWNTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.DOWNTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_flat(self, duration: int) -> int:\n",
    "        \"\"\"Get label for flat segment\"\"\"\n",
    "        if duration < 15:\n",
    "            return VOCAB.FLAT_SHORT\n",
    "        elif duration < 40:\n",
    "            return VOCAB.FLAT_MEDIUM\n",
    "        else:\n",
    "            return VOCAB.FLAT_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bdd1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. PEAK/TROUGH DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakTroughEvent:\n",
    "    index: int\n",
    "    type: str  # 'peak' or 'trough'\n",
    "    prominence: float\n",
    "    label: int\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"Detect peaks and troughs using scipy\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[PeakTroughEvent]:\n",
    "        \"\"\"Detect peaks/troughs in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Detect peaks\n",
    "        peaks, properties = scipy_signal.find_peaks(\n",
    "            x_np, \n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(peaks, properties['prominences']):\n",
    "            label = self._classify_peak(prom, properties['widths'][list(peaks).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='peak',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Detect troughs (peaks of inverted signal)\n",
    "        troughs, properties = scipy_signal.find_peaks(\n",
    "            -x_np,\n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(troughs, properties['prominences']):\n",
    "            label = self._classify_trough(prom, properties['widths'][list(troughs).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='trough',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Sort by index\n",
    "        events.sort(key=lambda e: e.index)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _classify_peak(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify peak type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_PEAK_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_PEAK\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_PEAK\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_TOP\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_STRONG\n",
    "    \n",
    "    def _classify_trough(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify trough type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_TROUGH_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_TROUGH\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_TROUGH\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_BOTTOM\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_STRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9b4203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. VOLATILITY REGIME DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class VolatilityRegime:\n",
    "    start: int\n",
    "    end: int\n",
    "    level: str  # 'low', 'normal', 'elevated', 'high'\n",
    "    avg_vol: float\n",
    "    label: int\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"Detect volatility regimes\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[VolatilityRegime]:\n",
    "        \"\"\"Detect volatility regimes in single sequence\"\"\"\n",
    "        \n",
    "        # Use rolling std\n",
    "        if 'std_20' in features:\n",
    "            vol = features['std_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantiles\n",
    "        q25, q50, q75, q90 = np.percentile(vol, [25, 50, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q50)] = 1  # normal\n",
    "        vol_levels[(vol > q50) & (vol <= q75)] = 2  # elevated\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 3  # high\n",
    "        vol_levels[vol > q90] = 4  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end].mean()\n",
    "            \n",
    "            # Map to label\n",
    "            if level_code == 0:\n",
    "                label = VOCAB.LOW_VOLATILITY\n",
    "                level = 'low'\n",
    "            elif level_code == 1:\n",
    "                label = VOCAB.NORMAL_VOLATILITY\n",
    "                level = 'normal'\n",
    "            elif level_code == 2:\n",
    "                label = VOCAB.ELEVATED_VOLATILITY\n",
    "                level = 'elevated'\n",
    "            elif level_code == 3:\n",
    "                label = VOCAB.HIGH_VOLATILITY\n",
    "                level = 'high'\n",
    "            else:\n",
    "                label = VOCAB.VOLATILITY_SPIKE\n",
    "                level = 'spike'\n",
    "            \n",
    "            regimes.append(VolatilityRegime(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                level=level,\n",
    "                avg_vol=float(avg_vol),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return regimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad6b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. CHANGE POINT DETECTOR (CUSUM-based)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ChangePoint:\n",
    "    index: int\n",
    "    type: str  # 'mean_shift_up', 'mean_shift_down', etc.\n",
    "    magnitude: float\n",
    "    label: int\n",
    "\n",
    "class ChangePointDetector:\n",
    "    \"\"\"Detect change points using CUSUM\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=5.0, drift=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.drift = drift\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[ChangePoint]:\n",
    "        \"\"\"Detect change points in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "        \n",
    "        # CUSUM for mean shifts\n",
    "        cusum_pos = np.zeros(L)\n",
    "        cusum_neg = np.zeros(L)\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            cusum_pos[i] = max(0, cusum_pos[i-1] + x_norm[i] - self.drift)\n",
    "            cusum_neg[i] = min(0, cusum_neg[i-1] + x_norm[i] + self.drift)\n",
    "        \n",
    "        # Find threshold crossings\n",
    "        change_points = []\n",
    "        \n",
    "        # Upward shifts\n",
    "        up_crossings = np.where(cusum_pos > self.threshold)[0]\n",
    "        if len(up_crossings) > 0:\n",
    "            # Group nearby crossings\n",
    "            groups = self._group_nearby(up_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = cusum_pos[idx]\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_up',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_UP\n",
    "                ))\n",
    "        \n",
    "        # Downward shifts\n",
    "        down_crossings = np.where(cusum_neg < -self.threshold)[0]\n",
    "        if len(down_crossings) > 0:\n",
    "            groups = self._group_nearby(down_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = abs(cusum_neg[idx])\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_down',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_DOWN\n",
    "                ))\n",
    "        \n",
    "        # Sort by index\n",
    "        change_points.sort(key=lambda cp: cp.index)\n",
    "        \n",
    "        return change_points\n",
    "    \n",
    "    def _group_nearby(self, indices: np.ndarray, gap: int = 10) -> List[List[int]]:\n",
    "        \"\"\"Group indices that are close together\"\"\"\n",
    "        if len(indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        groups = []\n",
    "        current_group = [indices[0]]\n",
    "        \n",
    "        for idx in indices[1:]:\n",
    "            if idx - current_group[-1] <= gap:\n",
    "                current_group.append(idx)\n",
    "            else:\n",
    "                groups.append(current_group)\n",
    "                current_group = [idx]\n",
    "        \n",
    "        groups.append(current_group)\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8b4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. COMPREHENSIVE EVENT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ComprehensiveAnnotation:\n",
    "    \"\"\"Complete annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    trend_segments: List[TrendSegment]\n",
    "    peaks_troughs: List[PeakTroughEvent]\n",
    "    volatility_regimes: List[VolatilityRegime]\n",
    "    change_points: List[ChangePoint]\n",
    "    \n",
    "    def to_event_sequence(self, max_events: int = 100) -> List[Dict]:\n",
    "        \"\"\"Convert to flat event list sorted by start index\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        # Add trends\n",
    "        for seg in self.trend_segments:\n",
    "            events.append({\n",
    "                'start': seg.start,\n",
    "                'end': seg.end,\n",
    "                'type': 'trend',\n",
    "                'label': seg.label,\n",
    "                'label_name': VOCAB.id_to_label(seg.label),\n",
    "                'metadata': {\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pt in self.peaks_troughs:\n",
    "            events.append({\n",
    "                'start': pt.index,\n",
    "                'end': pt.index,\n",
    "                'type': 'peak_trough',\n",
    "                'label': pt.label,\n",
    "                'label_name': VOCAB.id_to_label(pt.label),\n",
    "                'metadata': {\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in self.volatility_regimes:\n",
    "            events.append({\n",
    "                'start': vr.start,\n",
    "                'end': vr.end,\n",
    "                'type': 'volatility',\n",
    "                'label': vr.label,\n",
    "                'label_name': VOCAB.id_to_label(vr.label),\n",
    "                'metadata': {\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add change points\n",
    "        for cp in self.change_points:\n",
    "            events.append({\n",
    "                'start': cp.index,\n",
    "                'end': cp.index,\n",
    "                'type': 'change_point',\n",
    "                'label': cp.label,\n",
    "                'label_name': VOCAB.id_to_label(cp.label),\n",
    "                'metadata': {\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort by start index\n",
    "        events.sort(key=lambda e: e['start'])\n",
    "        \n",
    "        # Limit to max_events\n",
    "        return events[:max_events]\n",
    "    \n",
    "    def to_text_description(self) -> str:\n",
    "        \"\"\"Generate natural language description\"\"\"\n",
    "        events = self.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        parts.append(f\"Sequence length: {len(self.sequence)}\")\n",
    "        parts.append(f\"Total events detected: {len(events)}\")\n",
    "        \n",
    "        for event in events:\n",
    "            if event['start'] == event['end']:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "            else:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}-{event['end']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "\n",
    "class ComprehensiveEventDataset(Dataset):\n",
    "    \"\"\"Complete dataset with all event types\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize detectors\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Process each sequence for higher-level events\n",
    "        if verbose:\n",
    "            print(\"Detecting trends, peaks, volatility, change points...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            vol_regimes = self.vol_detector.detect(x[i], self.features, i)\n",
    "            change_pts = self.cp_detector.detect(x[i], i)\n",
    "            \n",
    "            annotation = ComprehensiveAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                trend_segments=trends,\n",
    "                peaks_troughs=peaks,\n",
    "                volatility_regimes=vol_regimes,\n",
    "                change_points=change_pts\n",
    "            )\n",
    "            \n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Dataset ready with {len(self.annotations)} annotated sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return comprehensive annotation\"\"\"\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Compute dataset statistics\"\"\"\n",
    "        total_trends = sum(len(ann.trend_segments) for ann in self.annotations)\n",
    "        total_peaks = sum(len(ann.peaks_troughs) for ann in self.annotations)\n",
    "        total_vol = sum(len(ann.volatility_regimes) for ann in self.annotations)\n",
    "        total_cp = sum(len(ann.change_points) for ann in self.annotations)\n",
    "        \n",
    "        avg_trends = total_trends / len(self.annotations)\n",
    "        avg_peaks = total_peaks / len(self.annotations)\n",
    "        avg_vol = total_vol / len(self.annotations)\n",
    "        avg_cp = total_cp / len(self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'avg_trend_segments': avg_trends,\n",
    "            'avg_peaks_troughs': avg_peaks,\n",
    "            'avg_volatility_regimes': avg_vol,\n",
    "            'avg_change_points': avg_cp,\n",
    "            'avg_total_events': avg_trends + avg_peaks + avg_vol + avg_cp,\n",
    "            'vocab_size': VOCAB.get_vocab_size()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6048cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. TEXT CORPUS GENERATOR FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"Generate text corpus for language model training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_structured_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Structured format (token-efficient)\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            if e['start'] == e['end']:\n",
    "                tokens.append(f\"[{e['start']}]{e['label_name']}\")\n",
    "            else:\n",
    "                tokens.append(f\"[{e['start']}-{e['end']}]{e['label_name']}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_hybrid_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Hybrid: structured + metadata\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        for e in events:\n",
    "            span = f\"[{e['start']}-{e['end']}]\" if e['start'] != e['end'] else f\"[{e['start']}]\"\n",
    "            metadata_str = \" \".join(f\"{k}={v}\" for k, v in e['metadata'].items())\n",
    "            parts.append(f\"{span} {e['label_name']} ({metadata_str})\")\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Natural language narrative\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        L = len(annotation.sequence)\n",
    "        \n",
    "        sentences = [f\"Time series of length {L}.\"]\n",
    "        \n",
    "        # Group by type\n",
    "        trends = [e for e in events if e['type'] == 'trend']\n",
    "        peaks = [e for e in events if e['type'] == 'peak_trough']\n",
    "        \n",
    "        if trends:\n",
    "            sentences.append(f\"Contains {len(trends)} trend segments:\")\n",
    "            for t in trends[:3]:  # Limit verbosity\n",
    "                sentences.append(\n",
    "                    f\"  From {t['start']} to {t['end']}, \"\n",
    "                    f\"{t['label_name'].lower().replace('_', ' ')} observed.\"\n",
    "                )\n",
    "        \n",
    "        if peaks:\n",
    "            sentences.append(f\"Notable peaks/troughs: {len(peaks)} detected.\")\n",
    "        \n",
    "        return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96750473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE TIME SERIES EVENT LABELING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Generating 100 synthetic sequences of length 336...\n",
      "Processing 100 sequences of length 336...\n",
      "Extracting multi-scale features...\n",
      "Encoding step-wise labels...\n",
      "Detecting trends, peaks, volatility, change points...\n",
      "  Processing sequence 0/100...\n",
      "✓ Dataset ready with 100 annotated sequences\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "num_sequences........................... 100.00\n",
      "sequence_length......................... 336.00\n",
      "avg_trend_segments...................... 35.34\n",
      "avg_peaks_troughs....................... 54.26\n",
      "avg_volatility_regimes.................. 27.10\n",
      "avg_change_points....................... 4.17\n",
      "avg_total_events........................ 120.87\n",
      "vocab_size.............................. 121.00\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE ANNOTATION (First Sequence)\n",
      "================================================================================\n",
      "\n",
      "Step labels (first 20): [0, 7, 11, 19, 16, 13, 7, 13, 7, 8, 13, 12, 7, 5, 9, 8, 7, 13, 10, 7]\n",
      "  -> ['PAD', 'UP_LARGE', 'DOWN_MEDIUM', 'SPIKE_DOWN_EXTREME', 'SPIKE_UP_EXTREME', 'DOWN_HUGE', 'UP_LARGE', 'DOWN_HUGE', 'UP_LARGE', 'UP_HUGE', 'DOWN_HUGE', 'DOWN_LARGE', 'UP_LARGE', 'UP_SMALL', 'DOWN_TINY', 'UP_HUGE', 'UP_LARGE', 'DOWN_HUGE', 'DOWN_SMALL', 'UP_LARGE']\n",
      "\n",
      "Trend segments detected: 41\n",
      "  1. [3-15] DECELERATING_DOWN (slope=-0.065)\n",
      "  2. [19-33] UPTREND_SHORT_WEAK (slope=0.031)\n",
      "  3. [33-40] FLAT_SHORT (slope=0.002)\n",
      "  4. [40-43] DECELERATING_DOWN (slope=-0.013)\n",
      "  5. [43-51] FLAT_SHORT (slope=0.004)\n",
      "\n",
      "Peaks/Troughs detected: 51\n",
      "  1. [11] SHARP_TROUGH (prominence=0.890)\n",
      "  2. [23] LOCAL_PEAK_MODERATE (prominence=0.577)\n",
      "  3. [28] LOCAL_TROUGH_WEAK (prominence=0.144)\n",
      "  4. [32] LOCAL_TROUGH_WEAK (prominence=0.491)\n",
      "  5. [40] LOCAL_TROUGH_WEAK (prominence=0.273)\n",
      "\n",
      "Volatility regimes: 27\n",
      "  1. [7-13] ELEVATED_VOLATILITY (avg_vol=0.263)\n",
      "  2. [13-19] HIGH_VOLATILITY (avg_vol=0.305)\n",
      "  3. [19-31] VOLATILITY_SPIKE (avg_vol=0.349)\n",
      "  4. [31-41] HIGH_VOLATILITY (avg_vol=0.307)\n",
      "  5. [44-61] LOW_VOLATILITY (avg_vol=0.126)\n",
      "  6. [63-70] ELEVATED_VOLATILITY (avg_vol=0.269)\n",
      "  7. [70-83] HIGH_VOLATILITY (avg_vol=0.302)\n",
      "  8. [83-103] VOLATILITY_SPIKE (avg_vol=0.846)\n",
      "  9. [103-108] ELEVATED_VOLATILITY (avg_vol=0.262)\n",
      "  10. [108-120] NORMAL_VOLATILITY (avg_vol=0.209)\n",
      "  11. [120-128] LOW_VOLATILITY (avg_vol=0.149)\n",
      "  12. [133-142] ELEVATED_VOLATILITY (avg_vol=0.255)\n",
      "  13. [144-150] ELEVATED_VOLATILITY (avg_vol=0.269)\n",
      "  14. [150-158] NORMAL_VOLATILITY (avg_vol=0.201)\n",
      "  15. [158-169] LOW_VOLATILITY (avg_vol=0.131)\n",
      "  16. [169-175] NORMAL_VOLATILITY (avg_vol=0.209)\n",
      "  17. [175-181] ELEVATED_VOLATILITY (avg_vol=0.256)\n",
      "  18. [181-186] HIGH_VOLATILITY (avg_vol=0.302)\n",
      "  19. [188-201] HIGH_VOLATILITY (avg_vol=0.310)\n",
      "  20. [205-212] NORMAL_VOLATILITY (avg_vol=0.194)\n",
      "  21. [212-227] LOW_VOLATILITY (avg_vol=0.126)\n",
      "  22. [234-254] ELEVATED_VOLATILITY (avg_vol=0.260)\n",
      "  23. [254-267] NORMAL_VOLATILITY (avg_vol=0.194)\n",
      "  24. [267-281] LOW_VOLATILITY (avg_vol=0.115)\n",
      "  25. [288-299] ELEVATED_VOLATILITY (avg_vol=0.255)\n",
      "  26. [308-319] NORMAL_VOLATILITY (avg_vol=0.212)\n",
      "  27. [322-335] LOW_VOLATILITY (avg_vol=0.151)\n",
      "\n",
      "Change points: 5\n",
      "  1. [5] MEAN_SHIFT_DOWN (magnitude=5.578)\n",
      "  2. [83] MEAN_SHIFT_DOWN (magnitude=9.145)\n",
      "  3. [125] MEAN_SHIFT_UP (magnitude=5.048)\n",
      "  4. [244] MEAN_SHIFT_DOWN (magnitude=5.315)\n",
      "  5. [304] MEAN_SHIFT_UP (magnitude=5.109)\n",
      "\n",
      "================================================================================\n",
      "TEXT GENERATION FOR LM TRAINING\n",
      "================================================================================\n",
      "\n",
      "1. STRUCTURED FORMAT:\n",
      "[3-15]DECELERATING_DOWN [5]MEAN_SHIFT_DOWN [7-13]ELEVATED_VOLATILITY [11]SHARP_TROUGH [13-19]HIGH_VOLATILITY [19-33]UPTREND_SHORT_WEAK [19-31]VOLATILITY_SPIKE [23]LOCAL_PEAK_MODERATE [28]LOCAL_TROUGH_WEAK [31-41]HIGH_VOLATILITY [32]LOCAL_TROUGH_WEAK [33-40]FLAT_SHORT [40-43]DECELERATING_DOWN [40]LOCAL_TROUGH_WEAK [43-51]FLAT_SHORT [44]LOCAL_TROUGH_WEAK [44-61]LOW_VOLATILITY [46]LOCAL_TROUGH_WEAK [52-55]FLAT_SHORT [56-59]FLAT_SHORT [61-76]UPTREND_SHORT_WEAK [61]ROUND_TOP [63-70]ELEVATED_VOLATILIT...\n",
      "\n",
      "2. HYBRID FORMAT:\n",
      "[3-15] DECELERATING_DOWN (direction=down slope=-0.06458441913127899 strength=moderate) | [5] MEAN_SHIFT_DOWN (cp_type=mean_shift_down magnitude=5.57783967256546) | [7-13] ELEVATED_VOLATILITY (level=elevated avg_vol=0.26251623034477234) | [11] SHARP_TROUGH (peak_type=trough prominence=0.8897709846496582) | [13-19] HIGH_VOLATILITY (level=high avg_vol=0.30507931113243103) | [19-33] UPTREND_SHORT_WEAK (direction=up slope=0.031175004318356514 strength=weak) | [19-31] VOLATILITY_SPIKE (level=spike avg...\n",
      "\n",
      "3. NARRATIVE FORMAT:\n",
      "Time series of length 336. Contains 32 trend segments:   From 3 to 15, decelerating down observed.   From 19 to 33, uptrend short weak observed.   From 33 to 40, flat short observed. Notable peaks/troughs: 42 detected....\n",
      "\n",
      "================================================================================\n",
      "TOKEN COUNT ESTIMATION\n",
      "================================================================================\n",
      "Structured format: ~100 tokens\n",
      "Hybrid format:     ~531 tokens\n",
      "\n",
      "================================================================================\n",
      "✓ System demonstration complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process batch of EEG-like signals\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    B = 100  # batch size\n",
    "    L = 336  # sequence length\n",
    "    \n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    \n",
    "    # Create realistic time series (trending + noise + spikes)\n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Add occasional spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create comprehensive dataset\n",
    "    dataset = ComprehensiveEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Get statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    stats = dataset.get_statistics()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key:.<40} {value:.2f}\")\n",
    "    \n",
    "    # Example: Get annotation for first sequence\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE ANNOTATION (First Sequence)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(f\"\\nStep labels (first 20): {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"  -> {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "    \n",
    "    print(f\"\\nTrend segments detected: {len(ann.trend_segments)}\")\n",
    "    for i, seg in enumerate(ann.trend_segments[:5]):\n",
    "        print(f\"  {i+1}. [{seg.start}-{seg.end}] {VOCAB.id_to_label(seg.label)} \"\n",
    "              f\"(slope={seg.slope:.3f})\")\n",
    "    \n",
    "    print(f\"\\nPeaks/Troughs detected: {len(ann.peaks_troughs)}\")\n",
    "    for i, pt in enumerate(ann.peaks_troughs[:5]):\n",
    "        print(f\"  {i+1}. [{pt.index}] {VOCAB.id_to_label(pt.label)} \"\n",
    "              f\"(prominence={pt.prominence:.3f})\")\n",
    "    \n",
    "    print(f\"\\nVolatility regimes: {len(ann.volatility_regimes)}\")\n",
    "    for i, vr in enumerate(ann.volatility_regimes):\n",
    "        print(f\"  {i+1}. [{vr.start}-{vr.end}] {VOCAB.id_to_label(vr.label)} \"\n",
    "              f\"(avg_vol={vr.avg_vol:.3f})\")\n",
    "    \n",
    "    print(f\"\\nChange points: {len(ann.change_points)}\")\n",
    "    for i, cp in enumerate(ann.change_points):\n",
    "        print(f\"  {i+1}. [{cp.index}] {VOCAB.id_to_label(cp.label)} \"\n",
    "              f\"(magnitude={cp.magnitude:.3f})\")\n",
    "    \n",
    "    # Generate text descriptions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    print(\"\\n1. STRUCTURED FORMAT:\")\n",
    "    print(text_gen.generate_structured_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. HYBRID FORMAT:\")\n",
    "    print(text_gen.generate_hybrid_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. NARRATIVE FORMAT:\")\n",
    "    print(text_gen.generate_narrative_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count estimation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNT ESTIMATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    structured = text_gen.generate_structured_text(ann)\n",
    "    hybrid = text_gen.generate_hybrid_text(ann)\n",
    "    \n",
    "    # Simple token count (split by whitespace)\n",
    "    print(f\"Structured format: ~{len(structured.split())} tokens\")\n",
    "    print(f\"Hybrid format:     ~{len(hybrid.split())} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ System demonstration complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "301c311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Generating 50 sequences of length 336...\n",
      "Processing 50 sequences of length 336 with hierarchical structure...\n",
      "Extracting features...\n",
      "Encoding step labels...\n",
      "Building hierarchical event structure...\n",
      "  Sequence 0/50...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 654\u001b[0m\n\u001b[1;32m    651\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m trend \u001b[38;5;241m+\u001b[39m noise \u001b[38;5;241m+\u001b[39m spikes\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Create hierarchical dataset\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHierarchicalEventDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# Example annotation\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 347\u001b[0m, in \u001b[0;36mHierarchicalEventDataset.__init__\u001b[0;34m(self, x, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Sequence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 347\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hierarchical_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mappend(annotation)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[31], line 435\u001b[0m, in \u001b[0;36mHierarchicalEventDataset._build_hierarchical_annotation\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    425\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_event(\n\u001b[1;32m    426\u001b[0m     start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    427\u001b[0m     end\u001b[38;5;241m=\u001b[39mL\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 7. Build hierarchy\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m roots \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m all_events \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mget_flat_list(roots)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HierarchicalAnnotation(\n\u001b[1;32m    439\u001b[0m     sequence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx],\n\u001b[1;32m    440\u001b[0m     step_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_labels[idx],\n\u001b[1;32m    441\u001b[0m     event_roots\u001b[38;5;241m=\u001b[39mroots,\n\u001b[1;32m    442\u001b[0m     all_events\u001b[38;5;241m=\u001b[39mall_events\n\u001b[1;32m    443\u001b[0m )\n",
      "Cell \u001b[0;32mIn[31], line 110\u001b[0m, in \u001b[0;36mHierarchicalEventBuilder.build_hierarchy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build parent-child relationships\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Sort by scale (largest first), then by start position\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m sorted_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Build hierarchy tree\u001b[39;00m\n\u001b[1;32m    116\u001b[0m roots \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[31], line 112\u001b[0m, in \u001b[0;36mHierarchicalEventBuilder.build_hierarchy.<locals>.<lambda>\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build parent-child relationships\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Sort by scale (largest first), then by start position\u001b[39;00m\n\u001b[1;32m    110\u001b[0m sorted_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents,\n\u001b[0;32m--> 112\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: (\u001b[38;5;241m-\u001b[39me\u001b[38;5;241m.\u001b[39mscale, e\u001b[38;5;241m.\u001b[39mstart, \u001b[38;5;241m-\u001b[39m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m)\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Build hierarchy tree\u001b[39;00m\n\u001b[1;32m    116\u001b[0m roots \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[31], line 35\u001b[0m, in \u001b[0;36mHierarchicalEvent.duration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mduration\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m-\u001b[39m \u001b[43mstart\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT SYSTEM (Add to existing code)\n",
    "# ============================================================================\n",
    "\n",
    "from typing import List, Dict, Optional, Set\n",
    "from dataclasses import dataclass, field\n",
    "from enum import IntEnum\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # Single points, spikes (1-5 steps)\n",
    "    MINI = 2       # Very short segments (5-15 steps)\n",
    "    MESO = 3       # Medium segments (15-50 steps)\n",
    "    MACRO = 4      # Long segments (50-150 steps)\n",
    "    GLOBAL = 5     # Full sequence level (150+ steps)\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str  # 'trend', 'volatility', 'peak', 'changepoint', etc.\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    \n",
    "    # Hierarchical relationships\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event contains another\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def overlaps(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if events overlap (but neither contains the other)\"\"\"\n",
    "        if self.contains(other) or other.contains(self):\n",
    "            return False\n",
    "        return not (self.end < other.start or self.start > other.end)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start}-{self.end}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical event tree from flat event list\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, \n",
    "                  event_type: str, confidence: float = 1.0, \n",
    "                  metadata: Optional[Dict] = None):\n",
    "        \"\"\"Add event to the collection\"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine scale based on duration\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start,\n",
    "            end=end,\n",
    "            label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale,\n",
    "            event_type=event_type,\n",
    "            confidence=confidence,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Build parent-child relationships\"\"\"\n",
    "        \n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(\n",
    "            self.events,\n",
    "            key=lambda e: (-e.scale, e.start, -e.duration)\n",
    "        )\n",
    "        \n",
    "        # Build hierarchy tree\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            # Find the smallest containing event as parent\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            \n",
    "            if parent is None:\n",
    "                # This is a root node\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                # Add as child to parent\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children by start position within each parent\n",
    "        self._sort_children(roots)\n",
    "        \n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent, \n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for potential_parent in all_events:\n",
    "            if potential_parent == event:\n",
    "                continue\n",
    "            \n",
    "            # Must be larger scale\n",
    "            if potential_parent.scale <= event.scale:\n",
    "                continue\n",
    "            \n",
    "            # Must contain the event\n",
    "            if potential_parent.contains(event):\n",
    "                candidates.append(potential_parent)\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        # Return the smallest containing event (most specific parent)\n",
    "        return min(candidates, key=lambda e: e.duration)\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: (c.start, -c.duration))\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list of all events (depth-first traversal)\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED COMPREHENSIVE ANNOTATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"Complete hierarchical annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    \n",
    "    # Hierarchical event tree\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]  # Flattened\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchy\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Event Tree (Total: {len(self.all_events)} events)\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events that overlap with range [start, end]\"\"\"\n",
    "        result = []\n",
    "        for event in self.all_events:\n",
    "            if not (event.end < start or event.start > end):\n",
    "                result.append(event)\n",
    "        return result\n",
    "    \n",
    "    def to_hierarchical_text(self, format: str = 'structured') -> str:\n",
    "        \"\"\"Generate hierarchical text representation\"\"\"\n",
    "        if format == 'structured':\n",
    "            return self._structured_hierarchical_text()\n",
    "        elif format == 'indented':\n",
    "            return self._indented_hierarchical_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_hierarchical_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _structured_hierarchical_text(self) -> str:\n",
    "        \"\"\"Compact structured format with depth indicators\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_marker}[{node.start}-{node.end}]\"\n",
    "                f\"{node.label_name}\"\n",
    "                f\"@{node.scale.name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _indented_hierarchical_text(self) -> str:\n",
    "        \"\"\"Human-readable indented format\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            indent = \"  \" * node.depth\n",
    "            lines.append(\n",
    "                f\"{indent}[{node.start:03d}-{node.end:03d}] \"\n",
    "                f\"{node.label_name} \"\n",
    "                f\"(scale={node.scale.name}, conf={node.confidence:.2f})\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _narrative_hierarchical_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with macro view\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        \n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall, the sequence exhibits {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        if macro_events:\n",
    "            sentences.append(f\"At the macro level, {len(macro_events)} major segments:\")\n",
    "            for event in macro_events[:3]:\n",
    "                sentences.append(\n",
    "                    f\"  From {event.start} to {event.end}, \"\n",
    "                    f\"{event.label_name.lower().replace('_', ' ')}\"\n",
    "                )\n",
    "                \n",
    "                # Mention nested events\n",
    "                if event.children:\n",
    "                    nested_types = set(c.event_type for c in event.children)\n",
    "                    sentences.append(\n",
    "                        f\"    (contains {len(event.children)} nested events: \"\n",
    "                        f\"{', '.join(nested_types)})\"\n",
    "                    )\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED DATASET WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Dataset with full hierarchical event structure\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L} with hierarchical structure...\")\n",
    "        \n",
    "        # Initialize all detectors (reuse from previous implementation)\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build hierarchical annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical event structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_hierarchical_annotation(i)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Dataset ready with hierarchical structure\")\n",
    "            self._print_statistics()\n",
    "    \n",
    "    def _build_hierarchical_annotation(self, idx: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # 1. Detect all events (as before)\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        change_pts = self.cp_detector.detect(self.x[idx], idx)\n",
    "        \n",
    "        # 2. Add trend segments to builder\n",
    "        for seg in trends:\n",
    "            builder.add_event(\n",
    "                start=seg.start,\n",
    "                end=seg.end,\n",
    "                label=seg.label,\n",
    "                event_type='trend',\n",
    "                confidence=0.9,\n",
    "                metadata={\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 3. Add peaks/troughs\n",
    "        for pt in peaks:\n",
    "            builder.add_event(\n",
    "                start=pt.index,\n",
    "                end=pt.index,\n",
    "                label=pt.label,\n",
    "                event_type='peak_trough',\n",
    "                confidence=0.85,\n",
    "                metadata={\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 4. Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(\n",
    "                start=vr.start,\n",
    "                end=vr.end,\n",
    "                label=vr.label,\n",
    "                event_type='volatility',\n",
    "                confidence=0.8,\n",
    "                metadata={\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 5. Add change points\n",
    "        for cp in change_pts:\n",
    "            builder.add_event(\n",
    "                start=cp.index,\n",
    "                end=cp.index,\n",
    "                label=cp.label,\n",
    "                event_type='changepoint',\n",
    "                confidence=0.75,\n",
    "                metadata={\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 6. Add global regime (macro-level characterization)\n",
    "        L = len(self.x[idx])\n",
    "        global_regime = self._classify_global_regime(idx, L)\n",
    "        builder.add_event(\n",
    "            start=0,\n",
    "            end=L-1,\n",
    "            label=global_regime,\n",
    "            event_type='global_regime',\n",
    "            confidence=0.7,\n",
    "            metadata={'scope': 'global'}\n",
    "        )\n",
    "        \n",
    "        # 7. Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int, length: int) -> int:\n",
    "        \"\"\"Classify overall regime for entire sequence\"\"\"\n",
    "        # Use net slope across entire sequence\n",
    "        if 'slope_100' in self.features:\n",
    "            avg_slope = self.features['slope_100'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        # Use average volatility\n",
    "        if 'std_20' in self.features:\n",
    "            avg_vol = self.features['std_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_vol = 0.5\n",
    "        \n",
    "        # Classify\n",
    "        if avg_vol > 1.5:\n",
    "            return VOCAB.VOLATILE_REGIME\n",
    "        elif avg_slope > 0.1:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.1:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print hierarchical statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"HIERARCHICAL STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Count events by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        total_events = 0\n",
    "        max_depth = 0\n",
    "        \n",
    "        for ann in self.annotations:\n",
    "            total_events += len(ann.all_events)\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "                max_depth = max(max_depth, event.depth)\n",
    "        \n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        print(f\"Sequences: {len(self.annotations)}\")\n",
    "        print(f\"Avg events per sequence: {avg_events:.1f}\")\n",
    "        print(f\"Max hierarchy depth: {max_depth}\")\n",
    "        print(f\"\\nEvents by scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"  {scale.name:.<20} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalTextGenerator:\n",
    "    \"\"\"Generate training text that preserves hierarchical structure\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_flat_sequential(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Flat list sorted by start (loses hierarchy)\"\"\"\n",
    "        events = sorted(ann.all_events, key=lambda e: e.start)\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            tokens.append(f\"[{e.start}-{e.end}]{e.label_name}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_depth_marked(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Include depth markers to indicate nesting\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_str = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_str}[{node.start}-{node.end}]{node.label_name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_xml_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"XML-like nested structure\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent, depth: int = 0):\n",
    "            indent = \"  \" * depth\n",
    "            if node.children:\n",
    "                lines.append(f\"{indent}<event type='{node.label_name}' span='{node.start}-{node.end}'>\")\n",
    "                for child in node.children:\n",
    "                    traverse(child, depth + 1)\n",
    "                lines.append(f\"{indent}</event>\")\n",
    "            else:\n",
    "                lines.append(\n",
    "                    f\"{indent}<event type='{node.label_name}' \"\n",
    "                    f\"span='{node.start}-{node.end}' />\"\n",
    "                )\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_json_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"JSON-like hierarchical structure\"\"\"\n",
    "        import json\n",
    "        \n",
    "        def event_to_dict(node: HierarchicalEvent) -> Dict:\n",
    "            result = {\n",
    "                'span': [node.start, node.end],\n",
    "                'label': node.label_name,\n",
    "                'scale': node.scale.name,\n",
    "                'confidence': node.confidence\n",
    "            }\n",
    "            if node.children:\n",
    "                result['children'] = [event_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree = [event_to_dict(root) for root in ann.event_roots]\n",
    "        return json.dumps(tree, separators=(',', ':'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_with_context(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Natural language that mentions parent context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            # Describe the event\n",
    "            if node.start == node.end:\n",
    "                span_desc = f\"At position {node.start}\"\n",
    "            else:\n",
    "                span_desc = f\"From {node.start} to {node.end}\"\n",
    "            \n",
    "            event_desc = node.label_name.lower().replace('_', ' ')\n",
    "            \n",
    "            # Add parent context if exists\n",
    "            if node.parent:\n",
    "                parent_desc = node.parent.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(\n",
    "                    f\"{span_desc}, {event_desc} occurs \"\n",
    "                    f\"(within {parent_desc} [{node.parent.start}-{node.parent.end}]).\"\n",
    "                )\n",
    "            else:\n",
    "                sentences.append(f\"{span_desc}, {event_desc}.\")\n",
    "            \n",
    "            # Recurse\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED EXAMPLE WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate test data\n",
    "    B = 50\n",
    "    L = 336\n",
    "    \n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Multi-scale trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        # Local corrections (create nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Add dip in middle of uptrend\n",
    "            x[i, 150:180] = x[i, 150:180] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create hierarchical dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Example annotation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HIERARCHICAL STRUCTURE EXAMPLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVENTS BY SCALE\")\n",
    "    print(\"=\" * 80)\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate different text formats\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FORMATS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = HierarchicalTextGenerator()\n",
    "    \n",
    "    print(\"\\n1. DEPTH-MARKED FORMAT:\")\n",
    "    print(text_gen.generate_depth_marked(ann)[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. NARRATIVE WITH CONTEXT:\")\n",
    "    narrative = text_gen.generate_narrative_with_context(ann)\n",
    "    print(narrative[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. JSON-STYLE (first 500 chars):\")\n",
    "    print(text_gen.generate_json_style(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNTS BY FORMAT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    formats = {\n",
    "        'Flat sequential': text_gen.generate_flat_sequential(ann),\n",
    "        'Depth-marked': text_gen.generate_depth_marked(ann),\n",
    "        'Narrative': text_gen.generate_narrative_with_context(ann),\n",
    "        'JSON-style': text_gen.generate_json_style(ann)\n",
    "    }\n",
    "    \n",
    "    for name, text in formats.items():\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"{name:.<30} {tokens:>6} tokens, {chars:>6} chars\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ Hierarchical system complete!\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## 🎯 **What's New in This Version**\\n\\n### **1. True Hierarchical Structure** ✅\\n```\\n[0-335] SIDEWAYS_REGIME (GLOBAL)\\n  ├─ [0-120] UPTREND_LONG_MODERATE (MACRO)\\n  │   ├─ [30-45] DOWNTREND_SHORT_WEAK (MESO)  ← NESTED!\\n  │   │   └─ [38] SPIKE_DOWN_STRONG (MICRO)\\n  │   └─ [50-55] VOLATILITY_SPIKE (MINI)\\n  ├─ [121-200] FLAT_LONG (MACRO)\\n  └─ [201-335] DOWNTREND_LONG_STRONG (MACRO)\\n      └─ [250-265] LOCAL_PEAK_MODERATE (MESO)\\n```\\n\\n### **2. Event Scale Classification**\\n- **MICRO** (1-5 steps): Spikes, single peaks\\n- **MINI** (5-15 steps): Very short segments\\n- **MESO** (15-50 steps): Medium trends, local corrections\\n- **MACRO** (50-150 steps): Major trends\\n- **GLOBAL** (150+ steps): Overall regime\\n\\n### **3. Parent-Child Relationships**\\n- Automatic detection of containment\\n- Smallest containing event becomes parent\\n- Children sorted by start position\\n\\n### **4. Multiple Text Formats**\\n\\n**Depth-Marked (Token-Efficient):**\\n```\\n[0-335]SIDEWAYS_REGIME@GLOBAL >[0-120]UPTREND_LONG_MODERATE@MACRO >>[30-45]DOWNTREND_SHORT_WEAK@MESO >>>[38]SPIKE_DOWN_STRONG@MICRO\\n```\\n\\n**Narrative with Context:**\\n```\\nFrom 0 to 335, sideways regime. From 0 to 120, uptrend long moderate. \\nFrom 30 to 45, downtrend short weak (within uptrend long moderate [0-120]). \\nAt position 38, spike down strong (within downtrend short weak [30-45]). \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 🎯 **What's New in This Version**\n",
    "\n",
    "### **1. True Hierarchical Structure** ✅\n",
    "```\n",
    "[0-335] SIDEWAYS_REGIME (GLOBAL)\n",
    "  ├─ [0-120] UPTREND_LONG_MODERATE (MACRO)\n",
    "  │   ├─ [30-45] DOWNTREND_SHORT_WEAK (MESO)  ← NESTED!\n",
    "  │   │   └─ [38] SPIKE_DOWN_STRONG (MICRO)\n",
    "  │   └─ [50-55] VOLATILITY_SPIKE (MINI)\n",
    "  ├─ [121-200] FLAT_LONG (MACRO)\n",
    "  └─ [201-335] DOWNTREND_LONG_STRONG (MACRO)\n",
    "      └─ [250-265] LOCAL_PEAK_MODERATE (MESO)\n",
    "```\n",
    "\n",
    "### **2. Event Scale Classification**\n",
    "- **MICRO** (1-5 steps): Spikes, single peaks\n",
    "- **MINI** (5-15 steps): Very short segments\n",
    "- **MESO** (15-50 steps): Medium trends, local corrections\n",
    "- **MACRO** (50-150 steps): Major trends\n",
    "- **GLOBAL** (150+ steps): Overall regime\n",
    "\n",
    "### **3. Parent-Child Relationships**\n",
    "- Automatic detection of containment\n",
    "- Smallest containing event becomes parent\n",
    "- Children sorted by start position\n",
    "\n",
    "### **4. Multiple Text Formats**\n",
    "\n",
    "**Depth-Marked (Token-Efficient):**\n",
    "```\n",
    "[0-335]SIDEWAYS_REGIME@GLOBAL >[0-120]UPTREND_LONG_MODERATE@MACRO >>[30-45]DOWNTREND_SHORT_WEAK@MESO >>>[38]SPIKE_DOWN_STRONG@MICRO\n",
    "```\n",
    "\n",
    "**Narrative with Context:**\n",
    "```\n",
    "From 0 to 335, sideways regime. From 0 to 120, uptrend long moderate. \n",
    "From 30 to 45, downtrend short weak (within uptrend long moderate [0-120]). \n",
    "At position 38, spike down strong (within downtrend short weak [30-45]). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "084a1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HIERARCHICAL EVENT LABELING SYSTEM - FIXED VERSION\n",
      "================================================================================\n",
      "\n",
      "Generating 20 sequences of length 336...\n",
      "Processing 20 sequences of length 336...\n",
      "Extracting features...\n",
      "Encoding step labels...\n",
      "Building hierarchical structure...\n",
      "  Sequence 0/20...\n",
      "✓ Complete! Avg events per sequence: 89.0\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE HIERARCHICAL ANNOTATION\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Events (Total: 85)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (61 children)\n",
      "  [000-020] FLAT_SEGMENT (scale=MESO) (3 children)\n",
      "    [003-003] LOCAL_TROUGH (scale=MICRO)\n",
      "    [009-009] LOCAL_PEAK (scale=MICRO)\n",
      "    [011-011] LOCAL_TROUGH (scale=MICRO)\n",
      "  [021-027] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "    [027-027] LOCAL_PEAK (scale=MICRO)\n",
      "  [032-032] LOCAL_TROUGH (scale=MICRO)\n",
      "  [035-035] LOCAL_PEAK (scale=MICRO)\n",
      "  [040-040] LOCAL_TROUGH (scale=MICRO)\n",
      "  [045-045] LOCAL_PEAK (scale=MICRO)\n",
      "  [046-046] LOCAL_TROUGH (scale=MICRO)\n",
      "  [053-053] LOCAL_PEAK (scale=MICRO)\n",
      "  [056-056] LOCAL_TROUGH (scale=MICRO)\n",
      "  [061-061] LOCAL_PEAK (scale=MICRO)\n",
      "  [062-062] LOCAL_TROUGH (scale=MICRO)\n",
      "  [066-066] LOCAL_PEAK (scale=MICRO)\n",
      "  [070-070] LOCAL_TROUGH (scale=MICRO)\n",
      "  [071-071] LOCAL_PEAK (scale=MICRO)\n",
      "  [076-096] FLAT_SEGMENT (scale=MESO) (5 children)\n",
      "    [078-078] LOCAL_TROUGH (scale=MICRO)\n",
      "    [086-086] LOCAL_PEAK (scale=MICRO)\n",
      "    [092-092] LOCAL_TROUGH (scale=MICRO)\n",
      "    [094-094] LOCAL_PEAK (scale=MICRO)\n",
      "    [096-096] LOCAL_TROUGH (scale=MICRO)\n",
      "  [103-108] FLAT_SEGMENT (scale=MINI)\n",
      "  [109-109] LOCAL_PEAK (scale=MICRO)\n",
      "  [112-112] LOCAL_TROUGH (scale=MICRO)\n",
      "  [116-137] FLAT_SEGMENT (scale=MESO) (2 children)\n",
      "    [128-128] LOCAL_PEAK (scale=MICRO)\n",
      "    [130-130] LOCAL_TROUGH (scale=MICRO)\n",
      "  [138-138] SHARP_PEAK (scale=MICRO)\n",
      "  [144-144] LOCAL_TROUGH (scale=MICRO)\n",
      "  [145-151] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "    [146-146] LOCAL_PEAK (scale=MICRO)\n",
      "    [150-150] LOCAL_TROUGH (scale=MICRO)\n",
      "  [155-155] LOCAL_PEAK (scale=MICRO)\n",
      "  [156-156] LOCAL_TROUGH (scale=MICRO)\n",
      "  [159-159] LOCAL_PEAK (scale=MICRO)\n",
      "  [160-160] LOCAL_TROUGH (scale=MICRO)\n",
      "  [161-161] LOCAL_PEAK (scale=MICRO)\n",
      "  [168-168] LOCAL_TROUGH (scale=MICRO)\n",
      "  [169-169] LOCAL_PEAK (scale=MICRO)\n",
      "  [172-172] LOCAL_TROUGH (scale=MICRO)\n",
      "  [175-175] LOCAL_PEAK (scale=MICRO)\n",
      "  [186-186] LOCAL_TROUGH (scale=MICRO)\n",
      "  [192-192] LOCAL_PEAK (scale=MICRO)\n",
      "  [196-196] LOCAL_TROUGH (scale=MICRO)\n",
      "  [201-201] LOCAL_PEAK (scale=MICRO)\n",
      "  [203-203] LOCAL_TROUGH (scale=MICRO)\n",
      "  [206-206] LOCAL_PEAK (scale=MICRO)\n",
      "  [212-217] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "    [215-215] SHARP_TROUGH (scale=MICRO)\n",
      "    [217-217] LOCAL_PEAK (scale=MICRO)\n",
      "  [218-218] LOCAL_TROUGH (scale=MICRO)\n",
      "  [219-219] LOCAL_PEAK (scale=MICRO)\n",
      "  [221-239] FLAT_SEGMENT (scale=MESO) (5 children)\n",
      "    [221-221] LOCAL_TROUGH (scale=MICRO)\n",
      "    [223-223] LOCAL_PEAK (scale=MICRO)\n",
      "    [227-227] LOCAL_TROUGH (scale=MICRO)\n",
      "    [231-231] LOCAL_PEAK (scale=MICRO)\n",
      "    [234-234] LOCAL_TROUGH (scale=MICRO)\n",
      "  [240-240] SHARP_PEAK (scale=MICRO)\n",
      "  [244-244] LOCAL_TROUGH (scale=MICRO)\n",
      "  [247-247] LOCAL_PEAK (scale=MICRO)\n",
      "  [259-259] LOCAL_TROUGH (scale=MICRO)\n",
      "  [263-263] LOCAL_PEAK (scale=MICRO)\n",
      "  [264-264] LOCAL_TROUGH (scale=MICRO)\n",
      "  [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "  [269-276] FLAT_SEGMENT (scale=MINI) (3 children)\n",
      "    [270-270] LOCAL_TROUGH (scale=MICRO)\n",
      "    [273-273] LOCAL_PEAK (scale=MICRO)\n",
      "    [274-274] SHARP_TROUGH (scale=MICRO)\n",
      "  [281-281] LOCAL_PEAK (scale=MICRO)\n",
      "  [289-289] LOCAL_TROUGH (scale=MICRO)\n",
      "  [309-309] LOCAL_PEAK (scale=MICRO)\n",
      "  [310-310] LOCAL_TROUGH (scale=MICRO)\n",
      "  [312-312] LOCAL_PEAK (scale=MICRO)\n",
      "  [313-313] LOCAL_TROUGH (scale=MICRO)\n",
      "  [318-318] LOCAL_PEAK (scale=MICRO)\n",
      "  [319-319] LOCAL_TROUGH (scale=MICRO)\n",
      "  [322-322] LOCAL_PEAK (scale=MICRO)\n",
      "  [323-323] LOCAL_TROUGH (scale=MICRO)\n",
      "  [329-329] LOCAL_PEAK (scale=MICRO)\n",
      "  [330-330] LOCAL_TROUGH (scale=MICRO)\n",
      "\n",
      "================================================================================\n",
      "TEXT OUTPUT\n",
      "================================================================================\n",
      "[0-335]SIDEWAYS_REGIME >[0-20]FLAT_SEGMENT >>[3-3]LOCAL_TROUGH >>[9-9]LOCAL_PEAK >>[11-11]LOCAL_TROUGH >[21-27]UPTREND_SHORT >>[27-27]LOCAL_PEAK >[32-32]LOCAL_TROUGH >[35-35]LOCAL_PEAK >[40-40]LOCAL_TROUGH >[45-45]LOCAL_PEAK >[46-46]LOCAL_TROUGH >[53-53]LOCAL_PEAK >[56-56]LOCAL_TROUGH >[61-61]LOCAL_PEAK >[62-62]LOCAL_TROUGH >[66-66]LOCAL_PEAK >[70-70]LOCAL_TROUGH >[71-71]LOCAL_PEAK >[76-96]FLAT_SEGMENT >>[78-78]LOCAL_TROUGH >>[86-86]LOCAL_PEAK >>[92-92]LOCAL_TROUGH >>[94-94]LOCAL_PEAK >>[96-96]L...\n",
      "\n",
      "Total tokens: ~85\n",
      "\n",
      "================================================================================\n",
      "✓ System working correctly!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMPLETE HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "========================================================\n",
    "Fixed and production-ready version\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VOCABULARY (same as before)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary\"\"\"\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trends\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HIERARCHICAL STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # 1-5 steps\n",
    "    MINI = 2       # 5-15 steps\n",
    "    MESO = 3       # 15-50 steps\n",
    "    MACRO = 4      # 50-150 steps\n",
    "    GLOBAL = 5     # 150+ steps\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FIXED FEATURE EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales - FIXED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First derivative\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Simple rolling slope\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Simple slope computation\"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            # Slope from start to end of window\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "# ============================================================================\n",
    "# 4. STEP ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep - SIMPLIFIED\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Classify\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SIMPLE DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "class SimpleTrendDetector:\n",
    "    \"\"\"Simplified trend detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Use slope if available\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "class SimplePeakDetector:\n",
    "    \"\"\"Simplified peak detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(x_np, prominence=0.2)\n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                label = VOCAB.SHARP_PEAK if prom > 0.5 else VOCAB.LOCAL_PEAK\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(-x_np, prominence=0.2)\n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                label = VOCAB.SHARP_TROUGH if prom > 0.5 else VOCAB.LOCAL_TROUGH\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return events\n",
    "\n",
    "# ============================================================================\n",
    "# 6. HIERARCHY BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# 7. HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"Generate hierarchical text\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Main dataset class - FIXED\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = SimpleTrendDetector()\n",
    "        self.peak_detector = SimplePeakDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode steps\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            builder = HierarchicalEventBuilder()\n",
    "            \n",
    "            # Detect trends\n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            for seg in trends:\n",
    "                builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                                metadata=seg.metadata)\n",
    "            \n",
    "            # Detect peaks\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            for pk in peaks:\n",
    "                builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                                metadata=pk.metadata)\n",
    "            \n",
    "            # Add global regime\n",
    "            if 'slope_20' in self.features:\n",
    "                avg_slope = self.features['slope_20'][i].mean().item()\n",
    "            else:\n",
    "                avg_slope = 0\n",
    "            \n",
    "            if avg_slope > 0.05:\n",
    "                global_label = VOCAB.BULLISH_REGIME\n",
    "            elif avg_slope < -0.05:\n",
    "                global_label = VOCAB.BEARISH_REGIME\n",
    "            else:\n",
    "                global_label = VOCAB.SIDEWAYS_REGIME\n",
    "            \n",
    "            builder.add_event(0, L-1, global_label, 'regime')\n",
    "            \n",
    "            # Build hierarchy\n",
    "            roots = builder.build_hierarchy()\n",
    "            all_events = builder.get_flat_list(roots)\n",
    "            \n",
    "            self.annotations.append(HierarchicalAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                event_roots=roots,\n",
    "                all_events=all_events\n",
    "            ))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Complete! Avg events per sequence: {self._avg_events():.1f}\")\n",
    "    \n",
    "    def _avg_events(self):\n",
    "        return sum(len(a.all_events) for a in self.annotations) / len(self.annotations)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# 9. TEST\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL EVENT LABELING SYSTEM - FIXED VERSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        noise = 0.1 * torch.randn(L)\n",
    "        spikes = torch.zeros(L)\n",
    "        spike_idx = torch.randint(50, L-50, (2,))\n",
    "        spikes[spike_idx] = torch.randn(2) * 2\n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE HIERARCHICAL ANNOTATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT OUTPUT\")\n",
    "    print(\"=\" * 80)\n",
    "    text = ann.to_text()\n",
    "    print(text[:500] + \"...\")\n",
    "    print(f\"\\nTotal tokens: ~{len(text.split())}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ System working correctly!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cf68f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
      "Demonstration\n",
      "================================================================================\n",
      "\n",
      "Generating 20 synthetic sequences of length 336...\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 20\n",
      "Length: 336\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 6720 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/20...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 3039\n",
      "      Avg per sequence: 151.9\n",
      "      By scale:\n",
      "        MICRO.......  125.2 per sequence\n",
      "        MINI........   17.6 per sequence\n",
      "        MESO........    8.1 per sequence\n",
      "        MACRO.......    0.0 per sequence\n",
      "        GLOBAL......    1.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: HIERARCHICAL STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Events (Total: 153)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (29 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO) (1 children)\n",
      "    [001-001] LOCAL_PEAK (scale=MICRO)\n",
      "  [003-018] NORMAL_VOLATILITY (scale=MESO) (9 children)\n",
      "    [003-003] SHARP_TROUGH (scale=MICRO)\n",
      "    [004-004] SHARP_PEAK (scale=MICRO)\n",
      "    [005-005] LOCAL_TROUGH (scale=MICRO)\n",
      "    [006-006] LOCAL_PEAK (scale=MICRO)\n",
      "    [007-007] SHARP_TROUGH (scale=MICRO)\n",
      "    [009-009] SHARP_PEAK (scale=MICRO)\n",
      "    [011-011] SHARP_TROUGH (scale=MICRO)\n",
      "    [016-016] SHARP_PEAK (scale=MICRO)\n",
      "    [018-018] LOCAL_TROUGH (scale=MICRO)\n",
      "  [019-030] HIGH_VOLATILITY (scale=MINI) (2 children)\n",
      "    [019-019] LOCAL_PEAK (scale=MICRO)\n",
      "    [020-020] SHARP_TROUGH (scale=MICRO)\n",
      "  [021-027] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [023-023] SHARP_PEAK (scale=MICRO)\n",
      "    [026-026] LOCAL_TROUGH (scale=MICRO)\n",
      "    [027-027] LOCAL_PEAK (scale=MICRO)\n",
      "  [032-032] LOCAL_TROUGH (scale=MICRO)\n",
      "  [034-044] NORMAL_VOLATILITY (scale=MINI) (2 children)\n",
      "    [035-035] LOCAL_PEAK (scale=MICRO)\n",
      "    [040-040] LOCAL_TROUGH (scale=MICRO)\n",
      "  [045-060] LOW_VOLATILITY (scale=MESO) (6 children)\n",
      "    [045-045] LOCAL_PEAK (scale=MICRO)\n",
      "    [046-046] LOCAL_TROUGH (scale=MICRO)\n",
      "    [053-053] LOCAL_PEAK (scale=MICRO)\n",
      "    [056-056] SHARP_TROUGH (scale=MICRO)\n",
      "    [059-059] LOCAL_PEAK (scale=MICRO)\n",
      "    [060-060] LOCAL_TROUGH (scale=MICRO)\n",
      "  [061-077] NORMAL_VOLATILITY (scale=MESO) (7 children)\n",
      "    [061-061] SHARP_PEAK (scale=MICRO)\n",
      "    [062-062] SHARP_TROUGH (scale=MICRO)\n",
      "    [063-068] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "      [066-066] SHARP_PEAK (scale=MICRO)\n",
      "    [070-070] SHARP_TROUGH (scale=MICRO)\n",
      "    [071-071] SHARP_PEAK (scale=MICRO)\n",
      "    [072-072] LOCAL_TROUGH (scale=MICRO)\n",
      "    [076-076] LOCAL_PEAK (scale=MICRO)\n",
      "  [078-078] SHARP_TROUGH (scale=MICRO)\n",
      "  [080-080] LOCAL_PEAK (scale=MICRO)\n",
      "  [081-097] NORMAL_VOLATILITY (scale=MESO) (5 children)\n",
      "    [082-082] LOCAL_TROUGH (scale=MICRO)\n",
      "    [086-086] LOCAL_PEAK (scale=MICRO)\n",
      "    [092-092] LOCAL_TROUGH (scale=MICRO)\n",
      "    [094-094] LOCAL_PEAK (scale=MICRO)\n",
      "    [096-096] LOCAL_TROUGH (scale=MICRO)\n",
      "  [098-127] LOW_VOLATILITY (scale=MESO) (6 children)\n",
      "    [103-108] FLAT_SEGMENT (scale=MINI)\n",
      "    [109-109] SHARP_PEAK (scale=MICRO)\n",
      "    [112-112] SHARP_TROUGH (scale=MICRO)\n",
      "    [116-122] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "      [116-116] LOCAL_PEAK (scale=MICRO)\n",
      "      [118-118] LOCAL_TROUGH (scale=MICRO)\n",
      "    [124-124] LOCAL_PEAK (scale=MICRO)\n",
      "    [125-125] LOCAL_TROUGH (scale=MICRO)\n",
      "  [128-137] NORMAL_VOLATILITY (scale=MINI) (3 children)\n",
      "    [128-128] SHARP_PEAK (scale=MICRO)\n",
      "    [130-130] SHARP_TROUGH (scale=MICRO)\n",
      "    [133-133] SHARP_PEAK (scale=MICRO)\n",
      "  [138-157] HIGH_VOLATILITY (scale=MESO) (7 children)\n",
      "    [138-138] SHARP_TROUGH (scale=MICRO)\n",
      "    [139-139] SHARP_PEAK (scale=MICRO)\n",
      "    [144-144] LOCAL_TROUGH (scale=MICRO)\n",
      "    [146-146] LOCAL_PEAK (scale=MICRO)\n",
      "    [150-150] LOCAL_TROUGH (scale=MICRO)\n",
      "    [155-155] LOCAL_PEAK (scale=MICRO)\n",
      "    [156-156] LOCAL_TROUGH (scale=MICRO)\n",
      "  [158-168] LOW_VOLATILITY (scale=MINI) (4 children)\n",
      "    [159-159] LOCAL_PEAK (scale=MICRO)\n",
      "    [160-160] LOCAL_TROUGH (scale=MICRO)\n",
      "    [161-161] LOCAL_PEAK (scale=MICRO)\n",
      "    [168-168] SHARP_TROUGH (scale=MICRO)\n",
      "  [169-185] NORMAL_VOLATILITY (scale=MESO) (5 children)\n",
      "    [169-169] SHARP_PEAK (scale=MICRO)\n",
      "    [172-172] SHARP_TROUGH (scale=MICRO)\n",
      "    [175-175] SHARP_PEAK (scale=MICRO)\n",
      "    [181-181] LOCAL_TROUGH (scale=MICRO)\n",
      "    [183-183] LOCAL_PEAK (scale=MICRO)\n",
      "  [186-186] SHARP_TROUGH (scale=MICRO)\n",
      "  [188-211] NORMAL_VOLATILITY (scale=MESO) (9 children)\n",
      "    [188-188] LOCAL_PEAK (scale=MICRO)\n",
      "    [189-189] LOCAL_TROUGH (scale=MICRO)\n",
      "    [192-192] SHARP_PEAK (scale=MICRO)\n",
      "    [193-193] SHARP_TROUGH (scale=MICRO)\n",
      "    [194-194] SHARP_PEAK (scale=MICRO)\n",
      "    [196-196] LOCAL_TROUGH (scale=MICRO)\n",
      "    [201-201] LOCAL_PEAK (scale=MICRO)\n",
      "    [203-203] LOCAL_TROUGH (scale=MICRO)\n",
      "    [206-206] LOCAL_PEAK (scale=MICRO)\n",
      "  [213-218] HIGH_VOLATILITY (scale=MINI) (5 children)\n",
      "    [213-213] SHARP_TROUGH (scale=MICRO)\n",
      "    [214-214] LOCAL_PEAK (scale=MICRO)\n",
      "    [215-215] LOCAL_TROUGH (scale=MICRO)\n",
      "    [217-217] LOCAL_PEAK (scale=MICRO)\n",
      "    [218-218] LOCAL_TROUGH (scale=MICRO)\n",
      "  [219-232] VOLATILITY_SPIKE (scale=MINI) (5 children)\n",
      "    [219-219] LOCAL_PEAK (scale=MICRO)\n",
      "    [221-221] LOCAL_TROUGH (scale=MICRO)\n",
      "    [225-225] SHARP_PEAK (scale=MICRO)\n",
      "    [227-227] SHARP_TROUGH (scale=MICRO)\n",
      "    [231-231] SHARP_PEAK (scale=MICRO)\n",
      "  [234-234] SHARP_TROUGH (scale=MICRO)\n",
      "  [239-239] SHARP_PEAK (scale=MICRO)\n",
      "  [240-259] VOLATILITY_SPIKE (scale=MESO) (9 children)\n",
      "    [240-240] SHARP_TROUGH (scale=MICRO)\n",
      "    [241-241] LOCAL_PEAK (scale=MICRO)\n",
      "    [242-242] LOCAL_TROUGH (scale=MICRO)\n",
      "    [243-243] LOCAL_PEAK (scale=MICRO)\n",
      "    [244-244] LOCAL_TROUGH (scale=MICRO)\n",
      "    [247-247] SHARP_PEAK (scale=MICRO)\n",
      "    [249-249] SHARP_TROUGH (scale=MICRO)\n",
      "    [250-250] LOCAL_PEAK (scale=MICRO)\n",
      "    [259-259] LOCAL_TROUGH (scale=MICRO)\n",
      "  [260-270] NORMAL_VOLATILITY (scale=MINI) (4 children)\n",
      "    [263-263] LOCAL_PEAK (scale=MICRO)\n",
      "    [264-264] LOCAL_TROUGH (scale=MICRO)\n",
      "    [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "    [270-270] LOCAL_TROUGH (scale=MICRO)\n",
      "  [271-276] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "    [273-273] LOCAL_PEAK (scale=MICRO)\n",
      "    [274-274] LOCAL_TROUGH (scale=MICRO)\n",
      "  [271-279] LOW_VOLATILITY (scale=MINI) (2 children)\n",
      "    [277-277] LOCAL_PEAK (scale=MICRO)\n",
      "    [279-279] LOCAL_TROUGH (scale=MICRO)\n",
      "  [277-286] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [281-281] SHARP_PEAK (scale=MICRO)\n",
      "    [284-284] LOCAL_TROUGH (scale=MICRO)\n",
      "    [285-285] LOCAL_PEAK (scale=MICRO)\n",
      "  [280-321] NORMAL_VOLATILITY (scale=MESO) (17 children)\n",
      "    [287-287] LOCAL_TROUGH (scale=MICRO)\n",
      "    [288-288] LOCAL_PEAK (scale=MICRO)\n",
      "    [289-289] SHARP_TROUGH (scale=MICRO)\n",
      "    [296-296] SHARP_PEAK (scale=MICRO)\n",
      "    [297-297] LOCAL_TROUGH (scale=MICRO)\n",
      "    [298-298] LOCAL_PEAK (scale=MICRO)\n",
      "    [300-300] LOCAL_TROUGH (scale=MICRO)\n",
      "    [302-302] LOCAL_PEAK (scale=MICRO)\n",
      "    [305-305] LOCAL_TROUGH (scale=MICRO)\n",
      "    [306-306] LOCAL_PEAK (scale=MICRO)\n",
      "    [307-307] SHARP_TROUGH (scale=MICRO)\n",
      "    [309-309] LOCAL_PEAK (scale=MICRO)\n",
      "    [310-310] LOCAL_TROUGH (scale=MICRO)\n",
      "    [312-312] LOCAL_PEAK (scale=MICRO)\n",
      "    [313-313] LOCAL_TROUGH (scale=MICRO)\n",
      "    [318-318] LOCAL_PEAK (scale=MICRO)\n",
      "    [319-319] LOCAL_TROUGH (scale=MICRO)\n",
      "  [322-335] LOW_VOLATILITY (scale=MINI) (4 children)\n",
      "    [322-322] LOCAL_PEAK (scale=MICRO)\n",
      "    [323-323] LOCAL_TROUGH (scale=MICRO)\n",
      "    [329-329] LOCAL_PEAK (scale=MICRO)\n",
      "    [330-330] LOCAL_TROUGH (scale=MICRO)\n",
      "\n",
      "================================================================================\n",
      "EVENTS BY HIERARCHICAL SCALE\n",
      "================================================================================\n",
      "\n",
      "MICRO (126 events):\n",
      "  [001-001] LOCAL_PEAK\n",
      "  [003-003] SHARP_TROUGH\n",
      "  [004-004] SHARP_PEAK\n",
      "  [005-005] LOCAL_TROUGH\n",
      "  [006-006] LOCAL_PEAK\n",
      "\n",
      "MINI (15 events):\n",
      "  [019-030] HIGH_VOLATILITY\n",
      "  [021-027] UPTREND_SHORT\n",
      "  [034-044] NORMAL_VOLATILITY\n",
      "  [063-068] UPTREND_SHORT\n",
      "  [103-108] FLAT_SEGMENT\n",
      "\n",
      "MESO (11 events):\n",
      "  [000-019] FLAT_SEGMENT\n",
      "  [003-018] NORMAL_VOLATILITY\n",
      "  [045-060] LOW_VOLATILITY\n",
      "  [061-077] NORMAL_VOLATILITY\n",
      "  [081-097] NORMAL_VOLATILITY\n",
      "\n",
      "MACRO (0 events):\n",
      "\n",
      "GLOBAL (1 events):\n",
      "  [000-335] SIDEWAYS_REGIME\n",
      "\n",
      "================================================================================\n",
      "TEXT GENERATION FOR LM TRAINING\n",
      "================================================================================\n",
      "\n",
      "DEPTH_MARKED:\n",
      "  Tokens: 153, Chars: 3482\n",
      "  Preview: [0-335]SIDEWAYS_REGIME >[0-19]FLAT_SEGMENT >>[1-1]LOCAL_PEAK >[3-18]NORMAL_VOLATILITY >>[3-3]SHARP_TROUGH >>[4-4]SHARP_PEAK >>[5-5]LOCAL_TROUGH >>[6-6]LOCAL_PEAK >>[7-7]SHARP_TROUGH >>[9-9]SHARP_PEAK ...\n",
      "\n",
      "FLAT:\n",
      "  Tokens: 153, Chars: 3204\n",
      "  Preview: [0-335]SIDEWAYS_REGIME [0-19]FLAT_SEGMENT [1-1]LOCAL_PEAK [3-18]NORMAL_VOLATILITY [3-3]SHARP_TROUGH [4-4]SHARP_PEAK [5-5]LOCAL_TROUGH [6-6]LOCAL_PEAK [7-7]SHARP_TROUGH [9-9]SHARP_PEAK [11-11]SHARP_TRO...\n",
      "\n",
      "NARRATIVE:\n",
      "  Tokens: 3, Chars: 25\n",
      "  Preview: Overall: sideways regime....\n",
      "\n",
      "================================================================================\n",
      "FULL CORPUS STATISTICS\n",
      "================================================================================\n",
      "  num_documents: 20\n",
      "  total_tokens: 3,039\n",
      "  total_chars: 69,374\n",
      "  avg_tokens_per_doc: 151.9\n",
      "  avg_chars_per_doc: 3,468.7\n",
      "\n",
      "================================================================================\n",
      "✓ DEMONSTRATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "The system is ready for processing real time series data.\n",
      "Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "===============================================\n",
    "\n",
    "A comprehensive system for detecting and labeling events in time series data\n",
    "with hierarchical structure preservation.\n",
    "\n",
    "Workflow:\n",
    "    1. Define vocabulary and hierarchical structures\n",
    "    2. Extract multi-scale features from raw time series\n",
    "    3. Encode step-wise labels for each timestep\n",
    "    4. Detect higher-level events (trends, peaks, volatility, change points)\n",
    "    5. Build hierarchical event tree\n",
    "    6. Generate training text in various formats\n",
    "\n",
    "Author: Sachith\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels for events\"\"\"\n",
    "    MICRO = 1      # 1-5 timesteps (spikes, single points)\n",
    "    MINI = 2       # 5-15 timesteps (very short segments)\n",
    "    MESO = 3       # 15-50 timesteps (medium segments, local patterns)\n",
    "    MACRO = 4      # 50-150 timesteps (major trends)\n",
    "    GLOBAL = 5     # 150+ timesteps (full sequence characteristics)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"\n",
    "    Complete event vocabulary with 64 distinct labels.\n",
    "    \n",
    "    Categories:\n",
    "        - Special tokens (0-2)\n",
    "        - Step movements (3-10)\n",
    "        - Trend segments (20-26)\n",
    "        - Peaks/troughs (30-33)\n",
    "        - Volatility regimes (40-43)\n",
    "        - Change points (50-51)\n",
    "        - Global regimes (60-63)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    \n",
    "    # Step-level movements\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trend segments\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks and troughs\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility regimes\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Global regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls) -> int:\n",
    "        \"\"\"Return total vocabulary size\"\"\"\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        \"\"\"Convert label ID to string name\"\"\"\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"\n",
    "    Event node in hierarchical tree structure.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting timestep index\n",
    "        end: Ending timestep index\n",
    "        label: Vocabulary ID\n",
    "        label_name: Human-readable label\n",
    "        scale: Hierarchical scale level\n",
    "        event_type: Category (trend/peak/volatility/changepoint/regime)\n",
    "        confidence: Detection confidence score\n",
    "        metadata: Additional event-specific information\n",
    "        parent: Parent event in hierarchy (None for root)\n",
    "        children: List of child events\n",
    "    \"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        \"\"\"Duration in timesteps\"\"\"\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy tree (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event fully contains another event\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "\n",
    "# Global vocabulary instance\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features at multiple temporal scales using efficient convolutions.\n",
    "    \n",
    "    Features computed:\n",
    "        - First derivative (dx)\n",
    "        - Rolling mean at multiple window sizes\n",
    "        - Rolling standard deviation (volatility)\n",
    "        - Rolling slope (trend strength)\n",
    "        - Z-scores for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        scales: List of window sizes for rolling features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scales: List[int] = [5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from time series batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor of shape [B, L]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature tensors, each shape [B, L]:\n",
    "                - 'dx': First derivative\n",
    "                - 'mean_{w}': Rolling mean with window w\n",
    "                - 'std_{w}': Rolling std with window w\n",
    "                - 'slope_{w}': Rolling slope with window w\n",
    "                - 'zscore': Normalized z-scores\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        features = {}\n",
    "        \n",
    "        # First derivative (rate of change)\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean using efficient convolution\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling standard deviation (volatility measure)\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (trend direction and strength)\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores for outlier detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rolling linear slope over window.\n",
    "        \n",
    "        Simple implementation: slope = (end - start) / window\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STEP-WISE LABEL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"\n",
    "    Encode each timestep with symbolic movement labels.\n",
    "    \n",
    "    Labels based on magnitude of first derivative:\n",
    "        - FLAT: negligible change\n",
    "        - UP/DOWN_SMALL/MEDIUM/LARGE: quantile-based magnitude bins\n",
    "        - SPIKE_UP/DOWN: extreme changes (>90th percentile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode step-wise movement labels for entire batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor [B, L]\n",
    "            features: Feature dictionary from MultiScaleFeatureExtractor\n",
    "        \n",
    "        Returns:\n",
    "            Label tensor [B, L] with vocabulary IDs\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padded value\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Compute quantiles for adaptive thresholding\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33  # Flat threshold\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat (negligible change)\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: EVENT DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    \"\"\"Simple segment representation for detector outputs\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"\n",
    "    Detect trend segments using slope sign changes.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Compute rolling slopes\n",
    "        2. Find sign changes (trend reversals)\n",
    "        3. Classify segments by direction and duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect trend segments in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of trend segments\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Get slopes\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend direction changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify segment\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs using scipy's find_peaks.\n",
    "    \n",
    "    Classifies peaks/troughs by:\n",
    "        - Prominence: How much the peak stands out\n",
    "        - Type: Sharp vs broad based on width\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            idx: Sequence index (unused but kept for consistency)\n",
    "        \n",
    "        Returns:\n",
    "            List of peak/trough events\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(x_np, prominence=0.2)\n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                label = VOCAB.SHARP_PEAK if prom > 0.5 else VOCAB.LOCAL_PEAK\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs (peaks of inverted signal)\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(-x_np, prominence=0.2)\n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                label = VOCAB.SHARP_TROUGH if prom > 0.5 else VOCAB.LOCAL_TROUGH\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return events\n",
    "\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect volatility regimes using rolling standard deviation.\n",
    "    \n",
    "    Classifies regimes by quantile thresholds:\n",
    "        - LOW: Below 25th percentile\n",
    "        - NORMAL: 25th-75th percentile\n",
    "        - HIGH: Above 75th percentile\n",
    "        - SPIKE: Above 90th percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect volatility regimes in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of volatility regime segments\n",
    "        \"\"\"\n",
    "        if 'std_20' not in features:\n",
    "            return []\n",
    "        \n",
    "        vol = features['std_20'][idx].cpu().numpy()\n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantile thresholds\n",
    "        q25, q75, q90 = np.percentile(vol, [25, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q75)] = 1  # normal\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 2  # high\n",
    "        vol_levels[vol > q90] = 3  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end+1].mean()\n",
    "            \n",
    "            # Map to vocabulary\n",
    "            label_map = {\n",
    "                0: VOCAB.LOW_VOLATILITY,\n",
    "                1: VOCAB.NORMAL_VOLATILITY,\n",
    "                2: VOCAB.HIGH_VOLATILITY,\n",
    "                3: VOCAB.VOLATILITY_SPIKE\n",
    "            }\n",
    "            \n",
    "            regimes.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label_map[level_code],\n",
    "                metadata={'avg_volatility': float(avg_vol)}\n",
    "            ))\n",
    "        \n",
    "        return regimes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: HIERARCHICAL STRUCTURE BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"\n",
    "    Build hierarchical event tree from flat event list.\n",
    "    \n",
    "    Process:\n",
    "        1. Classify each event's scale based on duration\n",
    "        2. Sort events by scale (largest first)\n",
    "        3. Build parent-child relationships via containment\n",
    "        4. Sort children by temporal order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[HierarchicalEvent] = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add event to collection with automatic scale classification.\n",
    "        \n",
    "        Args:\n",
    "            start: Starting timestep\n",
    "            end: Ending timestep\n",
    "            label: Vocabulary ID\n",
    "            event_type: Category string\n",
    "            confidence: Detection confidence score\n",
    "            metadata: Additional information dictionary\n",
    "        \"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine hierarchical scale\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"\n",
    "        Build hierarchical tree structure.\n",
    "        \n",
    "        Returns:\n",
    "            List of root events (events with no parent)\n",
    "        \"\"\"\n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        # Build parent-child relationships\n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children within each parent\n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event (most specific parent)\"\"\"\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list via depth-first traversal\"\"\"\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"\n",
    "    Complete hierarchical annotation for one sequence.\n",
    "    \n",
    "    Attributes:\n",
    "        sequence: Original time series [L]\n",
    "        step_labels: Step-wise labels [L]\n",
    "        event_roots: Root nodes of hierarchy tree\n",
    "        all_events: Flattened list of all events\n",
    "    \"\"\"\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchical structure\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific hierarchical scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events overlapping with time range\"\"\"\n",
    "        return [e for e in self.all_events \n",
    "                if not (e.end < start or e.start > end)]\n",
    "    \n",
    "    def to_text(self, format: str = 'depth_marked') -> str:\n",
    "        \"\"\"\n",
    "        Generate text representation.\n",
    "        \n",
    "        Args:\n",
    "            format: Output format\n",
    "                - 'depth_marked': Depth indicators with events\n",
    "                - 'flat': Simple sequential list\n",
    "                - 'narrative': Natural language description\n",
    "        \n",
    "        Returns:\n",
    "            Text string for language model training\n",
    "        \"\"\"\n",
    "        if format == 'depth_marked':\n",
    "            return self._depth_marked_text()\n",
    "        elif format == 'flat':\n",
    "            return self._flat_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _depth_marked_text(self) -> str:\n",
    "        \"\"\"Depth markers indicate nesting: > >> >>> etc.\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _flat_text(self) -> str:\n",
    "        \"\"\"Simple sequential list (loses hierarchy)\"\"\"\n",
    "        events = sorted(self.all_events, key=lambda e: e.start)\n",
    "        return \" \".join(f\"[{e.start}-{e.end}]{e.label_name}\" for e in events)\n",
    "    \n",
    "    def _narrative_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with global view\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall: {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        # Describe macro events\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        if macro_events:\n",
    "            sentences.append(f\"{len(macro_events)} major segments detected.\")\n",
    "            for event in macro_events[:3]:\n",
    "                desc = event.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(f\"[{event.start}-{event.end}]: {desc}\")\n",
    "                if event.children:\n",
    "                    nested = \", \".join(set(c.event_type for c in event.children))\n",
    "                    sentences.append(f\"  (contains: {nested})\")\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MAIN DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Main dataset class for hierarchical event labeling.\n",
    "    \n",
    "    Processing pipeline:\n",
    "        1. Extract multi-scale features\n",
    "        2. Encode step-wise labels\n",
    "        3. Detect events (trends, peaks, volatility)\n",
    "        4. Add global regime classification\n",
    "        5. Build hierarchical structure\n",
    "        6. Create annotations\n",
    "    \n",
    "    Args:\n",
    "        x: Time series tensor [B, L]\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INITIALIZING HIERARCHICAL EVENT DATASET\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Sequences: {B}\")\n",
    "            print(f\"Length: {L}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        \n",
    "        # STEP 1: Extract features\n",
    "        if verbose:\n",
    "            print(f\"\\n[1/4] Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Computed {len(self.features)} feature types\")\n",
    "        \n",
    "        # STEP 2: Encode step labels\n",
    "        if verbose:\n",
    "            print(f\"[2/4] Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Encoded {B * L} timesteps\")\n",
    "        \n",
    "        # STEP 3: Detect events and build hierarchy\n",
    "        if verbose:\n",
    "            print(f\"[3/4] Detecting events and building hierarchy...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"      Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_annotation(i, L)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        # STEP 4: Compute statistics\n",
    "        if verbose:\n",
    "            print(f\"[4/4] Computing statistics...\")\n",
    "            self._print_statistics()\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"✓ DATASET READY\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _build_annotation(self, idx: int, L: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # Detect all event types\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        \n",
    "        # Add trend segments\n",
    "        for seg in trends:\n",
    "            builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                            confidence=0.9, metadata=seg.metadata)\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pk in peaks:\n",
    "            builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                            confidence=0.85, metadata=pk.metadata)\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(vr.start, vr.end, vr.label, 'volatility',\n",
    "                            confidence=0.8, metadata=vr.metadata)\n",
    "        \n",
    "        # Add global regime\n",
    "        global_label = self._classify_global_regime(idx)\n",
    "        builder.add_event(0, L-1, global_label, 'regime', confidence=0.7)\n",
    "        \n",
    "        # Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int) -> int:\n",
    "        \"\"\"Classify overall sequence regime\"\"\"\n",
    "        if 'slope_20' in self.features:\n",
    "            avg_slope = self.features['slope_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        if avg_slope > 0.05:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.05:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        # Count by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        for ann in self.annotations:\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "        \n",
    "        print(f\"      Total events: {total_events}\")\n",
    "        print(f\"      Avg per sequence: {avg_events:.1f}\")\n",
    "        print(f\"      By scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"        {scale.name:.<12} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'vocab_size': VOCAB.get_vocab_size(),\n",
    "            'total_events': total_events,\n",
    "            'avg_events_per_sequence': total_events / len(self.annotations),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TEXT GENERATION FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"\n",
    "    Generate training text in various formats.\n",
    "    \n",
    "    Formats:\n",
    "        - depth_marked: Hierarchical with depth indicators (>)\n",
    "        - flat: Simple sequential list\n",
    "        - narrative: Natural language description\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_corpus(dataset: HierarchicalEventDataset, \n",
    "                       format: str = 'depth_marked') -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text corpus for all sequences.\n",
    "        \n",
    "        Args:\n",
    "            dataset: HierarchicalEventDataset instance\n",
    "            format: Text format\n",
    "        \n",
    "        Returns:\n",
    "            List of text strings, one per sequence\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for annotation in dataset.annotations:\n",
    "            text = annotation.to_text(format=format)\n",
    "            corpus.append(text)\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(corpus: List[str]) -> Dict:\n",
    "        \"\"\"Estimate token counts for corpus\"\"\"\n",
    "        total_tokens = sum(len(text.split()) for text in corpus)\n",
    "        total_chars = sum(len(text) for text in corpus)\n",
    "        \n",
    "        return {\n",
    "            'num_documents': len(corpus),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_chars': total_chars,\n",
    "            'avg_tokens_per_doc': total_tokens / len(corpus),\n",
    "            'avg_chars_per_doc': total_chars / len(corpus),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEMONSTRATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic time series.\n",
    "    \n",
    "    Components:\n",
    "        - Multi-scale sinusoidal trends\n",
    "        - Volatility clusters\n",
    "        - Random spikes\n",
    "        - Local corrections (creates nested events)\n",
    "    \n",
    "    Args:\n",
    "        B: Batch size (number of sequences)\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend (multiple scales)\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol_modulator = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol_modulator\n",
    "        \n",
    "        # Add random spikes\n",
    "        num_spikes = np.random.randint(2, 5)\n",
    "        spike_indices = torch.randint(50, L-50, (num_spikes,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(num_spikes) * 2\n",
    "        \n",
    "        # Add local correction (creates nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Dip in middle of uptrend\n",
    "            start = L // 2\n",
    "            end = start + 30\n",
    "            x[i, start:end] = x[i, start:end] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def demonstrate_system():\n",
    "    \"\"\"Run complete demonstration of the system\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"Demonstration\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    x = generate_synthetic_data(B, L)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: HIERARCHICAL STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTS BY HIERARCHICAL SCALE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate text in different formats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    for fmt in formats:\n",
    "        text = ann.to_text(format=fmt)\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"\\n{fmt.upper()}:\")\n",
    "        print(f\"  Tokens: {tokens}, Chars: {chars}\")\n",
    "        print(f\"  Preview: {text[:200]}...\")\n",
    "    \n",
    "    # Generate full corpus\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL CORPUS STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    corpus = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    stats = text_gen.estimate_tokens(corpus)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,.1f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThe system is ready for processing real time series data.\")\n",
    "    print(\"Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfac1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hierarchical_event_labeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mUSAGE GUIDE: Hierarchical Time Series Event Labeling System\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m============================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mQuick Start Guide and Common Use Cases\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhierarchical_event_labeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     HierarchicalEventDataset,\n\u001b[1;32m     11\u001b[0m     TextCorpusGenerator,\n\u001b[1;32m     12\u001b[0m     EventScale,\n\u001b[1;32m     13\u001b[0m     VOCAB\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# QUICK START\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquick_start_example\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hierarchical_event_labeling'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "USAGE GUIDE: Hierarchical Time Series Event Labeling System\n",
    "============================================================\n",
    "\n",
    "Quick Start Guide and Common Use Cases\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     TextCorpusGenerator,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_example():\n",
    "    \"\"\"Minimal example to get started\"\"\"\n",
    "    \n",
    "    # 1. Prepare your data as [B, L] tensor\n",
    "    B, L = 100, 336  # 100 sequences, each 336 timesteps\n",
    "    x = torch.randn(B, L)  # Replace with your real data\n",
    "    \n",
    "    # 2. Create dataset (this does all the processing)\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # 3. Access annotations\n",
    "    annotation = dataset[0]  # Get first sequence annotation\n",
    "    \n",
    "    # 4. Generate training text\n",
    "    text = annotation.to_text(format='depth_marked')\n",
    "    print(text)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_numpy():\n",
    "    \"\"\"Load from numpy arrays\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load your numpy data\n",
    "    data = np.load('your_data.npy')  # Shape: [num_samples, sequence_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_from_csv():\n",
    "    \"\"\"Load from CSV files\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Assuming each row is a sequence\n",
    "    data = df.values  # Shape: [num_sequences, sequence_length]\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_eeg_example():\n",
    "    \"\"\"Example for EEG data\"\"\"\n",
    "    \n",
    "    # Assuming you have EEG data: [num_trials, num_channels, time_points]\n",
    "    eeg_data = torch.randn(100, 64, 1000)  # Replace with real data\n",
    "    \n",
    "    # Process each channel separately\n",
    "    datasets = []\n",
    "    for channel in range(64):\n",
    "        channel_data = eeg_data[:, channel, :]  # [num_trials, time_points]\n",
    "        dataset = HierarchicalEventDataset(channel_data, verbose=False)\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORING ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def explore_annotation(dataset):\n",
    "    \"\"\"Explore annotation structure\"\"\"\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION EXPLORATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. View hierarchical structure\n",
    "    print(\"\\n1. HIERARCHICAL TREE:\")\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # 2. Get events at specific scale\n",
    "    print(\"\\n2. EVENTS BY SCALE:\")\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"{scale.name}: {len(events)} events\")\n",
    "    \n",
    "    # 3. Get events in time range\n",
    "    print(\"\\n3. EVENTS IN TIME RANGE [100-200]:\")\n",
    "    events = ann.get_events_in_range(100, 200)\n",
    "    for e in events:\n",
    "        print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name} ({e.scale.name})\")\n",
    "    \n",
    "    # 4. Access event metadata\n",
    "    print(\"\\n4. EVENT METADATA:\")\n",
    "    for event in ann.all_events[:5]:\n",
    "        print(f\"{event.label_name}: {event.metadata}\")\n",
    "    \n",
    "    # 5. Step-wise labels\n",
    "    print(f\"\\n5. STEP-WISE LABELS (first 20):\")\n",
    "    print(f\"IDs: {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"Names: {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_training_corpus(dataset, output_file='training_corpus.txt'):\n",
    "    \"\"\"Generate complete training corpus\"\"\"\n",
    "    \n",
    "    # Generate text for all sequences\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nGenerating {fmt} format...\")\n",
    "        corpus = text_gen.generate_corpus(dataset, format=fmt)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f'{fmt}_{output_file}'\n",
    "        with open(filename, 'w') as f:\n",
    "            for i, text in enumerate(corpus):\n",
    "                f.write(f\"<sequence_{i}>\\n{text}\\n</sequence_{i}>\\n\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = text_gen.estimate_tokens(corpus)\n",
    "        print(f\"Saved to {filename}\")\n",
    "        print(f\"  Documents: {stats['num_documents']}\")\n",
    "        print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "\n",
    "\n",
    "def create_autoregressive_pairs(dataset):\n",
    "    \"\"\"Create input-output pairs for autoregressive LM training\"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for ann in dataset.annotations:\n",
    "        # Get hierarchical text\n",
    "        full_text = ann.to_text(format='depth_marked')\n",
    "        tokens = full_text.split()\n",
    "        \n",
    "        # Create prefix-completion pairs\n",
    "        for i in range(1, len(tokens)):\n",
    "            input_text = \" \".join(tokens[:i])\n",
    "            target_token = tokens[i]\n",
    "            pairs.append((input_text, target_token))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTERING AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def filter_by_event_type(dataset, event_type='trend'):\n",
    "    \"\"\"Filter sequences by event type\"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        # Check if this annotation contains the event type\n",
    "        has_event = any(e.event_type == event_type for e in ann.all_events)\n",
    "        if has_event:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    print(f\"Found {len(filtered)} sequences with {event_type} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def analyze_event_statistics(dataset):\n",
    "    \"\"\"Compute detailed event statistics\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'total_sequences': len(dataset),\n",
    "        'events_by_type': {},\n",
    "        'events_by_scale': {},\n",
    "        'events_by_label': {},\n",
    "    }\n",
    "    \n",
    "    # Count events\n",
    "    for ann in dataset.annotations:\n",
    "        for event in ann.all_events:\n",
    "            # By type\n",
    "            stats['events_by_type'][event.event_type] = \\\n",
    "                stats['events_by_type'].get(event.event_type, 0) + 1\n",
    "            \n",
    "            # By scale\n",
    "            stats['events_by_scale'][event.scale.name] = \\\n",
    "                stats['events_by_scale'].get(event.scale.name, 0) + 1\n",
    "            \n",
    "            # By label\n",
    "            stats['events_by_label'][event.label_name] = \\\n",
    "                stats['events_by_label'].get(event.label_name, 0) + 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EVENT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nBy Event Type:\")\n",
    "    for event_type, count in sorted(stats['events_by_type'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {event_type:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nBy Scale:\")\n",
    "    for scale, count in sorted(stats['events_by_scale'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {scale:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Labels:\")\n",
    "    top_labels = sorted(stats['events_by_label'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for label, count in top_labels:\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {label:.<30} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TEXT FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "def create_custom_format(ann):\n",
    "    \"\"\"Create your own text format\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    parts.append(f\"SEQ_LEN:{len(ann.sequence)}\")\n",
    "    \n",
    "    # Add global regime\n",
    "    global_events = ann.get_events_at_scale(EventScale.GLOBAL)\n",
    "    if global_events:\n",
    "        parts.append(f\"REGIME:{global_events[0].label_name}\")\n",
    "    \n",
    "    # Add macro trends\n",
    "    macro_events = ann.get_events_at_scale(EventScale.MACRO)\n",
    "    parts.append(f\"TRENDS:{len(macro_events)}\")\n",
    "    for event in macro_events:\n",
    "        parts.append(f\"T[{event.start}-{event.end}]:{event.label_name}\")\n",
    "    \n",
    "    # Add peaks\n",
    "    peaks = [e for e in ann.all_events if e.event_type == 'peak']\n",
    "    parts.append(f\"PEAKS:{len(peaks)}\")\n",
    "    for peak in peaks:\n",
    "        parts.append(f\"P[{peak.start}]:{peak.label_name}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_large_dataset_in_batches(data_generator, batch_size=1000):\n",
    "    \"\"\"Process very large datasets in batches\"\"\"\n",
    "    \n",
    "    all_annotations = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(data_generator):\n",
    "        print(f\"\\nProcessing batch {batch_idx}...\")\n",
    "        \n",
    "        # Create dataset for this batch\n",
    "        dataset = HierarchicalEventDataset(batch_data, verbose=False)\n",
    "        \n",
    "        # Collect annotations\n",
    "        all_annotations.extend(dataset.annotations)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        torch.save(dataset.annotations, f'annotations_batch_{batch_idx}.pt')\n",
    "    \n",
    "    print(f\"\\nTotal annotations: {len(all_annotations)}\")\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH TRAINING PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "def create_pytorch_dataloader(dataset, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoader for training\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function for hierarchical annotations\"\"\"\n",
    "        sequences = torch.stack([ann.sequence for ann in batch])\n",
    "        step_labels = torch.stack([ann.step_labels for ann in batch])\n",
    "        \n",
    "        # Generate text representations\n",
    "        texts = [ann.to_text(format='depth_marked') for ann in batch]\n",
    "        \n",
    "        return {\n",
    "            'sequences': sequences,\n",
    "            'step_labels': step_labels,\n",
    "            'texts': texts,\n",
    "            'annotations': batch  # Keep full annotations if needed\n",
    "        }\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def prepare_for_huggingface(dataset, tokenizer):\n",
    "    \"\"\"Prepare data for HuggingFace transformers\"\"\"\n",
    "    \n",
    "    # Generate text corpus\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    texts = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE WORKFLOWS\n",
    "# ============================================================================\n",
    "\n",
    "def complete_workflow_example():\n",
    "    \"\"\"Complete end-to-end workflow\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Generate/Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    B, L = 100, 336\n",
    "    x = torch.randn(B, L)  # Replace with real data\n",
    "    \n",
    "    # 2. Create dataset\n",
    "    print(\"\\n[2/6] Creating hierarchical dataset...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    print(f\"  ✓ Processed {len(dataset)} sequences\")\n",
    "    \n",
    "    # 3. Explore one example\n",
    "    print(\"\\n[3/6] Exploring example annotation...\")\n",
    "    explore_annotation(dataset)\n",
    "    \n",
    "    # 4. Analyze statistics\n",
    "    print(\"\\n[4/6] Computing statistics...\")\n",
    "    stats = analyze_event_statistics(dataset)\n",
    "    \n",
    "    # 5. Generate training corpus\n",
    "    print(\"\\n[5/6] Generating training corpus...\")\n",
    "    generate_training_corpus(dataset, 'output_corpus.txt')\n",
    "    \n",
    "    # 6. Create DataLoader\n",
    "    print(\"\\n[6/6] Creating DataLoader...\")\n",
    "    dataloader = create_pytorch_dataloader(dataset, batch_size=16)\n",
    "    print(f\"  ✓ DataLoader ready with {len(dataloader)} batches\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete workflow\n",
    "    complete_workflow_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6c5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

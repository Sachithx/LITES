{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c471c1",
   "metadata": {},
   "source": [
    "Complete Time Series Event Labeling System\n",
    "==========================================\n",
    "\n",
    "Input:  [B, L] time series tensor\n",
    "Output: Rich event annotations at multiple scales\n",
    "\n",
    "Components:\n",
    "1. Multi-scale feature extraction\n",
    "2. Step-wise symbolic labels\n",
    "3. Trend segment detection  \n",
    "4. Peak/trough detection\n",
    "5. Volatility regime detection\n",
    "6. Change point detection\n",
    "7. Motif discovery\n",
    "8. Hierarchical event fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. COMPREHENSIVE VOCABULARY (110+ labels)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary with hierarchical structure\"\"\"\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    UNK = 2\n",
    "    \n",
    "    # Step-level movement (3-12)\n",
    "    FLAT = 3\n",
    "    UP_TINY = 4\n",
    "    UP_SMALL = 5\n",
    "    UP_MEDIUM = 6\n",
    "    UP_LARGE = 7\n",
    "    UP_HUGE = 8\n",
    "    DOWN_TINY = 9\n",
    "    DOWN_SMALL = 10\n",
    "    DOWN_MEDIUM = 11\n",
    "    DOWN_LARGE = 12\n",
    "    DOWN_HUGE = 13\n",
    "    \n",
    "    # Spikes (14-19)\n",
    "    SPIKE_UP_WEAK = 14\n",
    "    SPIKE_UP_STRONG = 15\n",
    "    SPIKE_UP_EXTREME = 16\n",
    "    SPIKE_DOWN_WEAK = 17\n",
    "    SPIKE_DOWN_STRONG = 18\n",
    "    SPIKE_DOWN_EXTREME = 19\n",
    "    \n",
    "    # Trend segments (20-39)\n",
    "    UPTREND_SHORT_WEAK = 20\n",
    "    UPTREND_SHORT_MODERATE = 21\n",
    "    UPTREND_SHORT_STRONG = 22\n",
    "    UPTREND_MEDIUM_WEAK = 23\n",
    "    UPTREND_MEDIUM_MODERATE = 24\n",
    "    UPTREND_MEDIUM_STRONG = 25\n",
    "    UPTREND_LONG_WEAK = 26\n",
    "    UPTREND_LONG_MODERATE = 27\n",
    "    UPTREND_LONG_STRONG = 28\n",
    "    ACCELERATING_UP = 29\n",
    "    DECELERATING_UP = 30\n",
    "    \n",
    "    DOWNTREND_SHORT_WEAK = 31\n",
    "    DOWNTREND_SHORT_MODERATE = 32\n",
    "    DOWNTREND_SHORT_STRONG = 33\n",
    "    DOWNTREND_MEDIUM_WEAK = 34\n",
    "    DOWNTREND_MEDIUM_MODERATE = 35\n",
    "    DOWNTREND_MEDIUM_STRONG = 36\n",
    "    DOWNTREND_LONG_WEAK = 37\n",
    "    DOWNTREND_LONG_MODERATE = 38\n",
    "    DOWNTREND_LONG_STRONG = 39\n",
    "    ACCELERATING_DOWN = 40\n",
    "    DECELERATING_DOWN = 41\n",
    "    \n",
    "    # Flat/stable (42-46)\n",
    "    FLAT_SHORT = 42\n",
    "    FLAT_MEDIUM = 43\n",
    "    FLAT_LONG = 44\n",
    "    PLATEAU = 45\n",
    "    STABLE_REGIME = 46\n",
    "    \n",
    "    # Peaks and troughs (47-62)\n",
    "    LOCAL_PEAK_WEAK = 47\n",
    "    LOCAL_PEAK_MODERATE = 48\n",
    "    LOCAL_PEAK_STRONG = 49\n",
    "    SHARP_PEAK = 50\n",
    "    BROAD_PEAK = 51\n",
    "    DOUBLE_TOP = 52\n",
    "    TRIPLE_TOP = 53\n",
    "    ROUND_TOP = 54\n",
    "    \n",
    "    LOCAL_TROUGH_WEAK = 55\n",
    "    LOCAL_TROUGH_MODERATE = 56\n",
    "    LOCAL_TROUGH_STRONG = 57\n",
    "    SHARP_TROUGH = 58\n",
    "    BROAD_TROUGH = 59\n",
    "    DOUBLE_BOTTOM = 60\n",
    "    TRIPLE_BOTTOM = 61\n",
    "    ROUND_BOTTOM = 62\n",
    "    \n",
    "    # Volatility regimes (63-72)\n",
    "    LOW_VOLATILITY = 63\n",
    "    NORMAL_VOLATILITY = 64\n",
    "    ELEVATED_VOLATILITY = 65\n",
    "    HIGH_VOLATILITY = 66\n",
    "    VOLATILITY_SPIKE = 67\n",
    "    VOLATILITY_CLUSTER = 68\n",
    "    CALM_PERIOD = 69\n",
    "    TURBULENT_PERIOD = 70\n",
    "    VOLATILITY_TRANSITION_UP = 71\n",
    "    VOLATILITY_TRANSITION_DOWN = 72\n",
    "    \n",
    "    # Oscillations (73-80)\n",
    "    OSCILLATION_REGULAR_SMALL = 73\n",
    "    OSCILLATION_REGULAR_MEDIUM = 74\n",
    "    OSCILLATION_REGULAR_LARGE = 75\n",
    "    OSCILLATION_IRREGULAR = 76\n",
    "    HIGH_FREQUENCY_NOISE = 77\n",
    "    CYCLIC_PATTERN = 78\n",
    "    DAMPENED_OSCILLATION = 79\n",
    "    AMPLIFYING_OSCILLATION = 80\n",
    "    \n",
    "    # Change points & regime shifts (81-90)\n",
    "    MEAN_SHIFT_UP = 81\n",
    "    MEAN_SHIFT_DOWN = 82\n",
    "    VARIANCE_INCREASE = 83\n",
    "    VARIANCE_DECREASE = 84\n",
    "    LEVEL_SHIFT_UP = 85\n",
    "    LEVEL_SHIFT_DOWN = 86\n",
    "    STRUCTURAL_BREAK = 87\n",
    "    REGIME_CHANGE = 88\n",
    "    TREND_REVERSAL = 89\n",
    "    SUDDEN_CHANGE = 90\n",
    "    \n",
    "    # Structural patterns (91-105)\n",
    "    RALLY_THEN_DROP = 91\n",
    "    DROP_THEN_RALLY = 92\n",
    "    UP_THEN_SIDEWAYS = 93\n",
    "    DOWN_THEN_SIDEWAYS = 94\n",
    "    STAIRCASE_UP = 95\n",
    "    STAIRCASE_DOWN = 96\n",
    "    V_SHAPE_RECOVERY = 97\n",
    "    INVERTED_V_SHAPE = 98\n",
    "    W_PATTERN = 99\n",
    "    M_PATTERN = 100\n",
    "    CUP_AND_HANDLE = 101\n",
    "    HEAD_AND_SHOULDERS = 102\n",
    "    ROUNDING_BOTTOM = 103\n",
    "    ROUNDING_TOP = 104\n",
    "    CONSOLIDATION = 105\n",
    "    \n",
    "    # Anomalies (106-112)\n",
    "    OUTLIER_HIGH = 106\n",
    "    OUTLIER_LOW = 107\n",
    "    RARE_EVENT = 108\n",
    "    DISCORD = 109\n",
    "    ANOMALOUS_SEGMENT = 110\n",
    "    UNEXPECTED_BEHAVIOR = 111\n",
    "    DATA_QUALITY_ISSUE = 112\n",
    "    \n",
    "    # Macro regimes (113-120)\n",
    "    BULLISH_REGIME = 113\n",
    "    BEARISH_REGIME = 114\n",
    "    SIDEWAYS_REGIME = 115\n",
    "    VOLATILE_REGIME = 116\n",
    "    TRENDING_REGIME = 117\n",
    "    MEAN_REVERTING_REGIME = 118\n",
    "    MOMENTUM_REGIME = 119\n",
    "    TRANSITION_REGIME = 120\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 121\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. MULTI-SCALE FEATURE EXTRACTOR (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50, 100]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First and second derivatives\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        d2x = torch.diff(dx, dim=1)  # [B, L-2]\n",
    "        features['d2x'] = F.pad(d2x, (2, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            # Prepare for conv1d: [B, 1, L]\n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            \n",
    "            # Rolling mean using conv1d\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w  # [1, 1, w]\n",
    "            \n",
    "            # Pad to maintain length\n",
    "            padding = w - 1\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_mean_3d = F.conv1d(x_padded, kernel)  # [B, 1, L]\n",
    "            rolling_mean = rolling_mean_3d.squeeze(1)  # [B, L]\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std (volatility)\n",
    "            # Compute (x - mean)^2 for each point\n",
    "            x_centered = x_3d - rolling_mean.unsqueeze(1)  # [B, 1, L]\n",
    "            x_centered_padded = F.pad(x_centered, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_var_3d = F.conv1d(x_centered_padded ** 2, kernel)  # [B, 1, L]\n",
    "            rolling_var = rolling_var_3d.squeeze(1)  # [B, L]\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (linear regression in window)\n",
    "            slopes = self._rolling_slope(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "            \n",
    "            # Energy (sum of absolute changes)\n",
    "            dx_abs = torch.abs(features['dx']).unsqueeze(1)  # [B, 1, L]\n",
    "            dx_abs_padded = F.pad(dx_abs, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            energy_3d = F.conv1d(dx_abs_padded, torch.ones(1, 1, w, device=device))  # [B, 1, L]\n",
    "            features[f'energy_{w}'] = energy_3d.squeeze(1)  # [B, L]\n",
    "        \n",
    "        # Z-scores for anomaly detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _rolling_slope(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Compute rolling linear regression slope\"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        slopes = torch.zeros(B, L, device=device)\n",
    "        \n",
    "        # Create time indices for regression\n",
    "        t = torch.arange(window, dtype=x.dtype, device=device)\n",
    "        t_mean = t.mean()\n",
    "        t_var = ((t - t_mean) ** 2).sum()\n",
    "        \n",
    "        # Use rolling window\n",
    "        for i in range(L):\n",
    "            start = max(0, i - window + 1)\n",
    "            end = i + 1\n",
    "            actual_window = end - start\n",
    "            \n",
    "            if actual_window < 3:  # Need minimum points for regression\n",
    "                continue\n",
    "            \n",
    "            # Get window data\n",
    "            window_data = x[:, start:end]  # [B, actual_window]\n",
    "            \n",
    "            # Time indices for this window\n",
    "            t_win = torch.arange(actual_window, dtype=x.dtype, device=device)\n",
    "            t_win_mean = t_win.mean()\n",
    "            t_win_var = ((t_win - t_win_mean) ** 2).sum()\n",
    "            \n",
    "            # Compute slope\n",
    "            x_mean = window_data.mean(dim=1, keepdim=True)\n",
    "            cov = ((t_win - t_win_mean) * (window_data - x_mean)).sum(dim=1)\n",
    "            slopes[:, i] = cov / (t_win_var + 1e-8)\n",
    "        \n",
    "        return slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc465bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. STEP-WISE SYMBOLIC ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep with symbolic labels\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, L]\n",
    "        features: dict from MultiScaleFeatureExtractor\n",
    "        Returns: [B, L] step labels\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Compute global quantiles for thresholding\n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padding\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles: 20%, 40%, 60%, 80%, 95%\n",
    "        quantiles = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.20, 0.40, 0.60, 0.80, 0.95], device=device)\n",
    "        )\n",
    "        q20, q40, q60, q80, q95 = quantiles\n",
    "        \n",
    "        epsilon = 0.1 * q20  # Flat threshold\n",
    "        \n",
    "        # Initialize with PAD\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Process t >= 1\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q20), t] = VOCAB.UP_TINY\n",
    "            labels[up_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.UP_HUGE\n",
    "            \n",
    "            # Extreme spikes\n",
    "            labels[up_mask & (abs_diff > q95), t] = VOCAB.SPIKE_UP_EXTREME\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q20), t] = VOCAB.DOWN_TINY\n",
    "            labels[down_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.DOWN_HUGE\n",
    "            labels[down_mask & (abs_diff > q95), t] = VOCAB.SPIKE_DOWN_EXTREME\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0488be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. TREND SEGMENT DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrendSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    direction: str  # 'up', 'down', 'flat'\n",
    "    slope: float\n",
    "    strength: str  # 'weak', 'moderate', 'strong'\n",
    "    label: int\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"Detect trend segments using piecewise linear approximation\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[TrendSegment]:\n",
    "        \"\"\"\n",
    "        Detect trends for a single sequence\n",
    "        x: [L] single sequence\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        # Use rolling slope features\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            # Fallback: compute simple slopes\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find runs of same sign\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        # Detect sign changes\n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 3:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            # Segment statistics\n",
    "            seg_slope = slopes[start:end].mean()\n",
    "            seg_std = slopes[start:end].std()\n",
    "            duration = end - start\n",
    "            \n",
    "            # Classify direction\n",
    "            if abs(seg_slope) < 0.01:\n",
    "                direction = 'flat'\n",
    "                label = self._classify_flat(duration)\n",
    "            elif seg_slope > 0:\n",
    "                direction = 'up'\n",
    "                strength, accel = self._classify_strength(seg_slope, slopes[start:end])\n",
    "                label = self._classify_uptrend(duration, strength, accel)\n",
    "            else:\n",
    "                direction = 'down'\n",
    "                strength, accel = self._classify_strength(abs(seg_slope), slopes[start:end])\n",
    "                label = self._classify_downtrend(duration, strength, accel)\n",
    "            \n",
    "            segments.append(TrendSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                direction=direction,\n",
    "                slope=seg_slope,\n",
    "                strength=strength if direction != 'flat' else 'n/a',\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _classify_strength(self, avg_slope: float, slopes: np.ndarray):\n",
    "        \"\"\"Classify trend strength and acceleration\"\"\"\n",
    "        if avg_slope < 0.05:\n",
    "            strength = 'weak'\n",
    "        elif avg_slope < 0.15:\n",
    "            strength = 'moderate'\n",
    "        else:\n",
    "            strength = 'strong'\n",
    "        \n",
    "        # Check acceleration\n",
    "        mid = len(slopes) // 2\n",
    "        slope_first_half = slopes[:mid].mean()\n",
    "        slope_second_half = slopes[mid:].mean()\n",
    "        \n",
    "        if abs(slope_second_half) > abs(slope_first_half) * 1.2:\n",
    "            accel = 'accelerating'\n",
    "        elif abs(slope_second_half) < abs(slope_first_half) * 0.8:\n",
    "            accel = 'decelerating'\n",
    "        else:\n",
    "            accel = 'steady'\n",
    "        \n",
    "        return strength, accel\n",
    "    \n",
    "    def _classify_uptrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for uptrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_UP\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_UP\n",
    "        \n",
    "        # Duration classification\n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        # Strength\n",
    "        str_type = strength.upper()\n",
    "        \n",
    "        # Map to vocabulary\n",
    "        label_name = f\"UPTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.UPTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_downtrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for downtrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_DOWN\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_DOWN\n",
    "        \n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        str_type = strength.upper()\n",
    "        label_name = f\"DOWNTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.DOWNTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_flat(self, duration: int) -> int:\n",
    "        \"\"\"Get label for flat segment\"\"\"\n",
    "        if duration < 15:\n",
    "            return VOCAB.FLAT_SHORT\n",
    "        elif duration < 40:\n",
    "            return VOCAB.FLAT_MEDIUM\n",
    "        else:\n",
    "            return VOCAB.FLAT_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. PEAK/TROUGH DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakTroughEvent:\n",
    "    index: int\n",
    "    type: str  # 'peak' or 'trough'\n",
    "    prominence: float\n",
    "    label: int\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"Detect peaks and troughs using scipy\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[PeakTroughEvent]:\n",
    "        \"\"\"Detect peaks/troughs in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Detect peaks\n",
    "        peaks, properties = scipy_signal.find_peaks(\n",
    "            x_np, \n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(peaks, properties['prominences']):\n",
    "            label = self._classify_peak(prom, properties['widths'][list(peaks).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='peak',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Detect troughs (peaks of inverted signal)\n",
    "        troughs, properties = scipy_signal.find_peaks(\n",
    "            -x_np,\n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(troughs, properties['prominences']):\n",
    "            label = self._classify_trough(prom, properties['widths'][list(troughs).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='trough',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Sort by index\n",
    "        events.sort(key=lambda e: e.index)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _classify_peak(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify peak type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_PEAK_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_PEAK\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_PEAK\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_TOP\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_STRONG\n",
    "    \n",
    "    def _classify_trough(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify trough type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_TROUGH_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_TROUGH\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_TROUGH\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_BOTTOM\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_STRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. VOLATILITY REGIME DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class VolatilityRegime:\n",
    "    start: int\n",
    "    end: int\n",
    "    level: str  # 'low', 'normal', 'elevated', 'high'\n",
    "    avg_vol: float\n",
    "    label: int\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"Detect volatility regimes\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[VolatilityRegime]:\n",
    "        \"\"\"Detect volatility regimes in single sequence\"\"\"\n",
    "        \n",
    "        # Use rolling std\n",
    "        if 'std_20' in features:\n",
    "            vol = features['std_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantiles\n",
    "        q25, q50, q75, q90 = np.percentile(vol, [25, 50, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q50)] = 1  # normal\n",
    "        vol_levels[(vol > q50) & (vol <= q75)] = 2  # elevated\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 3  # high\n",
    "        vol_levels[vol > q90] = 4  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end].mean()\n",
    "            \n",
    "            # Map to label\n",
    "            if level_code == 0:\n",
    "                label = VOCAB.LOW_VOLATILITY\n",
    "                level = 'low'\n",
    "            elif level_code == 1:\n",
    "                label = VOCAB.NORMAL_VOLATILITY\n",
    "                level = 'normal'\n",
    "            elif level_code == 2:\n",
    "                label = VOCAB.ELEVATED_VOLATILITY\n",
    "                level = 'elevated'\n",
    "            elif level_code == 3:\n",
    "                label = VOCAB.HIGH_VOLATILITY\n",
    "                level = 'high'\n",
    "            else:\n",
    "                label = VOCAB.VOLATILITY_SPIKE\n",
    "                level = 'spike'\n",
    "            \n",
    "            regimes.append(VolatilityRegime(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                level=level,\n",
    "                avg_vol=float(avg_vol),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return regimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. CHANGE POINT DETECTOR (CUSUM-based)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ChangePoint:\n",
    "    index: int\n",
    "    type: str  # 'mean_shift_up', 'mean_shift_down', etc.\n",
    "    magnitude: float\n",
    "    label: int\n",
    "\n",
    "class ChangePointDetector:\n",
    "    \"\"\"Detect change points using CUSUM\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=5.0, drift=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.drift = drift\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[ChangePoint]:\n",
    "        \"\"\"Detect change points in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "        \n",
    "        # CUSUM for mean shifts\n",
    "        cusum_pos = np.zeros(L)\n",
    "        cusum_neg = np.zeros(L)\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            cusum_pos[i] = max(0, cusum_pos[i-1] + x_norm[i] - self.drift)\n",
    "            cusum_neg[i] = min(0, cusum_neg[i-1] + x_norm[i] + self.drift)\n",
    "        \n",
    "        # Find threshold crossings\n",
    "        change_points = []\n",
    "        \n",
    "        # Upward shifts\n",
    "        up_crossings = np.where(cusum_pos > self.threshold)[0]\n",
    "        if len(up_crossings) > 0:\n",
    "            # Group nearby crossings\n",
    "            groups = self._group_nearby(up_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = cusum_pos[idx]\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_up',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_UP\n",
    "                ))\n",
    "        \n",
    "        # Downward shifts\n",
    "        down_crossings = np.where(cusum_neg < -self.threshold)[0]\n",
    "        if len(down_crossings) > 0:\n",
    "            groups = self._group_nearby(down_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = abs(cusum_neg[idx])\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_down',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_DOWN\n",
    "                ))\n",
    "        \n",
    "        # Sort by index\n",
    "        change_points.sort(key=lambda cp: cp.index)\n",
    "        \n",
    "        return change_points\n",
    "    \n",
    "    def _group_nearby(self, indices: np.ndarray, gap: int = 10) -> List[List[int]]:\n",
    "        \"\"\"Group indices that are close together\"\"\"\n",
    "        if len(indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        groups = []\n",
    "        current_group = [indices[0]]\n",
    "        \n",
    "        for idx in indices[1:]:\n",
    "            if idx - current_group[-1] <= gap:\n",
    "                current_group.append(idx)\n",
    "            else:\n",
    "                groups.append(current_group)\n",
    "                current_group = [idx]\n",
    "        \n",
    "        groups.append(current_group)\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. COMPREHENSIVE EVENT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ComprehensiveAnnotation:\n",
    "    \"\"\"Complete annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    trend_segments: List[TrendSegment]\n",
    "    peaks_troughs: List[PeakTroughEvent]\n",
    "    volatility_regimes: List[VolatilityRegime]\n",
    "    change_points: List[ChangePoint]\n",
    "    \n",
    "    def to_event_sequence(self, max_events: int = 100) -> List[Dict]:\n",
    "        \"\"\"Convert to flat event list sorted by start index\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        # Add trends\n",
    "        for seg in self.trend_segments:\n",
    "            events.append({\n",
    "                'start': seg.start,\n",
    "                'end': seg.end,\n",
    "                'type': 'trend',\n",
    "                'label': seg.label,\n",
    "                'label_name': VOCAB.id_to_label(seg.label),\n",
    "                'metadata': {\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pt in self.peaks_troughs:\n",
    "            events.append({\n",
    "                'start': pt.index,\n",
    "                'end': pt.index,\n",
    "                'type': 'peak_trough',\n",
    "                'label': pt.label,\n",
    "                'label_name': VOCAB.id_to_label(pt.label),\n",
    "                'metadata': {\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in self.volatility_regimes:\n",
    "            events.append({\n",
    "                'start': vr.start,\n",
    "                'end': vr.end,\n",
    "                'type': 'volatility',\n",
    "                'label': vr.label,\n",
    "                'label_name': VOCAB.id_to_label(vr.label),\n",
    "                'metadata': {\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add change points\n",
    "        for cp in self.change_points:\n",
    "            events.append({\n",
    "                'start': cp.index,\n",
    "                'end': cp.index,\n",
    "                'type': 'change_point',\n",
    "                'label': cp.label,\n",
    "                'label_name': VOCAB.id_to_label(cp.label),\n",
    "                'metadata': {\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort by start index\n",
    "        events.sort(key=lambda e: e['start'])\n",
    "        \n",
    "        # Limit to max_events\n",
    "        return events[:max_events]\n",
    "    \n",
    "    def to_text_description(self) -> str:\n",
    "        \"\"\"Generate natural language description\"\"\"\n",
    "        events = self.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        parts.append(f\"Sequence length: {len(self.sequence)}\")\n",
    "        parts.append(f\"Total events detected: {len(events)}\")\n",
    "        \n",
    "        for event in events:\n",
    "            if event['start'] == event['end']:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "            else:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}-{event['end']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "\n",
    "class ComprehensiveEventDataset(Dataset):\n",
    "    \"\"\"Complete dataset with all event types\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize detectors\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Process each sequence for higher-level events\n",
    "        if verbose:\n",
    "            print(\"Detecting trends, peaks, volatility, change points...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            vol_regimes = self.vol_detector.detect(x[i], self.features, i)\n",
    "            change_pts = self.cp_detector.detect(x[i], i)\n",
    "            \n",
    "            annotation = ComprehensiveAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                trend_segments=trends,\n",
    "                peaks_troughs=peaks,\n",
    "                volatility_regimes=vol_regimes,\n",
    "                change_points=change_pts\n",
    "            )\n",
    "            \n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Dataset ready with {len(self.annotations)} annotated sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return comprehensive annotation\"\"\"\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Compute dataset statistics\"\"\"\n",
    "        total_trends = sum(len(ann.trend_segments) for ann in self.annotations)\n",
    "        total_peaks = sum(len(ann.peaks_troughs) for ann in self.annotations)\n",
    "        total_vol = sum(len(ann.volatility_regimes) for ann in self.annotations)\n",
    "        total_cp = sum(len(ann.change_points) for ann in self.annotations)\n",
    "        \n",
    "        avg_trends = total_trends / len(self.annotations)\n",
    "        avg_peaks = total_peaks / len(self.annotations)\n",
    "        avg_vol = total_vol / len(self.annotations)\n",
    "        avg_cp = total_cp / len(self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'avg_trend_segments': avg_trends,\n",
    "            'avg_peaks_troughs': avg_peaks,\n",
    "            'avg_volatility_regimes': avg_vol,\n",
    "            'avg_change_points': avg_cp,\n",
    "            'avg_total_events': avg_trends + avg_peaks + avg_vol + avg_cp,\n",
    "            'vocab_size': VOCAB.get_vocab_size()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. TEXT CORPUS GENERATOR FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"Generate text corpus for language model training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_structured_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Structured format (token-efficient)\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            if e['start'] == e['end']:\n",
    "                tokens.append(f\"[{e['start']}]{e['label_name']}\")\n",
    "            else:\n",
    "                tokens.append(f\"[{e['start']}-{e['end']}]{e['label_name']}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_hybrid_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Hybrid: structured + metadata\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        for e in events:\n",
    "            span = f\"[{e['start']}-{e['end']}]\" if e['start'] != e['end'] else f\"[{e['start']}]\"\n",
    "            metadata_str = \" \".join(f\"{k}={v}\" for k, v in e['metadata'].items())\n",
    "            parts.append(f\"{span} {e['label_name']} ({metadata_str})\")\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Natural language narrative\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        L = len(annotation.sequence)\n",
    "        \n",
    "        sentences = [f\"Time series of length {L}.\"]\n",
    "        \n",
    "        # Group by type\n",
    "        trends = [e for e in events if e['type'] == 'trend']\n",
    "        peaks = [e for e in events if e['type'] == 'peak_trough']\n",
    "        \n",
    "        if trends:\n",
    "            sentences.append(f\"Contains {len(trends)} trend segments:\")\n",
    "            for t in trends[:3]:  # Limit verbosity\n",
    "                sentences.append(\n",
    "                    f\"  From {t['start']} to {t['end']}, \"\n",
    "                    f\"{t['label_name'].lower().replace('_', ' ')} observed.\"\n",
    "                )\n",
    "        \n",
    "        if peaks:\n",
    "            sentences.append(f\"Notable peaks/troughs: {len(peaks)} detected.\")\n",
    "        \n",
    "        return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96750473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10. EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process batch of EEG-like signals\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    B = 100  # batch size\n",
    "    L = 336  # sequence length\n",
    "    \n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    \n",
    "    # Create realistic time series (trending + noise + spikes)\n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Add occasional spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create comprehensive dataset\n",
    "    dataset = ComprehensiveEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Get statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    stats = dataset.get_statistics()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key:.<40} {value:.2f}\")\n",
    "    \n",
    "    # Example: Get annotation for first sequence\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE ANNOTATION (First Sequence)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(f\"\\nStep labels (first 20): {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"  -> {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "    \n",
    "    print(f\"\\nTrend segments detected: {len(ann.trend_segments)}\")\n",
    "    for i, seg in enumerate(ann.trend_segments[:5]):\n",
    "        print(f\"  {i+1}. [{seg.start}-{seg.end}] {VOCAB.id_to_label(seg.label)} \"\n",
    "              f\"(slope={seg.slope:.3f})\")\n",
    "    \n",
    "    print(f\"\\nPeaks/Troughs detected: {len(ann.peaks_troughs)}\")\n",
    "    for i, pt in enumerate(ann.peaks_troughs[:5]):\n",
    "        print(f\"  {i+1}. [{pt.index}] {VOCAB.id_to_label(pt.label)} \"\n",
    "              f\"(prominence={pt.prominence:.3f})\")\n",
    "    \n",
    "    print(f\"\\nVolatility regimes: {len(ann.volatility_regimes)}\")\n",
    "    for i, vr in enumerate(ann.volatility_regimes):\n",
    "        print(f\"  {i+1}. [{vr.start}-{vr.end}] {VOCAB.id_to_label(vr.label)} \"\n",
    "              f\"(avg_vol={vr.avg_vol:.3f})\")\n",
    "    \n",
    "    print(f\"\\nChange points: {len(ann.change_points)}\")\n",
    "    for i, cp in enumerate(ann.change_points):\n",
    "        print(f\"  {i+1}. [{cp.index}] {VOCAB.id_to_label(cp.label)} \"\n",
    "              f\"(magnitude={cp.magnitude:.3f})\")\n",
    "    \n",
    "    # Generate text descriptions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    print(\"\\n1. STRUCTURED FORMAT:\")\n",
    "    print(text_gen.generate_structured_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. HYBRID FORMAT:\")\n",
    "    print(text_gen.generate_hybrid_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. NARRATIVE FORMAT:\")\n",
    "    print(text_gen.generate_narrative_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count estimation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNT ESTIMATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    structured = text_gen.generate_structured_text(ann)\n",
    "    hybrid = text_gen.generate_hybrid_text(ann)\n",
    "    \n",
    "    # Simple token count (split by whitespace)\n",
    "    print(f\"Structured format: ~{len(structured.split())} tokens\")\n",
    "    print(f\"Hybrid format:     ~{len(hybrid.split())} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ“ System demonstration complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT SYSTEM (Add to existing code)\n",
    "# ============================================================================\n",
    "\n",
    "from typing import List, Dict, Optional, Set\n",
    "from dataclasses import dataclass, field\n",
    "from enum import IntEnum\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # Single points, spikes (1-5 steps)\n",
    "    MINI = 2       # Very short segments (5-15 steps)\n",
    "    MESO = 3       # Medium segments (15-50 steps)\n",
    "    MACRO = 4      # Long segments (50-150 steps)\n",
    "    GLOBAL = 5     # Full sequence level (150+ steps)\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str  # 'trend', 'volatility', 'peak', 'changepoint', etc.\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    \n",
    "    # Hierarchical relationships\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event contains another\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def overlaps(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if events overlap (but neither contains the other)\"\"\"\n",
    "        if self.contains(other) or other.contains(self):\n",
    "            return False\n",
    "        return not (self.end < other.start or self.start > other.end)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start}-{self.end}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical event tree from flat event list\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, \n",
    "                  event_type: str, confidence: float = 1.0, \n",
    "                  metadata: Optional[Dict] = None):\n",
    "        \"\"\"Add event to the collection\"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine scale based on duration\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start,\n",
    "            end=end,\n",
    "            label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale,\n",
    "            event_type=event_type,\n",
    "            confidence=confidence,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Build parent-child relationships\"\"\"\n",
    "        \n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(\n",
    "            self.events,\n",
    "            key=lambda e: (-e.scale, e.start, -e.duration)\n",
    "        )\n",
    "        \n",
    "        # Build hierarchy tree\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            # Find the smallest containing event as parent\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            \n",
    "            if parent is None:\n",
    "                # This is a root node\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                # Add as child to parent\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children by start position within each parent\n",
    "        self._sort_children(roots)\n",
    "        \n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent, \n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for potential_parent in all_events:\n",
    "            if potential_parent == event:\n",
    "                continue\n",
    "            \n",
    "            # Must be larger scale\n",
    "            if potential_parent.scale <= event.scale:\n",
    "                continue\n",
    "            \n",
    "            # Must contain the event\n",
    "            if potential_parent.contains(event):\n",
    "                candidates.append(potential_parent)\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        # Return the smallest containing event (most specific parent)\n",
    "        return min(candidates, key=lambda e: e.duration)\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: (c.start, -c.duration))\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list of all events (depth-first traversal)\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED COMPREHENSIVE ANNOTATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"Complete hierarchical annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    \n",
    "    # Hierarchical event tree\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]  # Flattened\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchy\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Event Tree (Total: {len(self.all_events)} events)\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events that overlap with range [start, end]\"\"\"\n",
    "        result = []\n",
    "        for event in self.all_events:\n",
    "            if not (event.end < start or event.start > end):\n",
    "                result.append(event)\n",
    "        return result\n",
    "    \n",
    "    def to_hierarchical_text(self, format: str = 'structured') -> str:\n",
    "        \"\"\"Generate hierarchical text representation\"\"\"\n",
    "        if format == 'structured':\n",
    "            return self._structured_hierarchical_text()\n",
    "        elif format == 'indented':\n",
    "            return self._indented_hierarchical_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_hierarchical_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _structured_hierarchical_text(self) -> str:\n",
    "        \"\"\"Compact structured format with depth indicators\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_marker}[{node.start}-{node.end}]\"\n",
    "                f\"{node.label_name}\"\n",
    "                f\"@{node.scale.name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _indented_hierarchical_text(self) -> str:\n",
    "        \"\"\"Human-readable indented format\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            indent = \"  \" * node.depth\n",
    "            lines.append(\n",
    "                f\"{indent}[{node.start:03d}-{node.end:03d}] \"\n",
    "                f\"{node.label_name} \"\n",
    "                f\"(scale={node.scale.name}, conf={node.confidence:.2f})\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _narrative_hierarchical_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with macro view\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        \n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall, the sequence exhibits {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        if macro_events:\n",
    "            sentences.append(f\"At the macro level, {len(macro_events)} major segments:\")\n",
    "            for event in macro_events[:3]:\n",
    "                sentences.append(\n",
    "                    f\"  From {event.start} to {event.end}, \"\n",
    "                    f\"{event.label_name.lower().replace('_', ' ')}\"\n",
    "                )\n",
    "                \n",
    "                # Mention nested events\n",
    "                if event.children:\n",
    "                    nested_types = set(c.event_type for c in event.children)\n",
    "                    sentences.append(\n",
    "                        f\"    (contains {len(event.children)} nested events: \"\n",
    "                        f\"{', '.join(nested_types)})\"\n",
    "                    )\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED DATASET WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Dataset with full hierarchical event structure\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L} with hierarchical structure...\")\n",
    "        \n",
    "        # Initialize all detectors (reuse from previous implementation)\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build hierarchical annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical event structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_hierarchical_annotation(i)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Dataset ready with hierarchical structure\")\n",
    "            self._print_statistics()\n",
    "    \n",
    "    def _build_hierarchical_annotation(self, idx: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # 1. Detect all events (as before)\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        change_pts = self.cp_detector.detect(self.x[idx], idx)\n",
    "        \n",
    "        # 2. Add trend segments to builder\n",
    "        for seg in trends:\n",
    "            builder.add_event(\n",
    "                start=seg.start,\n",
    "                end=seg.end,\n",
    "                label=seg.label,\n",
    "                event_type='trend',\n",
    "                confidence=0.9,\n",
    "                metadata={\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 3. Add peaks/troughs\n",
    "        for pt in peaks:\n",
    "            builder.add_event(\n",
    "                start=pt.index,\n",
    "                end=pt.index,\n",
    "                label=pt.label,\n",
    "                event_type='peak_trough',\n",
    "                confidence=0.85,\n",
    "                metadata={\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 4. Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(\n",
    "                start=vr.start,\n",
    "                end=vr.end,\n",
    "                label=vr.label,\n",
    "                event_type='volatility',\n",
    "                confidence=0.8,\n",
    "                metadata={\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 5. Add change points\n",
    "        for cp in change_pts:\n",
    "            builder.add_event(\n",
    "                start=cp.index,\n",
    "                end=cp.index,\n",
    "                label=cp.label,\n",
    "                event_type='changepoint',\n",
    "                confidence=0.75,\n",
    "                metadata={\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 6. Add global regime (macro-level characterization)\n",
    "        L = len(self.x[idx])\n",
    "        global_regime = self._classify_global_regime(idx, L)\n",
    "        builder.add_event(\n",
    "            start=0,\n",
    "            end=L-1,\n",
    "            label=global_regime,\n",
    "            event_type='global_regime',\n",
    "            confidence=0.7,\n",
    "            metadata={'scope': 'global'}\n",
    "        )\n",
    "        \n",
    "        # 7. Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int, length: int) -> int:\n",
    "        \"\"\"Classify overall regime for entire sequence\"\"\"\n",
    "        # Use net slope across entire sequence\n",
    "        if 'slope_100' in self.features:\n",
    "            avg_slope = self.features['slope_100'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        # Use average volatility\n",
    "        if 'std_20' in self.features:\n",
    "            avg_vol = self.features['std_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_vol = 0.5\n",
    "        \n",
    "        # Classify\n",
    "        if avg_vol > 1.5:\n",
    "            return VOCAB.VOLATILE_REGIME\n",
    "        elif avg_slope > 0.1:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.1:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print hierarchical statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"HIERARCHICAL STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Count events by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        total_events = 0\n",
    "        max_depth = 0\n",
    "        \n",
    "        for ann in self.annotations:\n",
    "            total_events += len(ann.all_events)\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "                max_depth = max(max_depth, event.depth)\n",
    "        \n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        print(f\"Sequences: {len(self.annotations)}\")\n",
    "        print(f\"Avg events per sequence: {avg_events:.1f}\")\n",
    "        print(f\"Max hierarchy depth: {max_depth}\")\n",
    "        print(f\"\\nEvents by scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"  {scale.name:.<20} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalTextGenerator:\n",
    "    \"\"\"Generate training text that preserves hierarchical structure\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_flat_sequential(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Flat list sorted by start (loses hierarchy)\"\"\"\n",
    "        events = sorted(ann.all_events, key=lambda e: e.start)\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            tokens.append(f\"[{e.start}-{e.end}]{e.label_name}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_depth_marked(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Include depth markers to indicate nesting\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_str = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_str}[{node.start}-{node.end}]{node.label_name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_xml_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"XML-like nested structure\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent, depth: int = 0):\n",
    "            indent = \"  \" * depth\n",
    "            if node.children:\n",
    "                lines.append(f\"{indent}<event type='{node.label_name}' span='{node.start}-{node.end}'>\")\n",
    "                for child in node.children:\n",
    "                    traverse(child, depth + 1)\n",
    "                lines.append(f\"{indent}</event>\")\n",
    "            else:\n",
    "                lines.append(\n",
    "                    f\"{indent}<event type='{node.label_name}' \"\n",
    "                    f\"span='{node.start}-{node.end}' />\"\n",
    "                )\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_json_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"JSON-like hierarchical structure\"\"\"\n",
    "        import json\n",
    "        \n",
    "        def event_to_dict(node: HierarchicalEvent) -> Dict:\n",
    "            result = {\n",
    "                'span': [node.start, node.end],\n",
    "                'label': node.label_name,\n",
    "                'scale': node.scale.name,\n",
    "                'confidence': node.confidence\n",
    "            }\n",
    "            if node.children:\n",
    "                result['children'] = [event_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree = [event_to_dict(root) for root in ann.event_roots]\n",
    "        return json.dumps(tree, separators=(',', ':'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_with_context(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Natural language that mentions parent context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            # Describe the event\n",
    "            if node.start == node.end:\n",
    "                span_desc = f\"At position {node.start}\"\n",
    "            else:\n",
    "                span_desc = f\"From {node.start} to {node.end}\"\n",
    "            \n",
    "            event_desc = node.label_name.lower().replace('_', ' ')\n",
    "            \n",
    "            # Add parent context if exists\n",
    "            if node.parent:\n",
    "                parent_desc = node.parent.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(\n",
    "                    f\"{span_desc}, {event_desc} occurs \"\n",
    "                    f\"(within {parent_desc} [{node.parent.start}-{node.parent.end}]).\"\n",
    "                )\n",
    "            else:\n",
    "                sentences.append(f\"{span_desc}, {event_desc}.\")\n",
    "            \n",
    "            # Recurse\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED EXAMPLE WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate test data\n",
    "    B = 50\n",
    "    L = 336\n",
    "    \n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Multi-scale trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        # Local corrections (create nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Add dip in middle of uptrend\n",
    "            x[i, 150:180] = x[i, 150:180] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create hierarchical dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Example annotation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HIERARCHICAL STRUCTURE EXAMPLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVENTS BY SCALE\")\n",
    "    print(\"=\" * 80)\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate different text formats\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FORMATS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = HierarchicalTextGenerator()\n",
    "    \n",
    "    print(\"\\n1. DEPTH-MARKED FORMAT:\")\n",
    "    print(text_gen.generate_depth_marked(ann)[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. NARRATIVE WITH CONTEXT:\")\n",
    "    narrative = text_gen.generate_narrative_with_context(ann)\n",
    "    print(narrative[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. JSON-STYLE (first 500 chars):\")\n",
    "    print(text_gen.generate_json_style(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNTS BY FORMAT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    formats = {\n",
    "        'Flat sequential': text_gen.generate_flat_sequential(ann),\n",
    "        'Depth-marked': text_gen.generate_depth_marked(ann),\n",
    "        'Narrative': text_gen.generate_narrative_with_context(ann),\n",
    "        'JSON-style': text_gen.generate_json_style(ann)\n",
    "    }\n",
    "    \n",
    "    for name, text in formats.items():\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"{name:.<30} {tokens:>6} tokens, {chars:>6} chars\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ“ Hierarchical system complete!\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## ðŸŽ¯ **What's New in This Version**\n",
    "\n",
    "### **1. True Hierarchical Structure** âœ…\n",
    "```\n",
    "[0-335] SIDEWAYS_REGIME (GLOBAL)\n",
    "  â”œâ”€ [0-120] UPTREND_LONG_MODERATE (MACRO)\n",
    "  â”‚   â”œâ”€ [30-45] DOWNTREND_SHORT_WEAK (MESO)  â† NESTED!\n",
    "  â”‚   â”‚   â””â”€ [38] SPIKE_DOWN_STRONG (MICRO)\n",
    "  â”‚   â””â”€ [50-55] VOLATILITY_SPIKE (MINI)\n",
    "  â”œâ”€ [121-200] FLAT_LONG (MACRO)\n",
    "  â””â”€ [201-335] DOWNTREND_LONG_STRONG (MACRO)\n",
    "      â””â”€ [250-265] LOCAL_PEAK_MODERATE (MESO)\n",
    "```\n",
    "\n",
    "### **2. Event Scale Classification**\n",
    "- **MICRO** (1-5 steps): Spikes, single peaks\n",
    "- **MINI** (5-15 steps): Very short segments\n",
    "- **MESO** (15-50 steps): Medium trends, local corrections\n",
    "- **MACRO** (50-150 steps): Major trends\n",
    "- **GLOBAL** (150+ steps): Overall regime\n",
    "\n",
    "### **3. Parent-Child Relationships**\n",
    "- Automatic detection of containment\n",
    "- Smallest containing event becomes parent\n",
    "- Children sorted by start position\n",
    "\n",
    "### **4. Multiple Text Formats**\n",
    "\n",
    "**Depth-Marked (Token-Efficient):**\n",
    "```\n",
    "[0-335]SIDEWAYS_REGIME@GLOBAL >[0-120]UPTREND_LONG_MODERATE@MACRO >>[30-45]DOWNTREND_SHORT_WEAK@MESO >>>[38]SPIKE_DOWN_STRONG@MICRO\n",
    "```\n",
    "\n",
    "**Narrative with Context:**\n",
    "```\n",
    "From 0 to 335, sideways regime. From 0 to 120, uptrend long moderate. \n",
    "From 30 to 45, downtrend short weak (within uptrend long moderate [0-120]). \n",
    "At position 38, spike down strong (within downtrend short weak [30-45]). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPLETE HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "========================================================\n",
    "Fixed and production-ready version\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VOCABULARY (same as before)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary\"\"\"\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trends\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HIERARCHICAL STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # 1-5 steps\n",
    "    MINI = 2       # 5-15 steps\n",
    "    MESO = 3       # 15-50 steps\n",
    "    MACRO = 4      # 50-150 steps\n",
    "    GLOBAL = 5     # 150+ steps\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FIXED FEATURE EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales - FIXED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First derivative\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Simple rolling slope\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Simple slope computation\"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            # Slope from start to end of window\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "# ============================================================================\n",
    "# 4. STEP ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep - SIMPLIFIED\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Classify\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SIMPLE DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "class SimpleTrendDetector:\n",
    "    \"\"\"Simplified trend detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Use slope if available\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "class SimplePeakDetector:\n",
    "    \"\"\"Simplified peak detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(x_np, prominence=0.2)\n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                label = VOCAB.SHARP_PEAK if prom > 0.5 else VOCAB.LOCAL_PEAK\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(-x_np, prominence=0.2)\n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                label = VOCAB.SHARP_TROUGH if prom > 0.5 else VOCAB.LOCAL_TROUGH\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return events\n",
    "\n",
    "# ============================================================================\n",
    "# 6. HIERARCHY BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# 7. HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"Generate hierarchical text\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Main dataset class - FIXED\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = SimpleTrendDetector()\n",
    "        self.peak_detector = SimplePeakDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode steps\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            builder = HierarchicalEventBuilder()\n",
    "            \n",
    "            # Detect trends\n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            for seg in trends:\n",
    "                builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                                metadata=seg.metadata)\n",
    "            \n",
    "            # Detect peaks\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            for pk in peaks:\n",
    "                builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                                metadata=pk.metadata)\n",
    "            \n",
    "            # Add global regime\n",
    "            if 'slope_20' in self.features:\n",
    "                avg_slope = self.features['slope_20'][i].mean().item()\n",
    "            else:\n",
    "                avg_slope = 0\n",
    "            \n",
    "            if avg_slope > 0.05:\n",
    "                global_label = VOCAB.BULLISH_REGIME\n",
    "            elif avg_slope < -0.05:\n",
    "                global_label = VOCAB.BEARISH_REGIME\n",
    "            else:\n",
    "                global_label = VOCAB.SIDEWAYS_REGIME\n",
    "            \n",
    "            builder.add_event(0, L-1, global_label, 'regime')\n",
    "            \n",
    "            # Build hierarchy\n",
    "            roots = builder.build_hierarchy()\n",
    "            all_events = builder.get_flat_list(roots)\n",
    "            \n",
    "            self.annotations.append(HierarchicalAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                event_roots=roots,\n",
    "                all_events=all_events\n",
    "            ))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Complete! Avg events per sequence: {self._avg_events():.1f}\")\n",
    "    \n",
    "    def _avg_events(self):\n",
    "        return sum(len(a.all_events) for a in self.annotations) / len(self.annotations)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# 9. TEST\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL EVENT LABELING SYSTEM - FIXED VERSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        noise = 0.1 * torch.randn(L)\n",
    "        spikes = torch.zeros(L)\n",
    "        spike_idx = torch.randint(50, L-50, (2,))\n",
    "        spikes[spike_idx] = torch.randn(2) * 2\n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE HIERARCHICAL ANNOTATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT OUTPUT\")\n",
    "    print(\"=\" * 80)\n",
    "    text = ann.to_text()\n",
    "    print(text[:500] + \"...\")\n",
    "    print(f\"\\nTotal tokens: ~{len(text.split())}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ“ System working correctly!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf68f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "===============================================\n",
    "\n",
    "A comprehensive system for detecting and labeling events in time series data\n",
    "with hierarchical structure preservation.\n",
    "\n",
    "Workflow:\n",
    "    1. Define vocabulary and hierarchical structures\n",
    "    2. Extract multi-scale features from raw time series\n",
    "    3. Encode step-wise labels for each timestep\n",
    "    4. Detect higher-level events (trends, peaks, volatility, change points)\n",
    "    5. Build hierarchical event tree\n",
    "    6. Generate training text in various formats\n",
    "\n",
    "Author: Sachith\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels for events\"\"\"\n",
    "    MICRO = 1      # 1-5 timesteps (spikes, single points)\n",
    "    MINI = 2       # 5-15 timesteps (very short segments)\n",
    "    MESO = 3       # 15-50 timesteps (medium segments, local patterns)\n",
    "    MACRO = 4      # 50-150 timesteps (major trends)\n",
    "    GLOBAL = 5     # 150+ timesteps (full sequence characteristics)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"\n",
    "    Complete event vocabulary with 64 distinct labels.\n",
    "    \n",
    "    Categories:\n",
    "        - Special tokens (0-2)\n",
    "        - Step movements (3-10)\n",
    "        - Trend segments (20-26)\n",
    "        - Peaks/troughs (30-33)\n",
    "        - Volatility regimes (40-43)\n",
    "        - Change points (50-51)\n",
    "        - Global regimes (60-63)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    \n",
    "    # Step-level movements\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trend segments\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks and troughs\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility regimes\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Global regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls) -> int:\n",
    "        \"\"\"Return total vocabulary size\"\"\"\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        \"\"\"Convert label ID to string name\"\"\"\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"\n",
    "    Event node in hierarchical tree structure.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting timestep index\n",
    "        end: Ending timestep index\n",
    "        label: Vocabulary ID\n",
    "        label_name: Human-readable label\n",
    "        scale: Hierarchical scale level\n",
    "        event_type: Category (trend/peak/volatility/changepoint/regime)\n",
    "        confidence: Detection confidence score\n",
    "        metadata: Additional event-specific information\n",
    "        parent: Parent event in hierarchy (None for root)\n",
    "        children: List of child events\n",
    "    \"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        \"\"\"Duration in timesteps\"\"\"\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy tree (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event fully contains another event\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "\n",
    "# Global vocabulary instance\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features at multiple temporal scales using efficient convolutions.\n",
    "    \n",
    "    Features computed:\n",
    "        - First derivative (dx)\n",
    "        - Rolling mean at multiple window sizes\n",
    "        - Rolling standard deviation (volatility)\n",
    "        - Rolling slope (trend strength)\n",
    "        - Z-scores for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        scales: List of window sizes for rolling features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scales: List[int] = [5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from time series batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor of shape [B, L]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature tensors, each shape [B, L]:\n",
    "                - 'dx': First derivative\n",
    "                - 'mean_{w}': Rolling mean with window w\n",
    "                - 'std_{w}': Rolling std with window w\n",
    "                - 'slope_{w}': Rolling slope with window w\n",
    "                - 'zscore': Normalized z-scores\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        features = {}\n",
    "        \n",
    "        # First derivative (rate of change)\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean using efficient convolution\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling standard deviation (volatility measure)\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (trend direction and strength)\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores for outlier detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rolling linear slope over window.\n",
    "        \n",
    "        Simple implementation: slope = (end - start) / window\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STEP-WISE LABEL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"\n",
    "    Encode each timestep with symbolic movement labels.\n",
    "    \n",
    "    Labels based on magnitude of first derivative:\n",
    "        - FLAT: negligible change\n",
    "        - UP/DOWN_SMALL/MEDIUM/LARGE: quantile-based magnitude bins\n",
    "        - SPIKE_UP/DOWN: extreme changes (>90th percentile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode step-wise movement labels for entire batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor [B, L]\n",
    "            features: Feature dictionary from MultiScaleFeatureExtractor\n",
    "        \n",
    "        Returns:\n",
    "            Label tensor [B, L] with vocabulary IDs\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padded value\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Compute quantiles for adaptive thresholding\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33  # Flat threshold\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat (negligible change)\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: EVENT DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    \"\"\"Simple segment representation for detector outputs\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"\n",
    "    Detect trend segments using slope sign changes.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Compute rolling slopes\n",
    "        2. Find sign changes (trend reversals)\n",
    "        3. Classify segments by direction and duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect trend segments in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of trend segments\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Get slopes\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend direction changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify segment\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs using scipy's find_peaks with proper filtering.\n",
    "    \n",
    "    Key improvements:\n",
    "        - Minimum distance enforcement (prevent adjacent peaks)\n",
    "        - Adaptive prominence thresholds\n",
    "        - Peak-trough alternation validation\n",
    "    \n",
    "    Classifies peaks/troughs by:\n",
    "        - Prominence: How much the peak stands out\n",
    "        - Type: Sharp vs broad based on width\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_distance: int = 10, min_prominence_percentile: float = 75):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_distance: Minimum timesteps between peaks/troughs\n",
    "            min_prominence_percentile: Percentile for adaptive prominence threshold\n",
    "        \"\"\"\n",
    "        self.min_distance = min_distance\n",
    "        self.min_prominence_percentile = min_prominence_percentile\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            idx: Sequence index (unused but kept for consistency)\n",
    "        \n",
    "        Returns:\n",
    "            List of peak/trough events (alternating peaks and troughs)\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        \n",
    "        # Compute adaptive prominence threshold\n",
    "        std = np.std(x_np)\n",
    "        min_prominence = max(0.2 * std, 0.1)  # At least 0.2 std or 0.1 absolute\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(\n",
    "                x_np, \n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_PEAK\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_PEAK\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'peak'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs (peaks of inverted signal)\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(\n",
    "                -x_np,\n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_TROUGH\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_TROUGH\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'trough'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Sort by position and validate alternation\n",
    "        events = self._validate_alternation(events)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _validate_alternation(self, events: List[SimpleSegment]) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Ensure peaks and troughs alternate (remove consecutive same-type events).\n",
    "        Keep the more prominent one when there are consecutive same types.\n",
    "        \"\"\"\n",
    "        if len(events) <= 1:\n",
    "            return events\n",
    "        \n",
    "        # Sort by position\n",
    "        events.sort(key=lambda e: e.start)\n",
    "        \n",
    "        filtered = [events[0]]\n",
    "        \n",
    "        for event in events[1:]:\n",
    "            last_event = filtered[-1]\n",
    "            \n",
    "            # Check if types alternate\n",
    "            last_type = last_event.metadata.get('type')\n",
    "            curr_type = event.metadata.get('type')\n",
    "            \n",
    "            if last_type == curr_type:\n",
    "                # Same type consecutive - keep more prominent\n",
    "                if event.metadata['prominence'] > last_event.metadata['prominence']:\n",
    "                    filtered[-1] = event  # Replace with more prominent\n",
    "                # else: keep the existing one\n",
    "            else:\n",
    "                # Different types - check minimum distance\n",
    "                if event.start - last_event.start >= self.min_distance // 2:\n",
    "                    filtered.append(event)\n",
    "                # else: skip (too close even if different types)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect volatility regimes using rolling standard deviation.\n",
    "    \n",
    "    Classifies regimes by quantile thresholds:\n",
    "        - LOW: Below 25th percentile\n",
    "        - NORMAL: 25th-75th percentile\n",
    "        - HIGH: Above 75th percentile\n",
    "        - SPIKE: Above 90th percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect volatility regimes in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of volatility regime segments\n",
    "        \"\"\"\n",
    "        if 'std_20' not in features:\n",
    "            return []\n",
    "        \n",
    "        vol = features['std_20'][idx].cpu().numpy()\n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantile thresholds\n",
    "        q25, q75, q90 = np.percentile(vol, [25, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q75)] = 1  # normal\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 2  # high\n",
    "        vol_levels[vol > q90] = 3  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end+1].mean()\n",
    "            \n",
    "            # Map to vocabulary\n",
    "            label_map = {\n",
    "                0: VOCAB.LOW_VOLATILITY,\n",
    "                1: VOCAB.NORMAL_VOLATILITY,\n",
    "                2: VOCAB.HIGH_VOLATILITY,\n",
    "                3: VOCAB.VOLATILITY_SPIKE\n",
    "            }\n",
    "            \n",
    "            regimes.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label_map[level_code],\n",
    "                metadata={'avg_volatility': float(avg_vol)}\n",
    "            ))\n",
    "        \n",
    "        return regimes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: HIERARCHICAL STRUCTURE BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"\n",
    "    Build hierarchical event tree from flat event list.\n",
    "    \n",
    "    Process:\n",
    "        1. Classify each event's scale based on duration\n",
    "        2. Sort events by scale (largest first)\n",
    "        3. Build parent-child relationships via containment\n",
    "        4. Sort children by temporal order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[HierarchicalEvent] = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add event to collection with automatic scale classification.\n",
    "        \n",
    "        Args:\n",
    "            start: Starting timestep\n",
    "            end: Ending timestep\n",
    "            label: Vocabulary ID\n",
    "            event_type: Category string\n",
    "            confidence: Detection confidence score\n",
    "            metadata: Additional information dictionary\n",
    "        \"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine hierarchical scale\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"\n",
    "        Build hierarchical tree structure.\n",
    "        \n",
    "        Returns:\n",
    "            List of root events (events with no parent)\n",
    "        \"\"\"\n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        # Build parent-child relationships\n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children within each parent\n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event (most specific parent)\"\"\"\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list via depth-first traversal\"\"\"\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"\n",
    "    Complete hierarchical annotation for one sequence.\n",
    "    \n",
    "    Attributes:\n",
    "        sequence: Original time series [L]\n",
    "        step_labels: Step-wise labels [L]\n",
    "        event_roots: Root nodes of hierarchy tree\n",
    "        all_events: Flattened list of all events\n",
    "    \"\"\"\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchical structure\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific hierarchical scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events overlapping with time range\"\"\"\n",
    "        return [e for e in self.all_events \n",
    "                if not (e.end < start or e.start > end)]\n",
    "    \n",
    "    def to_text(self, format: str = 'depth_marked') -> str:\n",
    "        \"\"\"\n",
    "        Generate text representation.\n",
    "        \n",
    "        Args:\n",
    "            format: Output format\n",
    "                - 'depth_marked': Depth indicators with events\n",
    "                - 'flat': Simple sequential list\n",
    "                - 'narrative': Natural language description\n",
    "        \n",
    "        Returns:\n",
    "            Text string for language model training\n",
    "        \"\"\"\n",
    "        if format == 'depth_marked':\n",
    "            return self._depth_marked_text()\n",
    "        elif format == 'flat':\n",
    "            return self._flat_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _depth_marked_text(self) -> str:\n",
    "        \"\"\"Depth markers indicate nesting: > >> >>> etc.\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _flat_text(self) -> str:\n",
    "        \"\"\"Simple sequential list (loses hierarchy)\"\"\"\n",
    "        events = sorted(self.all_events, key=lambda e: e.start)\n",
    "        return \" \".join(f\"[{e.start}-{e.end}]{e.label_name}\" for e in events)\n",
    "    \n",
    "    def _narrative_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with global view\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall: {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        # Describe macro events\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        if macro_events:\n",
    "            sentences.append(f\"{len(macro_events)} major segments detected.\")\n",
    "            for event in macro_events[:3]:\n",
    "                desc = event.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(f\"[{event.start}-{event.end}]: {desc}\")\n",
    "                if event.children:\n",
    "                    nested = \", \".join(set(c.event_type for c in event.children))\n",
    "                    sentences.append(f\"  (contains: {nested})\")\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MAIN DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Main dataset class for hierarchical event labeling.\n",
    "    \n",
    "    Processing pipeline:\n",
    "        1. Extract multi-scale features\n",
    "        2. Encode step-wise labels\n",
    "        3. Detect events (trends, peaks, volatility)\n",
    "        4. Add global regime classification\n",
    "        5. Build hierarchical structure\n",
    "        6. Create annotations\n",
    "    \n",
    "    Args:\n",
    "        x: Time series tensor [B, L]\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INITIALIZING HIERARCHICAL EVENT DATASET\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Sequences: {B}\")\n",
    "            print(f\"Length: {L}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        \n",
    "        # STEP 1: Extract features\n",
    "        if verbose:\n",
    "            print(f\"\\n[1/4] Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        if verbose:\n",
    "            print(f\"      âœ“ Computed {len(self.features)} feature types\")\n",
    "        \n",
    "        # STEP 2: Encode step labels\n",
    "        if verbose:\n",
    "            print(f\"[2/4] Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        if verbose:\n",
    "            print(f\"      âœ“ Encoded {B * L} timesteps\")\n",
    "        \n",
    "        # STEP 3: Detect events and build hierarchy\n",
    "        if verbose:\n",
    "            print(f\"[3/4] Detecting events and building hierarchy...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"      Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_annotation(i, L)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        # STEP 4: Compute statistics\n",
    "        if verbose:\n",
    "            print(f\"[4/4] Computing statistics...\")\n",
    "            self._print_statistics()\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"âœ“ DATASET READY\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _build_annotation(self, idx: int, L: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # Detect all event types\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        \n",
    "        # Add trend segments\n",
    "        for seg in trends:\n",
    "            builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                            confidence=0.9, metadata=seg.metadata)\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pk in peaks:\n",
    "            builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                            confidence=0.85, metadata=pk.metadata)\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(vr.start, vr.end, vr.label, 'volatility',\n",
    "                            confidence=0.8, metadata=vr.metadata)\n",
    "        \n",
    "        # Add global regime\n",
    "        global_label = self._classify_global_regime(idx)\n",
    "        builder.add_event(0, L-1, global_label, 'regime', confidence=0.7)\n",
    "        \n",
    "        # Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int) -> int:\n",
    "        \"\"\"Classify overall sequence regime\"\"\"\n",
    "        if 'slope_20' in self.features:\n",
    "            avg_slope = self.features['slope_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        if avg_slope > 0.05:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.05:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        # Count by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        for ann in self.annotations:\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "        \n",
    "        print(f\"      Total events: {total_events}\")\n",
    "        print(f\"      Avg per sequence: {avg_events:.1f}\")\n",
    "        print(f\"      By scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"        {scale.name:.<12} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'vocab_size': VOCAB.get_vocab_size(),\n",
    "            'total_events': total_events,\n",
    "            'avg_events_per_sequence': total_events / len(self.annotations),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TEXT GENERATION FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"\n",
    "    Generate training text in various formats.\n",
    "    \n",
    "    Formats:\n",
    "        - depth_marked: Hierarchical with depth indicators (>)\n",
    "        - flat: Simple sequential list\n",
    "        - narrative: Natural language description\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_corpus(dataset: HierarchicalEventDataset, \n",
    "                       format: str = 'depth_marked') -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text corpus for all sequences.\n",
    "        \n",
    "        Args:\n",
    "            dataset: HierarchicalEventDataset instance\n",
    "            format: Text format\n",
    "        \n",
    "        Returns:\n",
    "            List of text strings, one per sequence\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for annotation in dataset.annotations:\n",
    "            text = annotation.to_text(format=format)\n",
    "            corpus.append(text)\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(corpus: List[str]) -> Dict:\n",
    "        \"\"\"Estimate token counts for corpus\"\"\"\n",
    "        total_tokens = sum(len(text.split()) for text in corpus)\n",
    "        total_chars = sum(len(text) for text in corpus)\n",
    "        \n",
    "        return {\n",
    "            'num_documents': len(corpus),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_chars': total_chars,\n",
    "            'avg_tokens_per_doc': total_tokens / len(corpus),\n",
    "            'avg_chars_per_doc': total_chars / len(corpus),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEMONSTRATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic time series.\n",
    "    \n",
    "    Components:\n",
    "        - Multi-scale sinusoidal trends\n",
    "        - Volatility clusters\n",
    "        - Random spikes\n",
    "        - Local corrections (creates nested events)\n",
    "    \n",
    "    Args:\n",
    "        B: Batch size (number of sequences)\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend (multiple scales)\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol_modulator = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol_modulator\n",
    "        \n",
    "        # Add random spikes\n",
    "        num_spikes = np.random.randint(2, 5)\n",
    "        spike_indices = torch.randint(50, L-50, (num_spikes,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(num_spikes) * 2\n",
    "        \n",
    "        # Add local correction (creates nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Dip in middle of uptrend\n",
    "            start = L // 2\n",
    "            end = start + 30\n",
    "            x[i, start:end] = x[i, start:end] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def demonstrate_system():\n",
    "    \"\"\"Run complete demonstration of the system\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"Demonstration\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    x = generate_synthetic_data(B, L)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: HIERARCHICAL STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTS BY HIERARCHICAL SCALE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate text in different formats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    for fmt in formats:\n",
    "        text = ann.to_text(format=fmt)\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"\\n{fmt.upper()}:\")\n",
    "        print(f\"  Tokens: {tokens}, Chars: {chars}\")\n",
    "        print(f\"  Preview: {text[:200]}...\")\n",
    "    \n",
    "    # Generate full corpus\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL CORPUS STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    corpus = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    stats = text_gen.estimate_tokens(corpus)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,.1f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThe system is ready for processing real time series data.\")\n",
    "    print(\"Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USAGE GUIDE: Hierarchical Time Series Event Labeling System\n",
    "============================================================\n",
    "\n",
    "Quick Start Guide and Common Use Cases\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     TextCorpusGenerator,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_example():\n",
    "    \"\"\"Minimal example to get started\"\"\"\n",
    "    \n",
    "    # 1. Prepare your data as [B, L] tensor\n",
    "    B, L = 100, 336  # 100 sequences, each 336 timesteps\n",
    "    x = torch.randn(B, L)  # Replace with your real data\n",
    "    \n",
    "    # 2. Create dataset (this does all the processing)\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # 3. Access annotations\n",
    "    annotation = dataset[0]  # Get first sequence annotation\n",
    "    \n",
    "    # 4. Generate training text\n",
    "    text = annotation.to_text(format='depth_marked')\n",
    "    print(text)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_numpy():\n",
    "    \"\"\"Load from numpy arrays\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load your numpy data\n",
    "    data = np.load('your_data.npy')  # Shape: [num_samples, sequence_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_from_csv():\n",
    "    \"\"\"Load from CSV files\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Assuming each row is a sequence\n",
    "    data = df.values  # Shape: [num_sequences, sequence_length]\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_eeg_example():\n",
    "    \"\"\"Example for EEG data\"\"\"\n",
    "    \n",
    "    # Assuming you have EEG data: [num_trials, num_channels, time_points]\n",
    "    eeg_data = torch.randn(100, 64, 1000)  # Replace with real data\n",
    "    \n",
    "    # Process each channel separately\n",
    "    datasets = []\n",
    "    for channel in range(64):\n",
    "        channel_data = eeg_data[:, channel, :]  # [num_trials, time_points]\n",
    "        dataset = HierarchicalEventDataset(channel_data, verbose=False)\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORING ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def explore_annotation(dataset):\n",
    "    \"\"\"Explore annotation structure\"\"\"\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION EXPLORATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. View hierarchical structure\n",
    "    print(\"\\n1. HIERARCHICAL TREE:\")\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # 2. Get events at specific scale\n",
    "    print(\"\\n2. EVENTS BY SCALE:\")\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"{scale.name}: {len(events)} events\")\n",
    "    \n",
    "    # 3. Get events in time range\n",
    "    print(\"\\n3. EVENTS IN TIME RANGE [100-200]:\")\n",
    "    events = ann.get_events_in_range(100, 200)\n",
    "    for e in events:\n",
    "        print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name} ({e.scale.name})\")\n",
    "    \n",
    "    # 4. Access event metadata\n",
    "    print(\"\\n4. EVENT METADATA:\")\n",
    "    for event in ann.all_events[:5]:\n",
    "        print(f\"{event.label_name}: {event.metadata}\")\n",
    "    \n",
    "    # 5. Step-wise labels\n",
    "    print(f\"\\n5. STEP-WISE LABELS (first 20):\")\n",
    "    print(f\"IDs: {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"Names: {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_training_corpus(dataset, output_file='training_corpus.txt'):\n",
    "    \"\"\"Generate complete training corpus\"\"\"\n",
    "    \n",
    "    # Generate text for all sequences\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nGenerating {fmt} format...\")\n",
    "        corpus = text_gen.generate_corpus(dataset, format=fmt)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f'{fmt}_{output_file}'\n",
    "        with open(filename, 'w') as f:\n",
    "            for i, text in enumerate(corpus):\n",
    "                f.write(f\"<sequence_{i}>\\n{text}\\n</sequence_{i}>\\n\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = text_gen.estimate_tokens(corpus)\n",
    "        print(f\"Saved to {filename}\")\n",
    "        print(f\"  Documents: {stats['num_documents']}\")\n",
    "        print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "\n",
    "\n",
    "def create_autoregressive_pairs(dataset):\n",
    "    \"\"\"Create input-output pairs for autoregressive LM training\"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for ann in dataset.annotations:\n",
    "        # Get hierarchical text\n",
    "        full_text = ann.to_text(format='depth_marked')\n",
    "        tokens = full_text.split()\n",
    "        \n",
    "        # Create prefix-completion pairs\n",
    "        for i in range(1, len(tokens)):\n",
    "            input_text = \" \".join(tokens[:i])\n",
    "            target_token = tokens[i]\n",
    "            pairs.append((input_text, target_token))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTERING AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def filter_by_event_type(dataset, event_type='trend'):\n",
    "    \"\"\"Filter sequences by event type\"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        # Check if this annotation contains the event type\n",
    "        has_event = any(e.event_type == event_type for e in ann.all_events)\n",
    "        if has_event:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    print(f\"Found {len(filtered)} sequences with {event_type} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def analyze_event_statistics(dataset):\n",
    "    \"\"\"Compute detailed event statistics\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'total_sequences': len(dataset),\n",
    "        'events_by_type': {},\n",
    "        'events_by_scale': {},\n",
    "        'events_by_label': {},\n",
    "    }\n",
    "    \n",
    "    # Count events\n",
    "    for ann in dataset.annotations:\n",
    "        for event in ann.all_events:\n",
    "            # By type\n",
    "            stats['events_by_type'][event.event_type] = \\\n",
    "                stats['events_by_type'].get(event.event_type, 0) + 1\n",
    "            \n",
    "            # By scale\n",
    "            stats['events_by_scale'][event.scale.name] = \\\n",
    "                stats['events_by_scale'].get(event.scale.name, 0) + 1\n",
    "            \n",
    "            # By label\n",
    "            stats['events_by_label'][event.label_name] = \\\n",
    "                stats['events_by_label'].get(event.label_name, 0) + 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EVENT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nBy Event Type:\")\n",
    "    for event_type, count in sorted(stats['events_by_type'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {event_type:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nBy Scale:\")\n",
    "    for scale, count in sorted(stats['events_by_scale'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {scale:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Labels:\")\n",
    "    top_labels = sorted(stats['events_by_label'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for label, count in top_labels:\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {label:.<30} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TEXT FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "def create_custom_format(ann):\n",
    "    \"\"\"Create your own text format\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    parts.append(f\"SEQ_LEN:{len(ann.sequence)}\")\n",
    "    \n",
    "    # Add global regime\n",
    "    global_events = ann.get_events_at_scale(EventScale.GLOBAL)\n",
    "    if global_events:\n",
    "        parts.append(f\"REGIME:{global_events[0].label_name}\")\n",
    "    \n",
    "    # Add macro trends\n",
    "    macro_events = ann.get_events_at_scale(EventScale.MACRO)\n",
    "    parts.append(f\"TRENDS:{len(macro_events)}\")\n",
    "    for event in macro_events:\n",
    "        parts.append(f\"T[{event.start}-{event.end}]:{event.label_name}\")\n",
    "    \n",
    "    # Add peaks\n",
    "    peaks = [e for e in ann.all_events if e.event_type == 'peak']\n",
    "    parts.append(f\"PEAKS:{len(peaks)}\")\n",
    "    for peak in peaks:\n",
    "        parts.append(f\"P[{peak.start}]:{peak.label_name}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_large_dataset_in_batches(data_generator, batch_size=1000):\n",
    "    \"\"\"Process very large datasets in batches\"\"\"\n",
    "    \n",
    "    all_annotations = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(data_generator):\n",
    "        print(f\"\\nProcessing batch {batch_idx}...\")\n",
    "        \n",
    "        # Create dataset for this batch\n",
    "        dataset = HierarchicalEventDataset(batch_data, verbose=False)\n",
    "        \n",
    "        # Collect annotations\n",
    "        all_annotations.extend(dataset.annotations)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        torch.save(dataset.annotations, f'annotations_batch_{batch_idx}.pt')\n",
    "    \n",
    "    print(f\"\\nTotal annotations: {len(all_annotations)}\")\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH TRAINING PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "def create_pytorch_dataloader(dataset, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoader for training\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function for hierarchical annotations\"\"\"\n",
    "        sequences = torch.stack([ann.sequence for ann in batch])\n",
    "        step_labels = torch.stack([ann.step_labels for ann in batch])\n",
    "        \n",
    "        # Generate text representations\n",
    "        texts = [ann.to_text(format='depth_marked') for ann in batch]\n",
    "        \n",
    "        return {\n",
    "            'sequences': sequences,\n",
    "            'step_labels': step_labels,\n",
    "            'texts': texts,\n",
    "            'annotations': batch  # Keep full annotations if needed\n",
    "        }\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def prepare_for_huggingface(dataset, tokenizer):\n",
    "    \"\"\"Prepare data for HuggingFace transformers\"\"\"\n",
    "    \n",
    "    # Generate text corpus\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    texts = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE WORKFLOWS\n",
    "# ============================================================================\n",
    "\n",
    "def complete_workflow_example():\n",
    "    \"\"\"Complete end-to-end workflow\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Generate/Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    B, L = 100, 336\n",
    "    x = torch.randn(B, L)  # Replace with real data\n",
    "    \n",
    "    # 2. Create dataset\n",
    "    print(\"\\n[2/6] Creating hierarchical dataset...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    print(f\"  âœ“ Processed {len(dataset)} sequences\")\n",
    "    \n",
    "    # 3. Explore one example\n",
    "    print(\"\\n[3/6] Exploring example annotation...\")\n",
    "    explore_annotation(dataset)\n",
    "    \n",
    "    # 4. Analyze statistics\n",
    "    print(\"\\n[4/6] Computing statistics...\")\n",
    "    stats = analyze_event_statistics(dataset)\n",
    "    \n",
    "    # 5. Generate training corpus\n",
    "    print(\"\\n[5/6] Generating training corpus...\")\n",
    "    generate_training_corpus(dataset, 'output_corpus.txt')\n",
    "    \n",
    "    # 6. Create DataLoader\n",
    "    print(\"\\n[6/6] Creating DataLoader...\")\n",
    "    dataloader = create_pytorch_dataloader(dataset, batch_size=16)\n",
    "    print(f\"  âœ“ DataLoader ready with {len(dataloader)} batches\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete workflow\n",
    "    complete_workflow_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "\n",
    "# 1. Prepare your data [batch_size, sequence_length]\n",
    "x = torch.randn(100, 512)  # 100 sequences, 336 timesteps each\n",
    "\n",
    "# 2. Create dataset (this does all processing)\n",
    "dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "\n",
    "# 3. Get annotation for first sequence\n",
    "ann = dataset[0]\n",
    "\n",
    "# 4. View hierarchical structure\n",
    "ann.print_hierarchy()\n",
    "\n",
    "# 5. Generate training text\n",
    "text = ann.to_text(format='depth_marked')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3119d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae54a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0412b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cb084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce52cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa23a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785adb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd03730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a325e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5157001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4117019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "===============================================\n",
    "\n",
    "A comprehensive system for detecting and labeling events in time series data\n",
    "with hierarchical structure preservation.\n",
    "\n",
    "Workflow:\n",
    "    1. Define vocabulary and hierarchical structures\n",
    "    2. Extract multi-scale features from raw time series\n",
    "    3. Encode step-wise labels for each timestep\n",
    "    4. Detect higher-level events (trends, peaks, volatility, change points)\n",
    "    5. Build hierarchical event tree\n",
    "    6. Generate training text in various formats\n",
    "\n",
    "Author: Sachith\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels for events\"\"\"\n",
    "    MICRO = 1      # 1-5 timesteps (spikes, single points)\n",
    "    MINI = 2       # 5-15 timesteps (very short segments)\n",
    "    MESO = 3       # 15-50 timesteps (medium segments, local patterns)\n",
    "    MACRO = 4      # 50-150 timesteps (major trends)\n",
    "    GLOBAL = 5     # 150+ timesteps (full sequence characteristics)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"\n",
    "    Complete event vocabulary with 64 distinct labels.\n",
    "    \n",
    "    Categories:\n",
    "        - Special tokens (0-2)\n",
    "        - Step movements (3-10)\n",
    "        - Trend segments (20-26)\n",
    "        - Peaks/troughs (30-33)\n",
    "        - Volatility regimes (40-43)\n",
    "        - Change points (50-51)\n",
    "        - Global regimes (60-63)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    \n",
    "    # Step-level movements\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trend segments\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks and troughs\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility regimes\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Global regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls) -> int:\n",
    "        \"\"\"Return total vocabulary size\"\"\"\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        \"\"\"Convert label ID to string name\"\"\"\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"\n",
    "    Event node in hierarchical tree structure.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting timestep index\n",
    "        end: Ending timestep index\n",
    "        label: Vocabulary ID\n",
    "        label_name: Human-readable label\n",
    "        scale: Hierarchical scale level\n",
    "        event_type: Category (trend/peak/volatility/changepoint/regime)\n",
    "        confidence: Detection confidence score\n",
    "        metadata: Additional event-specific information\n",
    "        parent: Parent event in hierarchy (None for root)\n",
    "        children: List of child events\n",
    "    \"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        \"\"\"Duration in timesteps\"\"\"\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy tree (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event fully contains another event\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "\n",
    "# Global vocabulary instance\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features at multiple temporal scales using efficient convolutions.\n",
    "    \n",
    "    Features computed:\n",
    "        - First derivative (dx)\n",
    "        - Rolling mean at multiple window sizes\n",
    "        - Rolling standard deviation (volatility)\n",
    "        - Rolling slope (trend strength)\n",
    "        - Z-scores for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        scales: List of window sizes for rolling features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scales: List[int] = [5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from time series batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor of shape [B, L]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature tensors, each shape [B, L]:\n",
    "                - 'dx': First derivative\n",
    "                - 'mean_{w}': Rolling mean with window w\n",
    "                - 'std_{w}': Rolling std with window w\n",
    "                - 'slope_{w}': Rolling slope with window w\n",
    "                - 'zscore': Normalized z-scores\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        features = {}\n",
    "        \n",
    "        # First derivative (rate of change)\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean using efficient convolution\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling standard deviation (volatility measure)\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (trend direction and strength)\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores for outlier detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rolling linear slope over window.\n",
    "        \n",
    "        Simple implementation: slope = (end - start) / window\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STEP-WISE LABEL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"\n",
    "    Encode each timestep with symbolic movement labels.\n",
    "    \n",
    "    Labels based on magnitude of first derivative:\n",
    "        - FLAT: negligible change\n",
    "        - UP/DOWN_SMALL/MEDIUM/LARGE: quantile-based magnitude bins\n",
    "        - SPIKE_UP/DOWN: extreme changes (>90th percentile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode step-wise movement labels for entire batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor [B, L]\n",
    "            features: Feature dictionary from MultiScaleFeatureExtractor\n",
    "        \n",
    "        Returns:\n",
    "            Label tensor [B, L] with vocabulary IDs\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padded value\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Compute quantiles for adaptive thresholding\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33  # Flat threshold\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat (negligible change)\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: EVENT DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    \"\"\"Simple segment representation for detector outputs\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"\n",
    "    Detect trend segments using slope sign changes.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Compute rolling slopes\n",
    "        2. Find sign changes (trend reversals)\n",
    "        3. Classify segments by direction and duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect trend segments in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of trend segments\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Get slopes\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend direction changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify segment\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs using scipy's find_peaks with proper filtering.\n",
    "    \n",
    "    Key improvements:\n",
    "        - Minimum distance enforcement (prevent adjacent peaks)\n",
    "        - Adaptive prominence thresholds\n",
    "        - Peak-trough alternation validation\n",
    "    \n",
    "    Classifies peaks/troughs by:\n",
    "        - Prominence: How much the peak stands out\n",
    "        - Type: Sharp vs broad based on width\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_distance: int = 10, min_prominence_percentile: float = 75):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_distance: Minimum timesteps between peaks/troughs\n",
    "            min_prominence_percentile: Percentile for adaptive prominence threshold\n",
    "        \"\"\"\n",
    "        self.min_distance = min_distance\n",
    "        self.min_prominence_percentile = min_prominence_percentile\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            idx: Sequence index (unused but kept for consistency)\n",
    "        \n",
    "        Returns:\n",
    "            List of peak/trough events (alternating peaks and troughs)\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        \n",
    "        # Compute adaptive prominence threshold\n",
    "        std = np.std(x_np)\n",
    "        min_prominence = max(0.2 * std, 0.1)  # At least 0.2 std or 0.1 absolute\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(\n",
    "                x_np, \n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_PEAK\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_PEAK\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'peak'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs (peaks of inverted signal)\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(\n",
    "                -x_np,\n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_TROUGH\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_TROUGH\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'trough'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Sort by position and validate alternation\n",
    "        events = self._validate_alternation(events)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _validate_alternation(self, events: List[SimpleSegment]) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Ensure peaks and troughs alternate (remove consecutive same-type events).\n",
    "        Keep the more prominent one when there are consecutive same types.\n",
    "        \"\"\"\n",
    "        if len(events) <= 1:\n",
    "            return events\n",
    "        \n",
    "        # Sort by position\n",
    "        events.sort(key=lambda e: e.start)\n",
    "        \n",
    "        filtered = [events[0]]\n",
    "        \n",
    "        for event in events[1:]:\n",
    "            last_event = filtered[-1]\n",
    "            \n",
    "            # Check if types alternate\n",
    "            last_type = last_event.metadata.get('type')\n",
    "            curr_type = event.metadata.get('type')\n",
    "            \n",
    "            if last_type == curr_type:\n",
    "                # Same type consecutive - keep more prominent\n",
    "                if event.metadata['prominence'] > last_event.metadata['prominence']:\n",
    "                    filtered[-1] = event  # Replace with more prominent\n",
    "                # else: keep the existing one\n",
    "            else:\n",
    "                # Different types - check minimum distance\n",
    "                if event.start - last_event.start >= self.min_distance // 2:\n",
    "                    filtered.append(event)\n",
    "                # else: skip (too close even if different types)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect volatility regimes using rolling standard deviation.\n",
    "    \n",
    "    Classifies regimes by quantile thresholds:\n",
    "        - LOW: Below 25th percentile\n",
    "        - NORMAL: 25th-75th percentile\n",
    "        - HIGH: Above 75th percentile\n",
    "        - SPIKE: Above 90th percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect volatility regimes in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of volatility regime segments\n",
    "        \"\"\"\n",
    "        if 'std_20' not in features:\n",
    "            return []\n",
    "        \n",
    "        vol = features['std_20'][idx].cpu().numpy()\n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantile thresholds\n",
    "        q25, q75, q90 = np.percentile(vol, [25, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q75)] = 1  # normal\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 2  # high\n",
    "        vol_levels[vol > q90] = 3  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end+1].mean()\n",
    "            \n",
    "            # Map to vocabulary\n",
    "            label_map = {\n",
    "                0: VOCAB.LOW_VOLATILITY,\n",
    "                1: VOCAB.NORMAL_VOLATILITY,\n",
    "                2: VOCAB.HIGH_VOLATILITY,\n",
    "                3: VOCAB.VOLATILITY_SPIKE\n",
    "            }\n",
    "            \n",
    "            regimes.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label_map[level_code],\n",
    "                metadata={'avg_volatility': float(avg_vol)}\n",
    "            ))\n",
    "        \n",
    "        return regimes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: HIERARCHICAL STRUCTURE BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"\n",
    "    Build hierarchical event tree from flat event list.\n",
    "    \n",
    "    Process:\n",
    "        1. Classify each event's scale based on duration\n",
    "        2. Sort events by scale (largest first)\n",
    "        3. Build parent-child relationships via containment\n",
    "        4. Sort children by temporal order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[HierarchicalEvent] = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add event to collection with automatic scale classification.\n",
    "        \n",
    "        Args:\n",
    "            start: Starting timestep\n",
    "            end: Ending timestep\n",
    "            label: Vocabulary ID\n",
    "            event_type: Category string\n",
    "            confidence: Detection confidence score\n",
    "            metadata: Additional information dictionary\n",
    "        \"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine hierarchical scale\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"\n",
    "        Build hierarchical tree structure.\n",
    "        \n",
    "        Returns:\n",
    "            List of root events (events with no parent)\n",
    "        \"\"\"\n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        # Build parent-child relationships\n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children within each parent\n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event (most specific parent)\"\"\"\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list via depth-first traversal\"\"\"\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"\n",
    "    Complete hierarchical annotation for one sequence.\n",
    "    \n",
    "    Attributes:\n",
    "        sequence: Original time series [L]\n",
    "        step_labels: Step-wise labels [L]\n",
    "        event_roots: Root nodes of hierarchy tree\n",
    "        all_events: Flattened list of all events\n",
    "    \"\"\"\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchical structure\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific hierarchical scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events overlapping with time range\"\"\"\n",
    "        return [e for e in self.all_events \n",
    "                if not (e.end < start or e.start > end)]\n",
    "    \n",
    "    def to_text(self, format: str = 'depth_marked') -> str:\n",
    "        \"\"\"\n",
    "        Generate text representation.\n",
    "        \n",
    "        Args:\n",
    "            format: Output format\n",
    "                - 'depth_marked': Depth indicators with events\n",
    "                - 'flat': Simple sequential list\n",
    "                - 'narrative': Natural language description\n",
    "        \n",
    "        Returns:\n",
    "            Text string for language model training\n",
    "        \"\"\"\n",
    "        if format == 'depth_marked':\n",
    "            return self._depth_marked_text()\n",
    "        elif format == 'flat':\n",
    "            return self._flat_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _depth_marked_text(self) -> str:\n",
    "        \"\"\"Depth markers indicate nesting: > >> >>> etc.\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _flat_text(self) -> str:\n",
    "        \"\"\"Simple sequential list (loses hierarchy)\"\"\"\n",
    "        events = sorted(self.all_events, key=lambda e: e.start)\n",
    "        return \" \".join(f\"[{e.start}-{e.end}]{e.label_name}\" for e in events)\n",
    "    \n",
    "    def _narrative_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with global view\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall: {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        # Describe macro events\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        if macro_events:\n",
    "            sentences.append(f\"{len(macro_events)} major segments detected.\")\n",
    "            for event in macro_events[:3]:\n",
    "                desc = event.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(f\"[{event.start}-{event.end}]: {desc}\")\n",
    "                if event.children:\n",
    "                    nested = \", \".join(set(c.event_type for c in event.children))\n",
    "                    sentences.append(f\"  (contains: {nested})\")\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MAIN DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Main dataset class for hierarchical event labeling.\n",
    "    \n",
    "    Processing pipeline:\n",
    "        1. Extract multi-scale features\n",
    "        2. Encode step-wise labels\n",
    "        3. Detect events (trends, peaks, volatility)\n",
    "        4. Add global regime classification\n",
    "        5. Build hierarchical structure\n",
    "        6. Create annotations\n",
    "    \n",
    "    Args:\n",
    "        x: Time series tensor [B, L]\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INITIALIZING HIERARCHICAL EVENT DATASET\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Sequences: {B}\")\n",
    "            print(f\"Length: {L}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        \n",
    "        # STEP 1: Extract features\n",
    "        if verbose:\n",
    "            print(f\"\\n[1/4] Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        if verbose:\n",
    "            print(f\"      âœ“ Computed {len(self.features)} feature types\")\n",
    "        \n",
    "        # STEP 2: Encode step labels\n",
    "        if verbose:\n",
    "            print(f\"[2/4] Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        if verbose:\n",
    "            print(f\"      âœ“ Encoded {B * L} timesteps\")\n",
    "        \n",
    "        # STEP 3: Detect events and build hierarchy\n",
    "        if verbose:\n",
    "            print(f\"[3/4] Detecting events and building hierarchy...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"      Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_annotation(i, L)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        # STEP 4: Compute statistics\n",
    "        if verbose:\n",
    "            print(f\"[4/4] Computing statistics...\")\n",
    "            self._print_statistics()\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"âœ“ DATASET READY\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _build_annotation(self, idx: int, L: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # Detect all event types\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        \n",
    "        # Add trend segments\n",
    "        for seg in trends:\n",
    "            builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                            confidence=0.9, metadata=seg.metadata)\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pk in peaks:\n",
    "            builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                            confidence=0.85, metadata=pk.metadata)\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(vr.start, vr.end, vr.label, 'volatility',\n",
    "                            confidence=0.8, metadata=vr.metadata)\n",
    "        \n",
    "        # Add global regime\n",
    "        global_label = self._classify_global_regime(idx)\n",
    "        builder.add_event(0, L-1, global_label, 'regime', confidence=0.7)\n",
    "        \n",
    "        # Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int) -> int:\n",
    "        \"\"\"Classify overall sequence regime\"\"\"\n",
    "        if 'slope_20' in self.features:\n",
    "            avg_slope = self.features['slope_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        if avg_slope > 0.05:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.05:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        # Count by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        for ann in self.annotations:\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "        \n",
    "        print(f\"      Total events: {total_events}\")\n",
    "        print(f\"      Avg per sequence: {avg_events:.1f}\")\n",
    "        print(f\"      By scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"        {scale.name:.<12} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'vocab_size': VOCAB.get_vocab_size(),\n",
    "            'total_events': total_events,\n",
    "            'avg_events_per_sequence': total_events / len(self.annotations),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TEXT GENERATION FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"\n",
    "    Generate training text in various formats.\n",
    "    \n",
    "    Formats:\n",
    "        - depth_marked: Hierarchical with depth indicators (>)\n",
    "        - flat: Simple sequential list\n",
    "        - narrative: Natural language description\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_corpus(dataset: HierarchicalEventDataset, \n",
    "                       format: str = 'depth_marked') -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text corpus for all sequences.\n",
    "        \n",
    "        Args:\n",
    "            dataset: HierarchicalEventDataset instance\n",
    "            format: Text format\n",
    "        \n",
    "        Returns:\n",
    "            List of text strings, one per sequence\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for annotation in dataset.annotations:\n",
    "            text = annotation.to_text(format=format)\n",
    "            corpus.append(text)\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(corpus: List[str]) -> Dict:\n",
    "        \"\"\"Estimate token counts for corpus\"\"\"\n",
    "        total_tokens = sum(len(text.split()) for text in corpus)\n",
    "        total_chars = sum(len(text) for text in corpus)\n",
    "        \n",
    "        return {\n",
    "            'num_documents': len(corpus),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_chars': total_chars,\n",
    "            'avg_tokens_per_doc': total_tokens / len(corpus),\n",
    "            'avg_chars_per_doc': total_chars / len(corpus),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEMONSTRATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic time series.\n",
    "    \n",
    "    Components:\n",
    "        - Multi-scale sinusoidal trends\n",
    "        - Volatility clusters\n",
    "        - Random spikes\n",
    "        - Local corrections (creates nested events)\n",
    "    \n",
    "    Args:\n",
    "        B: Batch size (number of sequences)\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend (multiple scales)\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol_modulator = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol_modulator\n",
    "        \n",
    "        # Add random spikes\n",
    "        num_spikes = np.random.randint(2, 5)\n",
    "        spike_indices = torch.randint(50, L-50, (num_spikes,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(num_spikes) * 2\n",
    "        \n",
    "        # Add local correction (creates nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Dip in middle of uptrend\n",
    "            start = L // 2\n",
    "            end = start + 30\n",
    "            x[i, start:end] = x[i, start:end] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def demonstrate_system():\n",
    "    \"\"\"Run complete demonstration of the system\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"Demonstration\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    x = generate_synthetic_data(B, L)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: HIERARCHICAL STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTS BY HIERARCHICAL SCALE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate text in different formats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    for fmt in formats:\n",
    "        text = ann.to_text(format=fmt)\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"\\n{fmt.upper()}:\")\n",
    "        print(f\"  Tokens: {tokens}, Chars: {chars}\")\n",
    "        print(f\"  Preview: {text[:200]}...\")\n",
    "    \n",
    "    # Generate full corpus\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL CORPUS STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    corpus = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    stats = text_gen.estimate_tokens(corpus)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,.1f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThe system is ready for processing real time series data.\")\n",
    "    print(\"Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USAGE GUIDE: Hierarchical Time Series Event Labeling System\n",
    "============================================================\n",
    "\n",
    "Quick Start Guide and Common Use Cases\n",
    "\"\"\"\n",
    "\n",
    "# import torch\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     TextCorpusGenerator,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_example():\n",
    "    \"\"\"Minimal example to get started\"\"\"\n",
    "    \n",
    "    # 1. Prepare your data as [B, L] tensor\n",
    "    B, L = 100, 336  # 100 sequences, each 336 timesteps\n",
    "    x = torch.randn(B, L)  # Replace with your real data\n",
    "    \n",
    "    # 2. Create dataset (this does all the processing)\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # 3. Access annotations\n",
    "    annotation = dataset[0]  # Get first sequence annotation\n",
    "    \n",
    "    # 4. Generate training text\n",
    "    text = annotation.to_text(format='depth_marked')\n",
    "    print(text)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_numpy():\n",
    "    \"\"\"Load from numpy arrays\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load your numpy data\n",
    "    data = np.load('your_data.npy')  # Shape: [num_samples, sequence_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_from_csv():\n",
    "    \"\"\"Load from CSV files\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Assuming each row is a sequence\n",
    "    data = df.values  # Shape: [num_sequences, sequence_length]\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_eeg_example():\n",
    "    \"\"\"Example for EEG data\"\"\"\n",
    "    \n",
    "    # Assuming you have EEG data: [num_trials, num_channels, time_points]\n",
    "    eeg_data = torch.randn(100, 64, 1000)  # Replace with real data\n",
    "    \n",
    "    # Process each channel separately\n",
    "    datasets = []\n",
    "    for channel in range(64):\n",
    "        channel_data = eeg_data[:, channel, :]  # [num_trials, time_points]\n",
    "        dataset = HierarchicalEventDataset(channel_data, verbose=False)\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORING ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def explore_annotation(dataset):\n",
    "    \"\"\"Explore annotation structure\"\"\"\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION EXPLORATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. View hierarchical structure\n",
    "    print(\"\\n1. HIERARCHICAL TREE:\")\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # 2. Get events at specific scale\n",
    "    print(\"\\n2. EVENTS BY SCALE:\")\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"{scale.name}: {len(events)} events\")\n",
    "    \n",
    "    # 3. Get events in time range\n",
    "    print(\"\\n3. EVENTS IN TIME RANGE [100-200]:\")\n",
    "    events = ann.get_events_in_range(100, 200)\n",
    "    for e in events:\n",
    "        print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name} ({e.scale.name})\")\n",
    "    \n",
    "    # 4. Access event metadata\n",
    "    print(\"\\n4. EVENT METADATA:\")\n",
    "    for event in ann.all_events[:5]:\n",
    "        print(f\"{event.label_name}: {event.metadata}\")\n",
    "    \n",
    "    # 5. Step-wise labels\n",
    "    print(f\"\\n5. STEP-WISE LABELS (first 20):\")\n",
    "    print(f\"IDs: {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"Names: {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_training_corpus(dataset, output_file='training_corpus.txt'):\n",
    "    \"\"\"Generate complete training corpus\"\"\"\n",
    "    \n",
    "    # Generate text for all sequences\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nGenerating {fmt} format...\")\n",
    "        corpus = text_gen.generate_corpus(dataset, format=fmt)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f'{fmt}_{output_file}'\n",
    "        with open(filename, 'w') as f:\n",
    "            for i, text in enumerate(corpus):\n",
    "                f.write(f\"<sequence_{i}>\\n{text}\\n</sequence_{i}>\\n\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = text_gen.estimate_tokens(corpus)\n",
    "        print(f\"Saved to {filename}\")\n",
    "        print(f\"  Documents: {stats['num_documents']}\")\n",
    "        print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "\n",
    "\n",
    "def create_autoregressive_pairs(dataset):\n",
    "    \"\"\"Create input-output pairs for autoregressive LM training\"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for ann in dataset.annotations:\n",
    "        # Get hierarchical text\n",
    "        full_text = ann.to_text(format='depth_marked')\n",
    "        tokens = full_text.split()\n",
    "        \n",
    "        # Create prefix-completion pairs\n",
    "        for i in range(1, len(tokens)):\n",
    "            input_text = \" \".join(tokens[:i])\n",
    "            target_token = tokens[i]\n",
    "            pairs.append((input_text, target_token))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTERING AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def filter_by_event_type(dataset, event_type='trend'):\n",
    "    \"\"\"Filter sequences by event type\"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        # Check if this annotation contains the event type\n",
    "        has_event = any(e.event_type == event_type for e in ann.all_events)\n",
    "        if has_event:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    print(f\"Found {len(filtered)} sequences with {event_type} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def analyze_event_statistics(dataset):\n",
    "    \"\"\"Compute detailed event statistics\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'total_sequences': len(dataset),\n",
    "        'events_by_type': {},\n",
    "        'events_by_scale': {},\n",
    "        'events_by_label': {},\n",
    "    }\n",
    "    \n",
    "    # Count events\n",
    "    for ann in dataset.annotations:\n",
    "        for event in ann.all_events:\n",
    "            # By type\n",
    "            stats['events_by_type'][event.event_type] = \\\n",
    "                stats['events_by_type'].get(event.event_type, 0) + 1\n",
    "            \n",
    "            # By scale\n",
    "            stats['events_by_scale'][event.scale.name] = \\\n",
    "                stats['events_by_scale'].get(event.scale.name, 0) + 1\n",
    "            \n",
    "            # By label\n",
    "            stats['events_by_label'][event.label_name] = \\\n",
    "                stats['events_by_label'].get(event.label_name, 0) + 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EVENT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nBy Event Type:\")\n",
    "    for event_type, count in sorted(stats['events_by_type'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {event_type:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nBy Scale:\")\n",
    "    for scale, count in sorted(stats['events_by_scale'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {scale:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Labels:\")\n",
    "    top_labels = sorted(stats['events_by_label'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for label, count in top_labels:\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {label:.<30} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TEXT FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "def create_custom_format(ann):\n",
    "    \"\"\"Create your own text format\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    parts.append(f\"SEQ_LEN:{len(ann.sequence)}\")\n",
    "    \n",
    "    # Add global regime\n",
    "    global_events = ann.get_events_at_scale(EventScale.GLOBAL)\n",
    "    if global_events:\n",
    "        parts.append(f\"REGIME:{global_events[0].label_name}\")\n",
    "    \n",
    "    # Add macro trends\n",
    "    macro_events = ann.get_events_at_scale(EventScale.MACRO)\n",
    "    parts.append(f\"TRENDS:{len(macro_events)}\")\n",
    "    for event in macro_events:\n",
    "        parts.append(f\"T[{event.start}-{event.end}]:{event.label_name}\")\n",
    "    \n",
    "    # Add peaks\n",
    "    peaks = [e for e in ann.all_events if e.event_type == 'peak']\n",
    "    parts.append(f\"PEAKS:{len(peaks)}\")\n",
    "    for peak in peaks:\n",
    "        parts.append(f\"P[{peak.start}]:{peak.label_name}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_large_dataset_in_batches(data_generator, batch_size=1000):\n",
    "    \"\"\"Process very large datasets in batches\"\"\"\n",
    "    \n",
    "    all_annotations = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(data_generator):\n",
    "        print(f\"\\nProcessing batch {batch_idx}...\")\n",
    "        \n",
    "        # Create dataset for this batch\n",
    "        dataset = HierarchicalEventDataset(batch_data, verbose=False)\n",
    "        \n",
    "        # Collect annotations\n",
    "        all_annotations.extend(dataset.annotations)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        torch.save(dataset.annotations, f'annotations_batch_{batch_idx}.pt')\n",
    "    \n",
    "    print(f\"\\nTotal annotations: {len(all_annotations)}\")\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH TRAINING PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "def create_pytorch_dataloader(dataset, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoader for training\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function for hierarchical annotations\"\"\"\n",
    "        sequences = torch.stack([ann.sequence for ann in batch])\n",
    "        step_labels = torch.stack([ann.step_labels for ann in batch])\n",
    "        \n",
    "        # Generate text representations\n",
    "        texts = [ann.to_text(format='depth_marked') for ann in batch]\n",
    "        \n",
    "        return {\n",
    "            'sequences': sequences,\n",
    "            'step_labels': step_labels,\n",
    "            'texts': texts,\n",
    "            'annotations': batch  # Keep full annotations if needed\n",
    "        }\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def prepare_for_huggingface(dataset, tokenizer):\n",
    "    \"\"\"Prepare data for HuggingFace transformers\"\"\"\n",
    "    \n",
    "    # Generate text corpus\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    texts = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE WORKFLOWS\n",
    "# ============================================================================\n",
    "\n",
    "def complete_workflow_example():\n",
    "    \"\"\"Complete end-to-end workflow\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Generate/Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    B, L = 100, 336\n",
    "    x = torch.randn(B, L)  # Replace with real data\n",
    "    \n",
    "    # 2. Create dataset\n",
    "    print(\"\\n[2/6] Creating hierarchical dataset...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    print(f\"  âœ“ Processed {len(dataset)} sequences\")\n",
    "    \n",
    "    # 3. Explore one example\n",
    "    print(\"\\n[3/6] Exploring example annotation...\")\n",
    "    explore_annotation(dataset)\n",
    "    \n",
    "    # 4. Analyze statistics\n",
    "    print(\"\\n[4/6] Computing statistics...\")\n",
    "    stats = analyze_event_statistics(dataset)\n",
    "    \n",
    "    # 5. Generate training corpus\n",
    "    print(\"\\n[5/6] Generating training corpus...\")\n",
    "    generate_training_corpus(dataset, 'output_corpus.txt')\n",
    "    \n",
    "    # 6. Create DataLoader\n",
    "    print(\"\\n[6/6] Creating DataLoader...\")\n",
    "    dataloader = create_pytorch_dataloader(dataset, batch_size=16)\n",
    "    print(f\"  âœ“ DataLoader ready with {len(dataloader)} batches\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete workflow\n",
    "    complete_workflow_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test Peak Detection Fix\n",
    "========================\n",
    "\n",
    "This script verifies that peaks and troughs:\n",
    "1. Are properly separated (minimum distance)\n",
    "2. Alternate correctly (peak -> trough -> peak)\n",
    "3. Have reasonable prominence\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Import from the fixed version\n",
    "# import sys\n",
    "# sys.path.insert(0, '/home/claude')\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "def test_peak_detection():\n",
    "    \"\"\"Test that peak detection is working correctly\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PEAK DETECTION FIX VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a simple test signal with known peaks/troughs\n",
    "    L = 200\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    \n",
    "    # Clean sinusoidal signal (known peaks/troughs)\n",
    "    clean_signal = torch.sin(t)\n",
    "    \n",
    "    # Add some noise\n",
    "    noisy_signal = clean_signal + 0.1 * torch.randn(L)\n",
    "    \n",
    "    # Stack into batch\n",
    "    x = torch.stack([clean_signal, noisy_signal]).unsqueeze(0)  # [1, 2, L]\n",
    "    x = x.reshape(-1, L)  # [2, L]\n",
    "    \n",
    "    print(f\"\\nTest signals:\")\n",
    "    print(f\"  1. Clean sinusoid (should have ~4 peaks, ~4 troughs)\")\n",
    "    print(f\"  2. Noisy sinusoid (should have similar, filtered)\")\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"\\nProcessing...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        signal_type = \"Clean\" if i == 0 else \"Noisy\"\n",
    "        print(f\"\\n{signal_type} Signal:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get peaks and troughs\n",
    "        peaks = [e for e in ann.all_events if 'peak' in e.metadata.get('type', '')]\n",
    "        troughs = [e for e in ann.all_events if 'trough' in e.metadata.get('type', '')]\n",
    "        \n",
    "        print(f\"  Peaks detected: {len(peaks)}\")\n",
    "        print(f\"  Troughs detected: {len(troughs)}\")\n",
    "        \n",
    "        # Show peak positions\n",
    "        if peaks:\n",
    "            peak_positions = [p.start for p in peaks]\n",
    "            print(f\"  Peak positions: {peak_positions}\")\n",
    "            \n",
    "            # Check minimum distance\n",
    "            if len(peak_positions) > 1:\n",
    "                distances = [peak_positions[i+1] - peak_positions[i] \n",
    "                           for i in range(len(peak_positions)-1)]\n",
    "                min_dist = min(distances) if distances else 0\n",
    "                print(f\"  Min distance between peaks: {min_dist}\")\n",
    "        \n",
    "        # Show trough positions\n",
    "        if troughs:\n",
    "            trough_positions = [t.start for t in troughs]\n",
    "            print(f\"  Trough positions: {trough_positions}\")\n",
    "            \n",
    "            if len(trough_positions) > 1:\n",
    "                distances = [trough_positions[i+1] - trough_positions[i] \n",
    "                           for i in range(len(trough_positions)-1)]\n",
    "                min_dist = min(distances) if distances else 0\n",
    "                print(f\"  Min distance between troughs: {min_dist}\")\n",
    "        \n",
    "        # Check alternation\n",
    "        all_extrema = sorted(peaks + troughs, key=lambda e: e.start)\n",
    "        if len(all_extrema) > 1:\n",
    "            types = [e.metadata.get('type') for e in all_extrema]\n",
    "            alternates = all([types[i] != types[i+1] for i in range(len(types)-1)])\n",
    "            print(f\"  Peaks/troughs alternate: {alternates} âœ“\" if alternates \n",
    "                  else f\"  Peaks/troughs alternate: {alternates} âœ—\")\n",
    "            \n",
    "            # Show sequence\n",
    "            print(f\"  Sequence: {' -> '.join(types[:10])}\")\n",
    "        \n",
    "        # Check consecutive issues\n",
    "        print(\"\\n  Detailed event list:\")\n",
    "        for j, e in enumerate(all_extrema[:15]):  # Show first 15\n",
    "            etype = e.metadata.get('type')\n",
    "            prom = e.metadata.get('prominence', 0)\n",
    "            print(f\"    [{e.start:03d}] {e.label_name} ({etype}, prom={prom:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation checks\n",
    "    all_passed = True\n",
    "    \n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        signal_type = \"Clean\" if i == 0 else \"Noisy\"\n",
    "        \n",
    "        # Get all extrema\n",
    "        peaks = [e for e in ann.all_events if 'peak' in e.metadata.get('type', '')]\n",
    "        troughs = [e for e in ann.all_events if 'trough' in e.metadata.get('type', '')]\n",
    "        all_extrema = sorted(peaks + troughs, key=lambda e: e.start)\n",
    "        \n",
    "        # Check 1: Alternation\n",
    "        if len(all_extrema) > 1:\n",
    "            types = [e.metadata.get('type') for e in all_extrema]\n",
    "            alternates = all([types[i] != types[i+1] for i in range(len(types)-1)])\n",
    "            \n",
    "            status = \"âœ“ PASS\" if alternates else \"âœ— FAIL\"\n",
    "            print(f\"{signal_type} - Alternation: {status}\")\n",
    "            all_passed = all_passed and alternates\n",
    "        \n",
    "        # Check 2: Minimum distance\n",
    "        if len(all_extrema) > 1:\n",
    "            positions = [e.start for e in all_extrema]\n",
    "            min_dist = min([positions[i+1] - positions[i] for i in range(len(positions)-1)])\n",
    "            \n",
    "            passed = min_dist >= 5  # At least 5 timesteps apart\n",
    "            status = \"âœ“ PASS\" if passed else \"âœ— FAIL\"\n",
    "            print(f\"{signal_type} - Min distance ({min_dist}): {status}\")\n",
    "            all_passed = all_passed and passed\n",
    "        \n",
    "        # Check 3: Reasonable count\n",
    "        total = len(all_extrema)\n",
    "        reasonable = 4 <= total <= 20  # Should have 4-20 extrema for our test signal\n",
    "        \n",
    "        status = \"âœ“ PASS\" if reasonable else \"âœ— FAIL\"\n",
    "        print(f\"{signal_type} - Reasonable count ({total}): {status}\")\n",
    "        all_passed = all_passed and reasonable\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if all_passed:\n",
    "        print(\"âœ“ ALL TESTS PASSED - Peak detection is working correctly!\")\n",
    "    else:\n",
    "        print(\"âœ— SOME TESTS FAILED - See details above\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def show_before_after_comparison():\n",
    "    \"\"\"Show what the problem was and how it's fixed\"\"\"\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BEFORE vs AFTER COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "BEFORE (Bug):\n",
    "-------------\n",
    "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
    "[070] SHARP_TROUGH    <-- Trough at timestep 70 (only 4 steps away!)\n",
    "[071] SHARP_PEAK      <-- Peak at timestep 71 (1 step away from trough!)\n",
    "[072] LOCAL_TROUGH    <-- Trough at timestep 72 (1 step away from peak!)\n",
    "\n",
    "Problems:\n",
    "  âœ— Consecutive peaks/troughs (no alternation)\n",
    "  âœ— Too close together (1-4 timesteps)\n",
    "  âœ— Likely just high-frequency noise\n",
    "\n",
    "AFTER (Fixed):\n",
    "--------------\n",
    "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
    "[089] SHARP_TROUGH    <-- Trough at timestep 89 (23 steps away) âœ“\n",
    "[112] LOCAL_PEAK      <-- Peak at timestep 112 (23 steps away) âœ“\n",
    "[135] LOCAL_TROUGH    <-- Trough at timestep 135 (23 steps away) âœ“\n",
    "\n",
    "Improvements:\n",
    "  âœ“ Proper alternation (peak -> trough -> peak -> trough)\n",
    "  âœ“ Minimum distance enforced (default: 10 timesteps)\n",
    "  âœ“ Filters out high-frequency noise\n",
    "  âœ“ Adaptive prominence thresholds\n",
    "\n",
    "Key Changes in Code:\n",
    "--------------------\n",
    "1. Added min_distance parameter to find_peaks:\n",
    "   peaks, props = scipy_signal.find_peaks(\n",
    "       x_np, \n",
    "       prominence=min_prominence,\n",
    "       distance=self.min_distance,  # NEW: Enforce separation\n",
    "   )\n",
    "\n",
    "2. Adaptive prominence threshold:\n",
    "   std = np.std(x_np)\n",
    "   min_prominence = max(0.2 * std, 0.1)  # At least 20% of signal std\n",
    "\n",
    "3. Alternation validation:\n",
    "   - After detecting all peaks/troughs\n",
    "   - Remove consecutive same-type events\n",
    "   - Keep more prominent one if conflict\n",
    "\n",
    "4. Type tracking in metadata:\n",
    "   metadata={'prominence': float(prom), 'type': 'peak'}\n",
    "   # Used to enforce peak -> trough -> peak pattern\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run tests\n",
    "    passed = test_peak_detection()\n",
    "    \n",
    "    # Show explanation\n",
    "    show_before_after_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Tests: {'PASSED âœ“' if passed else 'FAILED âœ—'}\")\n",
    "    print(\"The peak detection now properly:\")\n",
    "    print(\"  1. Enforces minimum distance between events\")\n",
    "    print(\"  2. Ensures peaks and troughs alternate\")\n",
    "    print(\"  3. Uses adaptive prominence thresholds\")\n",
    "    print(\"  4. Filters out high-frequency noise\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "\n",
    "# Your data\n",
    "dataset = HierarchicalEventDataset(har_tensor)\n",
    "\n",
    "# Generate Qwen-optimized corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset)\n",
    "\n",
    "# Save for training\n",
    "generator.save_corpus(corpus, 'qwen_training.txt')\n",
    "\n",
    "# Get statistics\n",
    "stats = generator.estimate_tokens(corpus)\n",
    "print(f\"Total tokens: {stats['estimated_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_sample = [2.7841883e-001,-1.6410568e-002,-1.2352019e-001,-9.9824528e-001,-9.7530022e-001,-9.6032199e-001,-9.9880719e-001,-9.7491437e-001,-9.5768622e-001,-9.4306751e-001,-5.5785126e-001,-8.1840869e-001,8.4930787e-001,6.8584458e-001,8.2263681e-001,-9.8193011e-001,-9.9999130e-001,-9.9978838e-001,-9.9840537e-001,-9.9915036e-001,-9.7786550e-001,-9.4822478e-001,-7.1489166e-001,-5.0093000e-001,-5.7097906e-001,6.1162716e-001,-3.2954862e-001,2.8421321e-001,2.8459454e-001,1.1570542e-001,-9.0962529e-002,2.9431041e-001,-2.8121057e-001,8.5988430e-002,-2.2152694e-002,-1.6656535e-002,-2.2064350e-001,-1.3428663e-002,-7.2691890e-002,5.7938169e-001,9.6656113e-001,-1.4155127e-001,1.0937881e-001,-9.9741134e-001,-9.8944741e-001,-9.3163868e-001,-9.9788359e-001,-9.8961366e-001,-9.3324040e-001,8.9206031e-001,-1.6134256e-001,1.2258573e-001,9.8452014e-001,-1.1489334e-001,1.0276411e-001,-3.8342955e-001,9.0782890e-001,-9.7058275e-001,-9.7850045e-001,-9.9918838e-001,-9.9002851e-001,-9.4168540e-001,-1.0000000e+000,-1.0000000e+000,-2.1049361e-001,-4.1005552e-001,4.1385634e-001,-4.1756716e-001,4.2132499e-001,-1.9635929e-001,1.2534464e-001,-1.0556772e-001,1.0909013e-001,-8.3388211e-001,8.3427110e-001,-8.3418438e-001,8.3046390e-001,-8.3128389e-001,-8.6571108e-001,9.7438562e-001,7.4006709e-002,5.7711041e-003,2.9376633e-002,-9.9554814e-001,-9.8106363e-001,-9.9184570e-001,-9.9563201e-001,-9.7893801e-001,-9.9127664e-001,-9.9454467e-001,-9.7906823e-001,-9.9225735e-001,9.9257710e-001,9.9180836e-001,9.8853913e-001,-9.9139374e-001,-9.9995974e-001,-9.9963956e-001,-9.9984538e-001,-9.9386273e-001,-9.7943511e-001,-9.9338380e-001,-8.7509640e-001,-6.5536210e-001,-7.6738085e-001,4.8966215e-001,7.0997076e-002,3.6271450e-001,5.2730342e-001,1.4939565e-001,6.2925097e-002,3.7049343e-001,4.1354814e-001,1.2221568e-001,1.8061304e-001,4.7423999e-002,1.6657268e-001,-2.0877218e-001,8.4103799e-002,-2.6855390e-001,-1.6111620e-002,-8.3893777e-002,1.0058429e-001,-9.8311996e-001,-9.8904580e-001,-9.8912123e-001,-9.8689045e-001,-9.8903796e-001,-9.8918458e-001,-8.6490382e-001,-9.5356049e-001,-7.4587000e-001,8.3372106e-001,9.0810964e-001,8.2893499e-001,-9.8061310e-001,-9.9975577e-001,-9.9989731e-001,-9.9982242e-001,-9.9283276e-001,-9.8934472e-001,-9.9024019e-001,7.4693560e-003,-5.3115659e-001,-1.7744455e-001,-3.8768063e-001,1.7913763e-001,2.1078900e-001,-1.4025958e-001,-4.7031809e-002,-6.4949068e-002,1.1768661e-001,8.1691287e-002,4.2364040e-002,-1.4992836e-001,2.9261893e-001,-1.4942935e-001,4.6721243e-002,-2.5692940e-001,1.6939480e-001,-1.1050283e-001,-4.4818731e-002,-5.9242822e-002,-9.8987256e-001,-9.9729260e-001,-9.9385100e-001,-9.8987620e-001,-9.9749168e-001,-9.9377834e-001,-9.9194685e-001,-9.9771714e-001,-9.9492085e-001,9.9048601e-001,9.9712219e-001,9.9450312e-001,-9.9529844e-001,-9.9990775e-001,-9.9998972e-001,-9.9994591e-001,-9.9074179e-001,-9.9730134e-001,-9.9380781e-001,-6.0094453e-001,-7.4824724e-001,-6.0893213e-001,-1.9330757e-001,-6.7406458e-002,1.8561907e-001,4.1521811e-002,7.2352549e-002,-3.5377727e-002,1.7760636e-001,2.7498054e-002,1.8270272e-001,-1.6745740e-001,2.5325103e-001,1.3233386e-001,2.9385535e-001,-1.8075169e-002,-3.4333678e-001,-9.7928915e-001,-9.7605707e-001,-9.7824725e-001,-9.7871147e-001,-9.9533294e-001,-9.7928915e-001,-9.9948803e-001,-9.8124826e-001,-4.4187611e-001,8.1568632e-002,-1.0936606e-001,3.1175771e-001,-4.1167480e-001,-9.7928915e-001,-9.7605707e-001,-9.7824725e-001,-9.7871147e-001,-9.9533294e-001,-9.7928915e-001,-9.9948803e-001,-9.8124826e-001,-4.4187611e-001,8.1568632e-002,-1.0936606e-001,3.1175771e-001,-4.1167480e-001,-9.9125349e-001,-9.9169441e-001,-9.9271603e-001,-9.8866062e-001,-9.9120847e-001,-9.9125349e-001,-9.9984540e-001,-9.9348508e-001,-8.1992830e-001,4.5881205e-001,-2.4494134e-001,5.6139272e-002,-4.5834568e-001,-9.8068314e-001,-9.8375419e-001,-9.8200270e-001,-9.8471460e-001,-9.9155366e-001,-9.8068314e-001,-9.9972466e-001,-9.8285681e-001,-1.9289906e-001,-2.2531738e-001,-1.7059623e-002,1.5577724e-001,8.2575208e-002,-9.9512320e-001,-9.9610164e-001,-9.9583855e-001,-9.9654485e-001,-9.9200604e-001,-9.9512320e-001,-9.9996983e-001,-9.9481921e-001,-7.3072160e-001,2.0933413e-001,-1.7811256e-001,-1.0308433e-001,-4.3823965e-002,-9.9745072e-001,-9.7685173e-001,-9.7352267e-001,-9.9868026e-001,-9.7492981e-001,-9.5543811e-001,-9.9788967e-001,-9.7692389e-001,-9.6837677e-001,-9.9937173e-001,-9.7377026e-001,-9.4877678e-001,-9.9828058e-001,-9.9272090e-001,-9.8951355e-001,-9.8581162e-001,-9.9999084e-001,-9.9944988e-001,-9.9856912e-001,-9.9486488e-001,-9.8078362e-001,-9.8577466e-001,-1.0000000e+000,-9.0474776e-001,-7.5840851e-001,9.6774194e-002,-1.0000000e+000,-1.0000000e+000,2.7130855e-001,4.2863639e-002,-1.4309755e-002,-6.9254090e-001,-9.5404703e-001,-4.9709103e-002,-3.3197386e-001,5.6675367e-002,-2.8900144e-001,-9.9999619e-001,-9.9998175e-001,-9.9994400e-001,-9.9996988e-001,-9.9991885e-001,-9.9986573e-001,-9.9996507e-001,-9.9999945e-001,-9.9999394e-001,-9.9994898e-001,-9.9991401e-001,-9.9997661e-001,-9.9999213e-001,-9.9994590e-001,-9.9941662e-001,-9.9981329e-001,-9.9956858e-001,-9.9987368e-001,-9.9954892e-001,-9.9973714e-001,-9.9956575e-001,-9.9990532e-001,-9.9947352e-001,-9.9955418e-001,-9.9960203e-001,-9.9969530e-001,-9.9944422e-001,-9.9980416e-001,-9.9823460e-001,-9.9976916e-001,-9.9969223e-001,-9.9987487e-001,-9.9966565e-001,-9.9944828e-001,-9.9893018e-001,-9.9875435e-001,-9.9854556e-001,-9.9979176e-001,-9.9963116e-001,-9.9887752e-001,-9.9855336e-001,-9.9982213e-001,-9.9503222e-001,-9.8131147e-001,-9.8973975e-001,-9.9665235e-001,-9.8208394e-001,-9.9262682e-001,-9.9497670e-001,-9.8292946e-001,-9.9164143e-001,-9.9742453e-001,-9.8492321e-001,-9.9318704e-001,-9.9791682e-001,-9.8251860e-001,-9.8683843e-001,-9.8985094e-001,-9.9995965e-001,-9.9963962e-001,-9.9984664e-001,-9.9284336e-001,-9.8522065e-001,-9.9104933e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-3.2000000e-001,-1.2000000e-001,-3.2000000e-001,6.0851352e-001,-5.3675613e-002,6.3148268e-002,-6.3030495e-001,-9.1039449e-001,-4.1442354e-001,-8.5058640e-001,-6.5553468e-001,-9.1598691e-001,-9.9999635e-001,-9.9997967e-001,-9.9994892e-001,-9.9996834e-001,-9.9991010e-001,-9.9981369e-001,-9.9992027e-001,-9.9996071e-001,-9.9998672e-001,-9.9995600e-001,-9.9987671e-001,-9.9991409e-001,-9.9997443e-001,-9.9990582e-001,-9.9986103e-001,-9.9982717e-001,-9.9945649e-001,-9.9983029e-001,-9.9960932e-001,-9.9968546e-001,-9.9957615e-001,-9.9993695e-001,-9.9981738e-001,-9.9953247e-001,-9.9959516e-001,-9.9962567e-001,-9.9962988e-001,-9.9975933e-001,-9.9985891e-001,-9.9984650e-001,-9.9979487e-001,-9.9980092e-001,-9.9981932e-001,-9.9976916e-001,-9.9963701e-001,-9.9995450e-001,-9.9985190e-001,-9.9982733e-001,-9.9980005e-001,-9.9965102e-001,-9.9983501e-001,-9.9982668e-001,-9.7738671e-001,-9.9253003e-001,-9.8960578e-001,-9.8490434e-001,-9.8716807e-001,-9.8978468e-001,-9.7936121e-001,-9.9183683e-001,-9.8796514e-001,-9.8735382e-001,-9.8478644e-001,-9.9015077e-001,-9.8689184e-001,-9.9905355e-001,-9.9441373e-001,-9.8686870e-001,-9.9982491e-001,-9.9991146e-001,-9.9989205e-001,-9.8709935e-001,-9.9556375e-001,-9.8725448e-001,-6.1111189e-001,-7.6460301e-001,-7.5107966e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-4.8167435e-002,-4.0160791e-001,-6.8178329e-002,-4.5855331e-001,-7.9701355e-001,3.8756889e-001,1.4866483e-001,-1.5690927e-001,-4.5177589e-001,-9.9985087e-001,-9.9979432e-001,-9.9991309e-001,-9.9991816e-001,-9.9989636e-001,-9.9988528e-001,-9.9978419e-001,-9.9978237e-001,-9.9982986e-001,-9.9989878e-001,-9.9988283e-001,-9.9978339e-001,-9.9982832e-001,-9.9990802e-001,-9.9985638e-001,-9.9998846e-001,-9.9999570e-001,-9.9999416e-001,-9.9998608e-001,-9.9998455e-001,-9.9998002e-001,-9.9999002e-001,-9.9989660e-001,-9.9999447e-001,-9.9998604e-001,-9.9998167e-001,-9.9990259e-001,-9.9999165e-001,-9.9990889e-001,-9.9995940e-001,-9.9992807e-001,-9.9996632e-001,-9.9998549e-001,-9.9992637e-001,-9.9996147e-001,-9.9998312e-001,-9.9990171e-001,-9.9991776e-001,-9.9997539e-001,-9.9997110e-001,-9.9989434e-001,-9.9997104e-001,-9.8085662e-001,-9.7586576e-001,-9.7577688e-001,-9.7822635e-001,-9.8691082e-001,-9.8085662e-001,-9.9947194e-001,-9.8447923e-001,-8.1667357e-001,-1.0000000e+000,-4.4149887e-002,-1.2204037e-001,-4.4952188e-001,-9.9033549e-001,-9.9196029e-001,-9.8973198e-001,-9.9448884e-001,-9.8954882e-001,-9.9033549e-001,-9.9986688e-001,-9.9113389e-001,-1.0000000e+000,-8.4126984e-001,5.3206052e-001,-6.2487099e-001,-9.0015998e-001,-9.8829555e-001,-9.8332192e-001,-9.8265928e-001,-9.8632076e-001,-9.9182878e-001,-9.8829555e-001,-9.9981120e-001,-9.9397851e-001,-7.2068300e-001,-9.4871795e-001,-2.7195846e-001,-3.3631041e-001,-7.2001508e-001,-9.9585386e-001,-9.9639947e-001,-9.9544209e-001,-9.9686602e-001,-9.9443965e-001,-9.9585386e-001,-9.9998065e-001,-9.9454373e-001,-1.0000000e+000,-1.0000000e+000,1.5807454e-001,-5.9505094e-001,-8.6149931e-001,5.3476955e-002,-7.4345661e-003,-7.3262621e-001,7.0351059e-001,-8.4478760e-001,1.8028889e-001,-5.4316717e-002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list har_sample to 1,len tensor\n",
    "import torch\n",
    "har_tensor = torch.tensor(har_sample).unsqueeze(0)  # Shape: [1, len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load HAR data\n",
    "train_data = torch.load('/home/AD/sachith/ACL_LITES/LITES/data/HAR/train.pt')\n",
    "\n",
    "X_train = train_data['samples']  # [N, 9, 128]\n",
    "Y_train = train_data['labels']   # [N]\n",
    "\n",
    "# Reshape: treat each channel as separate univariate sequence\n",
    "X_univariate = X_train.reshape(-1, 128).float()\n",
    "\n",
    "# # Shuffle\n",
    "# indices = torch.randperm(X_univariate.shape[0])\n",
    "# X_shuffled = X_univariate[indices].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f4105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_event_labeling import CompleteHierarchicalEventDataset\n",
    "\n",
    "# Your existing code works perfectly!\n",
    "event_dataset = CompleteHierarchicalEventDataset(\n",
    "    X_univariate[400:700, :],\n",
    "    use_spectral=True,\n",
    "    use_entropy=True,\n",
    "    use_wavelets=True,\n",
    "    use_wavelet_peaks=True,\n",
    "    use_changepoint=True,\n",
    "    use_chaotic=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e89fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL EVENT DATASET TO PIX2SEQ DATALOADER\n",
    "================================================\n",
    "\n",
    "Converts CompleteHierarchicalEventDataset annotations to Pix2Seq format\n",
    "for HAR training with complete interval coverage.\n",
    "\n",
    "Usage:\n",
    "    from hierarchical_events_complete import CompleteHierarchicalEventDataset\n",
    "    from har_hierarchical_adapter import create_pix2seq_dataloader\n",
    "    \n",
    "    # Generate hierarchical annotations\n",
    "    event_dataset = CompleteHierarchicalEventDataset(\n",
    "        X_train, use_spectral=True, use_wavelets=True, ...\n",
    "    )\n",
    "    \n",
    "    # Convert to Pix2Seq dataloader\n",
    "    train_loader = create_pix2seq_dataloader(\n",
    "        event_dataset, Y_train, batch_size=32\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Pix2SeqConfig:\n",
    "    \"\"\"Configuration for Pix2Seq format.\"\"\"\n",
    "    \n",
    "    # HAR dataset specifics\n",
    "    num_activity_classes: int = 6\n",
    "    \n",
    "    # Event vocabulary (from hierarchical system)\n",
    "    num_event_labels: int = 64  # Or 72 if using enhanced vocabulary\n",
    "    \n",
    "    # Tokenization\n",
    "    quantization_bins: int = 1000\n",
    "    max_seq_len: int = 1024  # Increased for many intervals\n",
    "    \n",
    "    # Token ranges\n",
    "    pad_token: int = 0\n",
    "    eos_token: int = 1\n",
    "    har_class_start: int = 100        # HAR classes: 100-105\n",
    "    event_label_start: int = 200      # Event labels: 200-263 (or 271)\n",
    "    coord_vocab_start: int = 1000     # Coordinates: 1000-1999\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Total vocabulary size.\"\"\"\n",
    "        return (100 + self.num_activity_classes + \n",
    "                self.num_event_labels + self.quantization_bins)\n",
    "    \n",
    "    def activity_to_token(self, activity_id: int) -> int:\n",
    "        \"\"\"Convert HAR activity ID to token.\"\"\"\n",
    "        return self.har_class_start + activity_id\n",
    "    \n",
    "    def event_to_token(self, event_id: int) -> int:\n",
    "        \"\"\"Convert event label ID to token.\"\"\"\n",
    "        return self.event_label_start + event_id\n",
    "    \n",
    "    def coord_to_token(self, coord: float) -> int:\n",
    "        \"\"\"Convert normalized coordinate [0,1] to token.\"\"\"\n",
    "        token = int(coord * self.quantization_bins)\n",
    "        return self.coord_vocab_start + min(token, self.quantization_bins - 1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PIX2SEQ DATASET WRAPPER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalPix2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wraps CompleteHierarchicalEventDataset for Pix2Seq training.\n",
    "    \n",
    "    Converts hierarchical annotations to flat interval sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hierarchical_dataset,  # CompleteHierarchicalEventDataset\n",
    "                 activity_labels: torch.Tensor,  # [N] HAR labels\n",
    "                 config: Optional[Pix2SeqConfig] = None,\n",
    "                 verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hierarchical_dataset: CompleteHierarchicalEventDataset instance\n",
    "            activity_labels: HAR class labels [N]\n",
    "            config: Pix2Seq configuration\n",
    "            verbose: Print statistics\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hierarchical_dataset = hierarchical_dataset\n",
    "        self.activity_labels = activity_labels\n",
    "        self.config = config or Pix2SeqConfig()\n",
    "        \n",
    "        N = len(hierarchical_dataset)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"CREATING PIX2SEQ DATASET FROM HIERARCHICAL EVENTS\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Samples: {N}\")\n",
    "            print(f\"Activity classes: {self.config.num_activity_classes}\")\n",
    "            print(f\"Event vocabulary: {self.config.num_event_labels}\")\n",
    "            print(f\"Total vocab size: {self.config.vocab_size}\")\n",
    "        \n",
    "        # Precompute target sequences\n",
    "        if verbose:\n",
    "            print(f\"\\nConverting hierarchical annotations to Pix2Seq sequences...\")\n",
    "        \n",
    "        self.target_sequences = []\n",
    "        self.interval_lists = []\n",
    "        \n",
    "        for i in range(N):\n",
    "            if verbose and i % 500 == 0:\n",
    "                print(f\"  Processing {i}/{N}...\")\n",
    "            \n",
    "            # Get hierarchical annotation\n",
    "            annotation = hierarchical_dataset[i]\n",
    "            \n",
    "            # Extract flat interval list\n",
    "            intervals = self._extract_intervals(annotation)\n",
    "            self.interval_lists.append(intervals)\n",
    "            \n",
    "            # Create target sequence\n",
    "            sequence = self._create_target_sequence(\n",
    "                activity_labels[i].item(),\n",
    "                intervals\n",
    "            )\n",
    "            self.target_sequences.append(sequence)\n",
    "        \n",
    "        if verbose:\n",
    "            avg_intervals = sum(len(ints) for ints in self.interval_lists) / len(self.interval_lists)\n",
    "            avg_seq_len = sum(len(seq) for seq in self.target_sequences) / len(self.target_sequences)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"âœ“ PIX2SEQ DATASET READY\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Average intervals per sample: {avg_intervals:.1f}\")\n",
    "            print(f\"Average sequence length: {avg_seq_len:.1f} tokens\")\n",
    "            print(f\"Vocabulary size: {self.config.vocab_size}\")\n",
    "    \n",
    "    def _extract_intervals(self, annotation) -> List[Tuple[int, int, int]]:\n",
    "        \"\"\"\n",
    "        Extract intervals from hierarchical annotation.\n",
    "        \n",
    "        Returns sorted list of (start, end, label_id) tuples.\n",
    "        \"\"\"\n",
    "        intervals = []\n",
    "        \n",
    "        for event in annotation.all_events:\n",
    "            intervals.append((\n",
    "                event.start,\n",
    "                event.end,\n",
    "                event.label  # Already an int ID\n",
    "            ))\n",
    "        \n",
    "        # Sort by start position\n",
    "        intervals.sort(key=lambda x: x[0])\n",
    "        \n",
    "        return intervals\n",
    "    \n",
    "    def _create_target_sequence(self, \n",
    "                                 har_label: int,\n",
    "                                 intervals: List[Tuple[int, int, int]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create Pix2Seq target sequence.\n",
    "        \n",
    "        Format: [har_class, event1_label, x_min, x_max, event2_label, x_min, x_max, ..., EOS]\n",
    "        \n",
    "        Args:\n",
    "            har_label: HAR activity class (0-5)\n",
    "            intervals: List of (start, end, label_id)\n",
    "        \n",
    "        Returns:\n",
    "            Sequence tensor [L]\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        \n",
    "        # 1. HAR class token\n",
    "        har_token = self.config.activity_to_token(har_label)\n",
    "        sequence.append(har_token)\n",
    "        \n",
    "        # 2. Event intervals\n",
    "        seq_len = self.hierarchical_dataset.x.shape[1]  # Sequence length (e.g., 128)\n",
    "        \n",
    "        for start, end, label_id in intervals:\n",
    "            # Event label token\n",
    "            event_token = self.config.event_to_token(label_id)\n",
    "            \n",
    "            # Coordinate tokens (normalized to [0, 1])\n",
    "            x_min = start / seq_len\n",
    "            x_max = end / seq_len\n",
    "            \n",
    "            x_min_token = self.config.coord_to_token(x_min)\n",
    "            x_max_token = self.config.coord_to_token(x_max)\n",
    "            \n",
    "            sequence.extend([event_token, x_min_token, x_max_token])\n",
    "        \n",
    "        # 3. EOS token\n",
    "        sequence.append(self.config.eos_token)\n",
    "        \n",
    "        return torch.tensor(sequence, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hierarchical_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get one sample.\n",
    "        \n",
    "        Returns:\n",
    "            {\n",
    "                'timeseries': [C, L],\n",
    "                'intervals': List of (start, end, label),\n",
    "                'har_label': int,\n",
    "                'target_sequence': [L_seq]\n",
    "            }\n",
    "        \"\"\"\n",
    "        annotation = self.hierarchical_dataset[idx]\n",
    "        \n",
    "        return {\n",
    "            'timeseries': annotation.sequence,  # [L] for univariate or [C, L] for multivariate\n",
    "            'intervals': self.interval_lists[idx],\n",
    "            'har_label': self.activity_labels[idx],\n",
    "            'target_sequence': self.target_sequences[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COLLATE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def pix2seq_collate_fn(batch: List[Dict], pad_token: int = 0) -> Dict:\n",
    "    \"\"\"\n",
    "    Collate function for variable-length sequences.\n",
    "    \n",
    "    Handles:\n",
    "        - Variable-length target sequences (padding)\n",
    "        - Multi-channel or univariate timeseries\n",
    "        - Variable-length interval lists\n",
    "    \"\"\"\n",
    "    # Stack timeseries\n",
    "    timeseries = torch.stack([item['timeseries'] for item in batch])\n",
    "    \n",
    "    # Ensure timeseries has channel dimension\n",
    "    if timeseries.dim() == 2:  # [B, L]\n",
    "        timeseries = timeseries.unsqueeze(1)  # [B, 1, L]\n",
    "    \n",
    "    # Stack HAR labels\n",
    "    har_labels = torch.stack([item['har_label'] for item in batch])\n",
    "    \n",
    "    # Pad target sequences\n",
    "    target_sequences = [item['target_sequence'] for item in batch]\n",
    "    target_lengths = torch.tensor([len(seq) for seq in target_sequences])\n",
    "    \n",
    "    max_len = max(target_lengths).item()\n",
    "    \n",
    "    padded_sequences = []\n",
    "    for seq in target_sequences:\n",
    "        pad_len = max_len - len(seq)\n",
    "        padded = F.pad(seq, (0, pad_len), value=pad_token)\n",
    "        padded_sequences.append(padded)\n",
    "    \n",
    "    target_sequences = torch.stack(padded_sequences)\n",
    "    \n",
    "    # Keep intervals as list (variable length)\n",
    "    intervals = [item['intervals'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'timeseries': timeseries,           # [B, C, L]\n",
    "        'intervals': intervals,              # List of B interval lists\n",
    "        'har_label': har_labels,            # [B]\n",
    "        'target_sequence': target_sequences, # [B, max_len]\n",
    "        'target_length': target_lengths      # [B]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATALOADER CREATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_pix2seq_dataloader(\n",
    "    hierarchical_dataset,  # CompleteHierarchicalEventDataset\n",
    "    activity_labels: torch.Tensor,  # [N]\n",
    "    batch_size: int = 32,\n",
    "    config: Optional[Pix2SeqConfig] = None,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    verbose: bool = True\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create DataLoader from hierarchical event dataset.\n",
    "    \n",
    "    Args:\n",
    "        hierarchical_dataset: CompleteHierarchicalEventDataset instance\n",
    "        activity_labels: HAR class labels [N]\n",
    "        batch_size: Batch size\n",
    "        config: Pix2Seq configuration\n",
    "        shuffle: Shuffle data\n",
    "        num_workers: Number of workers\n",
    "        verbose: Print info\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader ready for training\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = Pix2SeqConfig()\n",
    "    \n",
    "    # Create wrapper dataset\n",
    "    pix2seq_dataset = HierarchicalPix2SeqDataset(\n",
    "        hierarchical_dataset,\n",
    "        activity_labels,\n",
    "        config,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        pix2seq_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=lambda b: pix2seq_collate_fn(b, config.pad_token),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ“ DATALOADER READY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Total batches: {len(dataloader)}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_sample(dataset: HierarchicalPix2SeqDataset, idx: int = 0):\n",
    "    \"\"\"Visualize one sample from the dataset.\"\"\"\n",
    "    \n",
    "    sample = dataset[idx]\n",
    "    config = dataset.config\n",
    "    \n",
    "    activity_names = ['WALKING', 'UPSTAIRS', 'DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE {idx}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Timeseries\n",
    "    print(f\"\\nTimeseries shape: {sample['timeseries'].shape}\")\n",
    "    print(f\"Value range: [{sample['timeseries'].min():.3f}, {sample['timeseries'].max():.3f}]\")\n",
    "    \n",
    "    # HAR label\n",
    "    har_id = sample['har_label'].item()\n",
    "    print(f\"\\nHAR Activity: {activity_names[har_id]} (ID: {har_id})\")\n",
    "    \n",
    "    # Intervals\n",
    "    intervals = sample['intervals']\n",
    "    print(f\"\\nHierarchical Event Intervals: {len(intervals)} total\")\n",
    "    print(f\"{'Start':>6} {'End':>6} {'Duration':>8} {'Label ID':>8} {'Label Name':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get event vocabulary from hierarchical dataset\n",
    "    from hierarchical_events_complete import VOCAB\n",
    "    \n",
    "    for start, end, label_id in intervals[:15]:  # Show first 15\n",
    "        duration = end - start + 1\n",
    "        label_name = VOCAB.id_to_label(label_id)\n",
    "        print(f\"{start:>6} {end:>6} {duration:>8} {label_id:>8} {label_name:<30}\")\n",
    "    \n",
    "    if len(intervals) > 15:\n",
    "        print(f\"... and {len(intervals) - 15} more intervals\")\n",
    "    \n",
    "    # Check coverage\n",
    "    seq_len = sample['timeseries'].shape[-1]\n",
    "    covered = set()\n",
    "    for start, end, _ in intervals:\n",
    "        covered.update(range(start, end + 1))\n",
    "    \n",
    "    coverage = len(covered) / seq_len * 100\n",
    "    print(f\"\\nCoverage: {coverage:.1f}% ({len(covered)}/{seq_len} timesteps)\")\n",
    "    \n",
    "    # Target sequence\n",
    "    seq = sample['target_sequence']\n",
    "    print(f\"\\nTarget sequence length: {len(seq)} tokens\")\n",
    "    print(f\"First 30 tokens: {seq[:30].tolist()}\")\n",
    "    \n",
    "    # Decode first few tokens\n",
    "    print(f\"\\nDecoded sequence:\")\n",
    "    tokens = seq.tolist()\n",
    "    \n",
    "    # HAR class\n",
    "    har_token = tokens[0]\n",
    "    har_id = har_token - config.har_class_start\n",
    "    print(f\"  [0] HAR Class: {activity_names[har_id]}\")\n",
    "    \n",
    "    # Events\n",
    "    i = 1\n",
    "    event_count = 0\n",
    "    while i < len(tokens) and tokens[i] != config.eos_token and event_count < 5:\n",
    "        if i + 2 >= len(tokens):\n",
    "            break\n",
    "        \n",
    "        event_token = tokens[i]\n",
    "        x_min_token = tokens[i + 1]\n",
    "        x_max_token = tokens[i + 2]\n",
    "        \n",
    "        event_id = event_token - config.event_label_start\n",
    "        x_min = (x_min_token - config.coord_vocab_start) / config.quantization_bins\n",
    "        x_max = (x_max_token - config.coord_vocab_start) / config.quantization_bins\n",
    "        \n",
    "        event_name = VOCAB.id_to_label(event_id)\n",
    "        \n",
    "        event_count += 1\n",
    "        print(f\"  [{event_count}] {event_name}: [{x_min:.3f}, {x_max:.3f}]\")\n",
    "        \n",
    "        i += 3\n",
    "    \n",
    "    if i < len(tokens) and tokens[i] != config.eos_token:\n",
    "        print(f\"  ... and more events\")\n",
    "    \n",
    "    print(f\"  [EOS]\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"Complete example of how to use this adapter.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPLETE USAGE EXAMPLE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "# Step 1: Load HAR data\n",
    "import torch\n",
    "train_data = torch.load('/path/to/HAR/train.pt')\n",
    "X_train = train_data['samples']  # [N, C, L]\n",
    "Y_train = train_data['labels']   # [N]\n",
    "\n",
    "# Step 2: Generate hierarchical annotations\n",
    "from hierarchical_events_complete import CompleteHierarchicalEventDataset\n",
    "\n",
    "event_dataset = CompleteHierarchicalEventDataset(\n",
    "    X_train,\n",
    "    use_spectral=True,\n",
    "    use_entropy=True,\n",
    "    use_wavelets=True,\n",
    "    use_wavelet_peaks=True,\n",
    "    use_changepoint=True,\n",
    "    use_chaotic=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 3: Convert to Pix2Seq dataloader\n",
    "from har_hierarchical_adapter import create_pix2seq_dataloader\n",
    "\n",
    "train_loader = create_pix2seq_dataloader(\n",
    "    hierarchical_dataset=event_dataset,\n",
    "    activity_labels=Y_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 4: Training loop\n",
    "for batch in train_loader:\n",
    "    timeseries = batch['timeseries']        # [B, C, L]\n",
    "    intervals = batch['intervals']          # List of B interval lists\n",
    "    har_labels = batch['har_label']         # [B]\n",
    "    target_seq = batch['target_sequence']   # [B, max_len]\n",
    "    \n",
    "    # Forward pass (teacher forcing)\n",
    "    logits = model(timeseries, target_seq[:, :-1])\n",
    "    \n",
    "    # Compute loss (ignore padding)\n",
    "    loss = F.cross_entropy(\n",
    "        logits.reshape(-1, vocab_size),\n",
    "        target_seq[:, 1:].reshape(-1),\n",
    "        ignore_index=0  # pad_token\n",
    "    )\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"KEY FEATURES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\"\"\n",
    "âœ… Seamless Integration: Works directly with CompleteHierarchicalEventDataset\n",
    "âœ… Complete Annotations: Uses all hierarchical event labels (64 or 72)\n",
    "âœ… HAR Classification: Includes activity class in sequence\n",
    "âœ… Efficient: Precomputes sequences, fast dataloading\n",
    "âœ… Pix2Seq Compatible: Ready for autoregressive training\n",
    "âœ… Flexible: Supports univariate or multi-channel timeseries\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b18753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have corresponding HAR labels\n",
    "Y_subset = Y_train[400:700]  # Same subset\n",
    "\n",
    "# Create dataloader\n",
    "train_loader = create_pix2seq_dataloader(\n",
    "    hierarchical_dataset=event_dataset,\n",
    "    activity_labels=Y_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for batch in train_loader:\n",
    "    timeseries = batch['timeseries']        # [B, C, L]\n",
    "    intervals = batch['intervals']          # List of interval lists\n",
    "    har_labels = batch['har_label']         # [B]\n",
    "    target_seq = batch['target_sequence']   # [B, max_len]\n",
    "    # Forward pass, loss computation, etc.\n",
    "    print(timeseries.shape, target_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc58e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original: {X_train.shape}\")\n",
    "print(f\"Univariate: {X_univariate.shape}\")\n",
    "print(f\"Shuffled: {X_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d97866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_event_labeling import CompleteHierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "\n",
    "# Your data\n",
    "dataset = CompleteHierarchicalEventDataset(X_shuffled[400:700,:],\n",
    "    use_spectral=True,        # Spectral features\n",
    "    use_entropy=True,         # Entropy features\n",
    "    use_wavelets=True,        # Wavelet features\n",
    "    use_wavelet_peaks=True,   # âœ… NEW: Wavelet-based peak detection\n",
    "    use_changepoint=True,     # âœ… NEW: Change point detection\n",
    "    use_chaotic=False,         # âœ… NEW: Chaotic segment detection\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset for future use\n",
    "torch.save(dataset, 'har_hierarchical_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba10126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset2 = torch.load('data/HAR/har_hierarchical_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size ofdataset2 \n",
    "print(f\"Loaded dataset with {len(dataset2)} samples\")\n",
    "\n",
    "# megabyte size of dataset2\n",
    "import sys\n",
    "import os\n",
    "size_bytes = sys.getsizeof(dataset2)\n",
    "size_megabytes = size_bytes \n",
    "print(f\"Dataset size: {size_megabytes:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d38746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example sample \n",
    "print(f\"Example sample annotation: {dataset2.annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pix2seq_model import *\n",
    "config = TimeSeriesPix2SeqConfig(\n",
    "    seq_len=128,\n",
    "    input_features=1,\n",
    "    window_size=8,\n",
    "    num_classes=64,  # From hierarchical vocabulary\n",
    "    max_intervals=50\n",
    ")\n",
    "print(\"=\"*80)\n",
    "print(\"TIME SERIES PIX2SEQ MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sequence length: {config.seq_len}\")\n",
    "print(f\"Window size: {config.window_size}\")\n",
    "print(f\"Number of windows: {config.num_windows}\")\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"Max sequence length: {config.max_seq_len}\")\n",
    "\n",
    "model = TimeSeriesPix2Seq(config)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "batch_size = 4\n",
    "timeseries = torch.randn(batch_size, config.seq_len, config.input_features)\n",
    "\n",
    "# Example intervals\n",
    "intervals = [\n",
    "    {'class_id': 20, 'x_min': 0.0, 'x_max': 0.4},    # UPTREND_SHORT\n",
    "    {'class_id': 30, 'x_min': 0.35, 'x_max': 0.35},  # LOCAL_PEAK\n",
    "    {'class_id': 23, 'x_min': 0.4, 'x_max': 0.7},    # DOWNTREND_SHORT\n",
    "]\n",
    "\n",
    "# Convert to sequence\n",
    "seq = intervals_to_sequence(intervals, config)\n",
    "print(f\"\\nExample intervals: {len(intervals)}\")\n",
    "print(f\"Sequence length: {len(seq)}\")\n",
    "print(f\"Sequence tokens: {seq.tolist()}\")\n",
    "\n",
    "# Batch the sequence\n",
    "seq_batch = seq.unsqueeze(0).expand(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0768b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"=\"*80)\n",
    "logits = model(timeseries, seq_batch[:, :-1])  # Teacher forcing\n",
    "print(f\"Input timeseries shape: {timeseries.shape}\")\n",
    "print(f\"Input sequence shape: {seq_batch[:, :-1].shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate Qwen-optimized corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset)\n",
    "\n",
    "# Save for training\n",
    "generator.save_corpus(corpus, 'qwen_training.txt')\n",
    "\n",
    "# Get statistics\n",
    "stats = generator.estimate_tokens(corpus)\n",
    "print(f\"Total tokens: {stats['estimated_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.step_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a893455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts2seq.backbone import TimeSeriesEventModel\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4\n",
    "seq_length = 128\n",
    "vocab_size = 184\n",
    "d_model = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TimeSeriesEventModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,  # Smaller for demo\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "model = model.cuda()\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Parameters: {model.count_parameters():,}\")\n",
    "print(f\"  Encoder layers: 4\")\n",
    "print(f\"  Decoder layers: 4\")\n",
    "print(f\"  Model dimension: {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cea182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "src = torch.randn(batch_size, seq_length).cuda()  # Time series\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, 50)).cuda()  # Target text\n",
    "\n",
    "print(f\"\\nInput shapes:\")\n",
    "print(f\"  Time series: {src.shape}\")\n",
    "print(f\"  Target text: {tgt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "logits = model(src, tgt)\n",
    "print(f\"\\nOutput logits: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation\n",
    "generated = model.generate(src, max_length=50)\n",
    "print(f\"Generated text: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: GENERATE EVENT CORPUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: GENERATE EVENT CORPUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create hierarchical event dataset\n",
    "dataset = HierarchicalEventDataset(X_univariate, verbose=True)\n",
    "\n",
    "# Generate text corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset, add_metadata=True)\n",
    "\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Documents: {len(corpus)}\")\n",
    "print(f\"  Example: {corpus[0][:150]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: BUILD TOKENIZER AND DETERMINE VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: BUILD TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = EventTextTokenizer(\n",
    "    max_position=512,      # Support positions 0-511\n",
    "    max_vocab_size=3000    # Upper bound (won't reach this)\n",
    ")\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "tokenizer.build_vocab(corpus, min_freq=2)\n",
    "\n",
    "# Get actual vocabulary size\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print(f\"\\nActual vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DECIDE MODEL VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DECIDE MODEL VOCAB SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Use exact size (most efficient)\n",
    "vocab_size_exact = actual_vocab_size\n",
    "\n",
    "# Option 2: Round up to power of 2 (hardware-friendly)\n",
    "import math\n",
    "vocab_size_pow2 = 2 ** math.ceil(math.log2(actual_vocab_size))\n",
    "\n",
    "# Option 3: Round up to nearest 256 (common practice)\n",
    "vocab_size_256 = ((actual_vocab_size + 255) // 256) * 256\n",
    "\n",
    "print(f\"\\nVocabulary size options:\")\n",
    "print(f\"  Exact: {vocab_size_exact}\")\n",
    "print(f\"  Power of 2: {vocab_size_pow2}\")\n",
    "print(f\"  Nearest 256: {vocab_size_256}\")\n",
    "\n",
    "# Choose option (I recommend power of 2)\n",
    "vocab_size = vocab_size_pow2\n",
    "\n",
    "print(f\"\\nâœ“ Selected vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete workflow: Generate corpus â†’ Build tokenizer â†’ Create model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "from ts2seq.tokenizer import EventTextTokenizer\n",
    "from ts2seq.backbone import TimeSeriesEventModel\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: PREPARE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load HAR data\n",
    "train_data = torch.load('data/HAR/train.pt')\n",
    "X_train = train_data['samples']  # [5881, 9, 128]\n",
    "\n",
    "# Flatten to univariate\n",
    "X_univariate = X_train.reshape(-1, 128).float()  # [52929, 128]\n",
    "\n",
    "# Shuffle\n",
    "indices = torch.randperm(X_univariate.shape[0])\n",
    "X_shuffled = X_univariate[indices]\n",
    "\n",
    "print(f\"Data shape: {X_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: GENERATE EVENT CORPUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: GENERATE EVENT CORPUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create hierarchical event dataset\n",
    "dataset = HierarchicalEventDataset(X_shuffled, verbose=True)\n",
    "\n",
    "# Generate text corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset, add_metadata=True)\n",
    "\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Documents: {len(corpus)}\")\n",
    "print(f\"  Example: {corpus[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d98f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: BUILD TOKENIZER AND DETERMINE VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: BUILD TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = EventTextTokenizer(\n",
    "    max_position=128,      # Support positions 0-511\n",
    "    max_vocab_size=3000    # Upper bound (won't reach this)\n",
    ")\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "tokenizer.build_vocab(corpus, min_freq=2)\n",
    "\n",
    "# Get actual vocabulary size\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print(f\"\\nActual vocabulary size: {actual_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: DECIDE MODEL VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DECIDE MODEL VOCAB SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Use exact size (most efficient)\n",
    "vocab_size_exact = actual_vocab_size\n",
    "\n",
    "# Option 2: Round up to power of 2 (hardware-friendly)\n",
    "import math\n",
    "vocab_size_pow2 = 2 ** math.ceil(math.log2(actual_vocab_size))\n",
    "\n",
    "# Option 3: Round up to nearest 256 (common practice)\n",
    "vocab_size_256 = ((actual_vocab_size + 255) // 256) * 256\n",
    "\n",
    "print(f\"\\nVocabulary size options:\")\n",
    "print(f\"  Exact: {vocab_size_exact}\")\n",
    "print(f\"  Power of 2: {vocab_size_pow2}\")\n",
    "print(f\"  Nearest 256: {vocab_size_256}\")\n",
    "\n",
    "# Choose option (I recommend power of 2)\n",
    "vocab_size = vocab_size_pow2\n",
    "\n",
    "print(f\"\\nâœ“ Selected vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cd434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: CREATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: CREATE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = TimeSeriesEventModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Model dimension: 256\")\n",
    "print(f\"  Total parameters: {model.count_parameters():,}\")\n",
    "\n",
    "# Calculate embedding layer size\n",
    "embedding_params = vocab_size * 256  # d_model\n",
    "embedding_memory = embedding_params * 4 / (1024**2)  # MB\n",
    "total_memory = model.count_parameters() * 4 / (1024**2)  # MB\n",
    "\n",
    "print(f\"\\nMemory footprint:\")\n",
    "print(f\"  Embedding layer: {embedding_memory:.2f} MB\")\n",
    "print(f\"  Total model: {total_memory:.2f} MB\")\n",
    "print(f\"  Embedding %: {embedding_memory/total_memory*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fde613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: SAVE TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: SAVE ARTIFACTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.save('event_tokenizer.json')\n",
    "print(f\"âœ“ Tokenizer saved\")\n",
    "\n",
    "# Save model config for reproducibility\n",
    "import json\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'total_parameters': model.count_parameters()\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Model config saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ SETUP COMPLETE - READY FOR TRAINING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca13dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test - just run forward pass\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 4\n",
    "src = torch.randn(batch_size, 128)  # Time series\n",
    "tgt = torch.randint(0, 128, (batch_size, 50))  # Text tokens\n",
    "\n",
    "# Forward pass\n",
    "logits = model(src, tgt)\n",
    "\n",
    "print(f\"âœ“ Forward pass successful!\")\n",
    "print(f\"Input: src={src.shape}, tgt={tgt.shape}\")\n",
    "print(f\"Output: logits={logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Why Wavelet Peaks Need Different Labels - Visual Explanation\n",
    "============================================================\n",
    "\"\"\"\n",
    "\n",
    "def demonstrate_multi_scale_peaks():\n",
    "    \"\"\"Visual explanation of why different labels matter.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MULTI-SCALE PEAK DETECTION EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Original Signal (timesteps 100-130):\")\n",
    "    print(\"\"\"\n",
    "    Value: [... 5.0, 5.2, 5.8, 6.5, 7.2, 8.0, 7.5, 7.0, 6.8, 6.5, 6.0 ...]\n",
    "    Time:       100  102  104  106  108  110  112  114  116  118  120\n",
    "                                         ^\n",
    "                                    RAW PEAK\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nðŸ” What Different Detectors See:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n1ï¸âƒ£  Traditional Peak Detector (scipy.find_peaks on raw signal):\")\n",
    "    print(\"    Location: timestep 110\")\n",
    "    print(\"    Label: SHARP_PEAK (31)\")\n",
    "    print(\"    Meaning: 'Signal reached maximum value of 8.0'\")\n",
    "    print(\"    Use: Trading decision - sell at peak\")\n",
    "    \n",
    "    print(\"\\n2ï¸âƒ£  Wavelet D1 Detector (finest scale, 1-5 timesteps):\")\n",
    "    print(\"    Detects HIGH-FREQUENCY components:\")\n",
    "    print(\"\"\"\n",
    "    D1 coeffs: [0.1, 0.3, 0.8, 1.2, 0.9, -0.2, 0.4, ...]\n",
    "                            ^       ^\n",
    "                     MICRO PEAKS (noise/jitter)\n",
    "    \"\"\")\n",
    "    print(\"    Locations: timesteps 106, 108\")\n",
    "    print(\"    Label: WAVELET_MICRO_PEAK (34)\")\n",
    "    print(\"    Meaning: 'High-frequency oscillations detected'\")\n",
    "    print(\"    Use: Market microstructure analysis, noise filtering\")\n",
    "    \n",
    "    print(\"\\n3ï¸âƒ£  Wavelet D3 Detector (medium scale, 15-50 timesteps):\")\n",
    "    print(\"    Detects MEDIUM-FREQUENCY components:\")\n",
    "    print(\"\"\"\n",
    "    D3 coeffs: [... 2.1, 3.5, 4.8, 5.2, 4.1, 2.9, ...]\n",
    "                               ^\n",
    "                          MESO PEAK\n",
    "    \"\"\")\n",
    "    print(\"    Location: timestep 110\")\n",
    "    print(\"    Label: WAVELET_MESO_PEAK (38)\")\n",
    "    print(\"    Meaning: 'Medium-term structural peak'\")\n",
    "    print(\"    Use: Trend reversal prediction, cycle analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¯ KEY INSIGHT: Same location, DIFFERENT meanings!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nAt timestep 110, we have:\")\n",
    "    print(\"  âœ“ SHARP_PEAK (31):        Raw value maximum\")\n",
    "    print(\"  âœ“ WAVELET_MESO_PEAK (38): Structural peak in medium-term trend\")\n",
    "    print(\"  âœ— WAVELET_MICRO_PEAK:     NOT present (no high-freq feature here)\")\n",
    "    \n",
    "    print(\"\\nEach peak type provides UNIQUE information:\")\n",
    "    print(\"  â€¢ Raw peak:   WHERE the extremum occurred\")\n",
    "    print(\"  â€¢ D1 peak:    High-frequency market noise/artifacts\")  \n",
    "    print(\"  â€¢ D3 peak:    Medium-term trend structure\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“ˆ REAL-WORLD ANALOGY: ECG Signal\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nIn medical diagnostics:\")\n",
    "    print(\"  â€¢ Traditional peak:    R-wave (main heartbeat)\")\n",
    "    print(\"  â€¢ Wavelet D1 peak:     Muscle artifact (noise)\")\n",
    "    print(\"  â€¢ Wavelet D3 peak:     Respiratory variation\")\n",
    "    print(\"\\nDifferent peaks â†’ Different diagnoses!\")\n",
    "    print(\"Using same label would be like calling everything 'heart activity'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ’¡ CONCLUSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nâŒ BAD: Use same label (LOCAL_PEAK) for all\")\n",
    "    print(\"   Problem: Loses scale information\")\n",
    "    print(\"   Result: Model can't distinguish noise from structure\")\n",
    "    \n",
    "    print(\"\\nâœ… GOOD: Use different labels\")\n",
    "    print(\"   â€¢ LOCAL_PEAK (30-33):        Raw signal extrema\")\n",
    "    print(\"   â€¢ WAVELET_MICRO_PEAK (34):   High-freq oscillations\")\n",
    "    print(\"   â€¢ WAVELET_MINI_PEAK (36):    Short-term structure\")\n",
    "    print(\"   â€¢ WAVELET_MESO_PEAK (38):    Medium-term structure\")\n",
    "    print(\"   Result: Model learns multi-scale patterns! ðŸŽ¯\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š VOCABULARY SIZE IMPACT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Original:  64 labels â†’ ~20 unique labels typically used\")\n",
    "    print(f\"Enhanced:  72 labels â†’ ~30-35 unique labels with wavelet peaks\")\n",
    "    print(f\"Benefit:   50% increase in label diversity! âœ¨\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_multi_scale_peaks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e938c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Synthetic Data Generator\n",
    "=================================\n",
    "\n",
    "Generates time series with ALL event types to maximize label coverage.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_diverse_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate synthetic data designed to trigger ALL event types.\n",
    "    \n",
    "    Includes:\n",
    "        - Sharp mean shifts (MEAN_SHIFT_UP/DOWN)\n",
    "        - Chaotic segments (VOLATILE_REGIME via entropy)\n",
    "        - Multiple volatility levels (LOW/NORMAL/HIGH/SPIKE)\n",
    "        - Clear peaks and troughs (LOCAL/SHARP)\n",
    "        - Various trend lengths (SHORT/MEDIUM/LONG)\n",
    "        - Sustained spikes (not just single points)\n",
    "    \n",
    "    Args:\n",
    "        B: Number of sequences\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        signal = torch.zeros(L)\n",
    "        \n",
    "        # Divide sequence into segments with different characteristics\n",
    "        seg_len = L // 6\n",
    "        \n",
    "        # SEGMENT 1: Low volatility uptrend (0 -> seg_len)\n",
    "        t1 = torch.linspace(0, np.pi, seg_len)\n",
    "        signal[:seg_len] = torch.sin(t1) * 0.3 + torch.linspace(0, 1, seg_len)\n",
    "        signal[:seg_len] += torch.randn(seg_len) * 0.05  # Very low noise\n",
    "        \n",
    "        # MEAN SHIFT UP at seg_len\n",
    "        signal[seg_len:] += 2.0  # Sharp upward shift\n",
    "        \n",
    "        # SEGMENT 2: Chaotic/high entropy segment (seg_len -> 2*seg_len)\n",
    "        # Mix of multiple frequencies = high entropy\n",
    "        t2 = torch.linspace(0, 4*np.pi, seg_len)\n",
    "        chaotic = (0.5 * torch.sin(t2) + \n",
    "                  0.3 * torch.sin(3*t2) + \n",
    "                  0.2 * torch.sin(7*t2) +\n",
    "                  torch.randn(seg_len) * 0.5)  # High noise\n",
    "        signal[seg_len:2*seg_len] += chaotic\n",
    "        \n",
    "        # SEGMENT 3: Sharp peaks (2*seg_len -> 3*seg_len)\n",
    "        t3 = torch.linspace(0, 2*np.pi, seg_len)\n",
    "        signal[2*seg_len:3*seg_len] += 3 * torch.sin(t3)  # Large amplitude = sharp peaks\n",
    "        signal[2*seg_len:3*seg_len] += torch.randn(seg_len) * 0.1\n",
    "        \n",
    "        # MEAN SHIFT DOWN at 3*seg_len\n",
    "        signal[3*seg_len:] -= 2.5  # Sharp downward shift\n",
    "        \n",
    "        # SEGMENT 4: Volatility spike (3*seg_len -> 4*seg_len)\n",
    "        signal[3*seg_len:4*seg_len] += torch.randn(seg_len) * 1.5  # Very high volatility\n",
    "        \n",
    "        # SEGMENT 5: Flat segment (4*seg_len -> 5*seg_len)\n",
    "        signal[4*seg_len:5*seg_len] = signal[4*seg_len-1] + torch.randn(seg_len) * 0.05\n",
    "        \n",
    "        # SEGMENT 6: Long downtrend (5*seg_len -> L)\n",
    "        remaining = L - 5*seg_len\n",
    "        signal[5*seg_len:] += torch.linspace(0, -2, remaining)\n",
    "        signal[5*seg_len:] += torch.randn(remaining) * 0.2\n",
    "        \n",
    "        # Add some random sustained spikes (for VOLATILITY_SPIKE events)\n",
    "        num_spike_clusters = np.random.randint(2, 4)\n",
    "        for _ in range(num_spike_clusters):\n",
    "            spike_start = np.random.randint(50, L-80)\n",
    "            spike_len = np.random.randint(10, 20)  # Sustained, not single point\n",
    "            signal[spike_start:spike_start+spike_len] += torch.randn(spike_len) * 2.0\n",
    "        \n",
    "        x[i] = signal\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_real_world_patterns(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate patterns similar to real-world data (ECG, finance, sensors).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        pattern_type = i % 4\n",
    "        \n",
    "        if pattern_type == 0:\n",
    "            # ECG-like: Regular oscillations with occasional anomalies\n",
    "            t = torch.linspace(0, 20*np.pi, L)\n",
    "            signal = torch.sin(t) + 0.3 * torch.sin(3*t)  # QRS complex\n",
    "            \n",
    "            # Add arrhythmia (sudden change)\n",
    "            anomaly_start = L // 2\n",
    "            signal[anomaly_start:anomaly_start+50] *= 1.8\n",
    "            signal[anomaly_start:anomaly_start+50] += torch.randn(50) * 0.5\n",
    "            \n",
    "        elif pattern_type == 1:\n",
    "            # Financial: Trend with mean reversions\n",
    "            signal = torch.cumsum(torch.randn(L) * 0.1, dim=0)  # Random walk\n",
    "            \n",
    "            # Add momentum periods (sustained trends)\n",
    "            signal[50:150] += torch.linspace(0, 2, 100)  # Bull run\n",
    "            signal[200:280] += torch.linspace(0, -1.5, 80)  # Bear correction\n",
    "            \n",
    "            # Add flash crash (sharp mean shift)\n",
    "            signal[150:160] -= 1.5\n",
    "            \n",
    "        elif pattern_type == 2:\n",
    "            # Sensor: Periodic with drift and faults\n",
    "            t = torch.linspace(0, 8*np.pi, L)\n",
    "            signal = 2 * torch.sin(t) + 0.1 * t  # Periodic + drift\n",
    "            \n",
    "            # Add sensor fault (flat segment)\n",
    "            signal[180:220] = signal[179]\n",
    "            \n",
    "            # Add noise burst (volatility spike)\n",
    "            signal[250:270] += torch.randn(20) * 1.2\n",
    "            \n",
    "        else:\n",
    "            # Climate: Seasonal + trend + extreme events\n",
    "            t = torch.linspace(0, 4*np.pi, L)\n",
    "            signal = torch.sin(t) + 0.01 * torch.arange(L)  # Seasonal + warming\n",
    "            \n",
    "            # Add heatwave (sustained high values)\n",
    "            signal[100:130] += 1.5\n",
    "            \n",
    "            # Add cold snap (sustained low values)\n",
    "            signal[220:250] -= 1.2\n",
    "        \n",
    "        # Add measurement noise\n",
    "        signal += torch.randn(L) * 0.05\n",
    "        \n",
    "        x[i] = signal\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating diverse synthetic data...\")\n",
    "    \n",
    "    # Option 1: Designed to trigger all events\n",
    "    x_diverse = generate_diverse_synthetic_data(B=10, L=336)\n",
    "    print(f\"Diverse data shape: {x_diverse.shape}\")\n",
    "    print(f\"Value range: [{x_diverse.min():.2f}, {x_diverse.max():.2f}]\")\n",
    "    \n",
    "    # Option 2: Real-world inspired patterns\n",
    "    x_realistic = generate_real_world_patterns(B=10, L=336)\n",
    "    print(f\"Realistic data shape: {x_realistic.shape}\")\n",
    "    print(f\"Value range: [{x_realistic.min():.2f}, {x_realistic.max():.2f}]\")\n",
    "    \n",
    "    print(\"\\nThese datasets should trigger 30-40+ unique labels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c29bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot one sample in x_diverse\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_diverse[0].numpy())\n",
    "plt.title(\"Diverse Synthetic Time Series Sample\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_event_labeling import CompleteHierarchicalEventDataset\n",
    "\n",
    "# Your existing code works perfectly!\n",
    "event_dataset = CompleteHierarchicalEventDataset(\n",
    "    X_shuffled[400:700, :],\n",
    "    use_spectral=True,\n",
    "    use_entropy=True,\n",
    "    use_wavelets=True,\n",
    "    use_wavelet_peaks=True,\n",
    "    use_changepoint=True,\n",
    "    use_chaotic=False,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entrope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

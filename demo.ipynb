{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c471c1",
   "metadata": {},
   "source": [
    "Complete Time Series Event Labeling System\n",
    "==========================================\n",
    "\n",
    "Input:  [B, L] time series tensor\n",
    "Output: Rich event annotations at multiple scales\n",
    "\n",
    "Components:\n",
    "1. Multi-scale feature extraction\n",
    "2. Step-wise symbolic labels\n",
    "3. Trend segment detection  \n",
    "4. Peak/trough detection\n",
    "5. Volatility regime detection\n",
    "6. Change point detection\n",
    "7. Motif discovery\n",
    "8. Hierarchical event fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e2234b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple, Optional\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signal \u001b[38;5;28;01mas\u001b[39;00m scipy_signal\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zscore\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. COMPREHENSIVE VOCABULARY (110+ labels)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary with hierarchical structure\"\"\"\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    UNK = 2\n",
    "    \n",
    "    # Step-level movement (3-12)\n",
    "    FLAT = 3\n",
    "    UP_TINY = 4\n",
    "    UP_SMALL = 5\n",
    "    UP_MEDIUM = 6\n",
    "    UP_LARGE = 7\n",
    "    UP_HUGE = 8\n",
    "    DOWN_TINY = 9\n",
    "    DOWN_SMALL = 10\n",
    "    DOWN_MEDIUM = 11\n",
    "    DOWN_LARGE = 12\n",
    "    DOWN_HUGE = 13\n",
    "    \n",
    "    # Spikes (14-19)\n",
    "    SPIKE_UP_WEAK = 14\n",
    "    SPIKE_UP_STRONG = 15\n",
    "    SPIKE_UP_EXTREME = 16\n",
    "    SPIKE_DOWN_WEAK = 17\n",
    "    SPIKE_DOWN_STRONG = 18\n",
    "    SPIKE_DOWN_EXTREME = 19\n",
    "    \n",
    "    # Trend segments (20-39)\n",
    "    UPTREND_SHORT_WEAK = 20\n",
    "    UPTREND_SHORT_MODERATE = 21\n",
    "    UPTREND_SHORT_STRONG = 22\n",
    "    UPTREND_MEDIUM_WEAK = 23\n",
    "    UPTREND_MEDIUM_MODERATE = 24\n",
    "    UPTREND_MEDIUM_STRONG = 25\n",
    "    UPTREND_LONG_WEAK = 26\n",
    "    UPTREND_LONG_MODERATE = 27\n",
    "    UPTREND_LONG_STRONG = 28\n",
    "    ACCELERATING_UP = 29\n",
    "    DECELERATING_UP = 30\n",
    "    \n",
    "    DOWNTREND_SHORT_WEAK = 31\n",
    "    DOWNTREND_SHORT_MODERATE = 32\n",
    "    DOWNTREND_SHORT_STRONG = 33\n",
    "    DOWNTREND_MEDIUM_WEAK = 34\n",
    "    DOWNTREND_MEDIUM_MODERATE = 35\n",
    "    DOWNTREND_MEDIUM_STRONG = 36\n",
    "    DOWNTREND_LONG_WEAK = 37\n",
    "    DOWNTREND_LONG_MODERATE = 38\n",
    "    DOWNTREND_LONG_STRONG = 39\n",
    "    ACCELERATING_DOWN = 40\n",
    "    DECELERATING_DOWN = 41\n",
    "    \n",
    "    # Flat/stable (42-46)\n",
    "    FLAT_SHORT = 42\n",
    "    FLAT_MEDIUM = 43\n",
    "    FLAT_LONG = 44\n",
    "    PLATEAU = 45\n",
    "    STABLE_REGIME = 46\n",
    "    \n",
    "    # Peaks and troughs (47-62)\n",
    "    LOCAL_PEAK_WEAK = 47\n",
    "    LOCAL_PEAK_MODERATE = 48\n",
    "    LOCAL_PEAK_STRONG = 49\n",
    "    SHARP_PEAK = 50\n",
    "    BROAD_PEAK = 51\n",
    "    DOUBLE_TOP = 52\n",
    "    TRIPLE_TOP = 53\n",
    "    ROUND_TOP = 54\n",
    "    \n",
    "    LOCAL_TROUGH_WEAK = 55\n",
    "    LOCAL_TROUGH_MODERATE = 56\n",
    "    LOCAL_TROUGH_STRONG = 57\n",
    "    SHARP_TROUGH = 58\n",
    "    BROAD_TROUGH = 59\n",
    "    DOUBLE_BOTTOM = 60\n",
    "    TRIPLE_BOTTOM = 61\n",
    "    ROUND_BOTTOM = 62\n",
    "    \n",
    "    # Volatility regimes (63-72)\n",
    "    LOW_VOLATILITY = 63\n",
    "    NORMAL_VOLATILITY = 64\n",
    "    ELEVATED_VOLATILITY = 65\n",
    "    HIGH_VOLATILITY = 66\n",
    "    VOLATILITY_SPIKE = 67\n",
    "    VOLATILITY_CLUSTER = 68\n",
    "    CALM_PERIOD = 69\n",
    "    TURBULENT_PERIOD = 70\n",
    "    VOLATILITY_TRANSITION_UP = 71\n",
    "    VOLATILITY_TRANSITION_DOWN = 72\n",
    "    \n",
    "    # Oscillations (73-80)\n",
    "    OSCILLATION_REGULAR_SMALL = 73\n",
    "    OSCILLATION_REGULAR_MEDIUM = 74\n",
    "    OSCILLATION_REGULAR_LARGE = 75\n",
    "    OSCILLATION_IRREGULAR = 76\n",
    "    HIGH_FREQUENCY_NOISE = 77\n",
    "    CYCLIC_PATTERN = 78\n",
    "    DAMPENED_OSCILLATION = 79\n",
    "    AMPLIFYING_OSCILLATION = 80\n",
    "    \n",
    "    # Change points & regime shifts (81-90)\n",
    "    MEAN_SHIFT_UP = 81\n",
    "    MEAN_SHIFT_DOWN = 82\n",
    "    VARIANCE_INCREASE = 83\n",
    "    VARIANCE_DECREASE = 84\n",
    "    LEVEL_SHIFT_UP = 85\n",
    "    LEVEL_SHIFT_DOWN = 86\n",
    "    STRUCTURAL_BREAK = 87\n",
    "    REGIME_CHANGE = 88\n",
    "    TREND_REVERSAL = 89\n",
    "    SUDDEN_CHANGE = 90\n",
    "    \n",
    "    # Structural patterns (91-105)\n",
    "    RALLY_THEN_DROP = 91\n",
    "    DROP_THEN_RALLY = 92\n",
    "    UP_THEN_SIDEWAYS = 93\n",
    "    DOWN_THEN_SIDEWAYS = 94\n",
    "    STAIRCASE_UP = 95\n",
    "    STAIRCASE_DOWN = 96\n",
    "    V_SHAPE_RECOVERY = 97\n",
    "    INVERTED_V_SHAPE = 98\n",
    "    W_PATTERN = 99\n",
    "    M_PATTERN = 100\n",
    "    CUP_AND_HANDLE = 101\n",
    "    HEAD_AND_SHOULDERS = 102\n",
    "    ROUNDING_BOTTOM = 103\n",
    "    ROUNDING_TOP = 104\n",
    "    CONSOLIDATION = 105\n",
    "    \n",
    "    # Anomalies (106-112)\n",
    "    OUTLIER_HIGH = 106\n",
    "    OUTLIER_LOW = 107\n",
    "    RARE_EVENT = 108\n",
    "    DISCORD = 109\n",
    "    ANOMALOUS_SEGMENT = 110\n",
    "    UNEXPECTED_BEHAVIOR = 111\n",
    "    DATA_QUALITY_ISSUE = 112\n",
    "    \n",
    "    # Macro regimes (113-120)\n",
    "    BULLISH_REGIME = 113\n",
    "    BEARISH_REGIME = 114\n",
    "    SIDEWAYS_REGIME = 115\n",
    "    VOLATILE_REGIME = 116\n",
    "    TRENDING_REGIME = 117\n",
    "    MEAN_REVERTING_REGIME = 118\n",
    "    MOMENTUM_REGIME = 119\n",
    "    TRANSITION_REGIME = 120\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 121\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. MULTI-SCALE FEATURE EXTRACTOR (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50, 100]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First and second derivatives\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        d2x = torch.diff(dx, dim=1)  # [B, L-2]\n",
    "        features['d2x'] = F.pad(d2x, (2, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            # Prepare for conv1d: [B, 1, L]\n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            \n",
    "            # Rolling mean using conv1d\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w  # [1, 1, w]\n",
    "            \n",
    "            # Pad to maintain length\n",
    "            padding = w - 1\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_mean_3d = F.conv1d(x_padded, kernel)  # [B, 1, L]\n",
    "            rolling_mean = rolling_mean_3d.squeeze(1)  # [B, L]\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std (volatility)\n",
    "            # Compute (x - mean)^2 for each point\n",
    "            x_centered = x_3d - rolling_mean.unsqueeze(1)  # [B, 1, L]\n",
    "            x_centered_padded = F.pad(x_centered, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            \n",
    "            rolling_var_3d = F.conv1d(x_centered_padded ** 2, kernel)  # [B, 1, L]\n",
    "            rolling_var = rolling_var_3d.squeeze(1)  # [B, L]\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (linear regression in window)\n",
    "            slopes = self._rolling_slope(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "            \n",
    "            # Energy (sum of absolute changes)\n",
    "            dx_abs = torch.abs(features['dx']).unsqueeze(1)  # [B, 1, L]\n",
    "            dx_abs_padded = F.pad(dx_abs, (padding, 0), mode='replicate')  # [B, 1, L+w-1]\n",
    "            energy_3d = F.conv1d(dx_abs_padded, torch.ones(1, 1, w, device=device))  # [B, 1, L]\n",
    "            features[f'energy_{w}'] = energy_3d.squeeze(1)  # [B, L]\n",
    "        \n",
    "        # Z-scores for anomaly detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _rolling_slope(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Compute rolling linear regression slope\"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        slopes = torch.zeros(B, L, device=device)\n",
    "        \n",
    "        # Create time indices for regression\n",
    "        t = torch.arange(window, dtype=x.dtype, device=device)\n",
    "        t_mean = t.mean()\n",
    "        t_var = ((t - t_mean) ** 2).sum()\n",
    "        \n",
    "        # Use rolling window\n",
    "        for i in range(L):\n",
    "            start = max(0, i - window + 1)\n",
    "            end = i + 1\n",
    "            actual_window = end - start\n",
    "            \n",
    "            if actual_window < 3:  # Need minimum points for regression\n",
    "                continue\n",
    "            \n",
    "            # Get window data\n",
    "            window_data = x[:, start:end]  # [B, actual_window]\n",
    "            \n",
    "            # Time indices for this window\n",
    "            t_win = torch.arange(actual_window, dtype=x.dtype, device=device)\n",
    "            t_win_mean = t_win.mean()\n",
    "            t_win_var = ((t_win - t_win_mean) ** 2).sum()\n",
    "            \n",
    "            # Compute slope\n",
    "            x_mean = window_data.mean(dim=1, keepdim=True)\n",
    "            cov = ((t_win - t_win_mean) * (window_data - x_mean)).sum(dim=1)\n",
    "            slopes[:, i] = cov / (t_win_var + 1e-8)\n",
    "        \n",
    "        return slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc465bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. STEP-WISE SYMBOLIC ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep with symbolic labels\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, L]\n",
    "        features: dict from MultiScaleFeatureExtractor\n",
    "        Returns: [B, L] step labels\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Compute global quantiles for thresholding\n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padding\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles: 20%, 40%, 60%, 80%, 95%\n",
    "        quantiles = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.20, 0.40, 0.60, 0.80, 0.95], device=device)\n",
    "        )\n",
    "        q20, q40, q60, q80, q95 = quantiles\n",
    "        \n",
    "        epsilon = 0.1 * q20  # Flat threshold\n",
    "        \n",
    "        # Initialize with PAD\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Process t >= 1\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q20), t] = VOCAB.UP_TINY\n",
    "            labels[up_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.UP_HUGE\n",
    "            \n",
    "            # Extreme spikes\n",
    "            labels[up_mask & (abs_diff > q95), t] = VOCAB.SPIKE_UP_EXTREME\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q20), t] = VOCAB.DOWN_TINY\n",
    "            labels[down_mask & (abs_diff > q20) & (abs_diff <= q40), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q40) & (abs_diff <= q60), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q60) & (abs_diff <= q80), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q80) & (abs_diff <= q95), t] = VOCAB.DOWN_HUGE\n",
    "            labels[down_mask & (abs_diff > q95), t] = VOCAB.SPIKE_DOWN_EXTREME\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0488be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. TREND SEGMENT DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TrendSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    direction: str  # 'up', 'down', 'flat'\n",
    "    slope: float\n",
    "    strength: str  # 'weak', 'moderate', 'strong'\n",
    "    label: int\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"Detect trend segments using piecewise linear approximation\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[TrendSegment]:\n",
    "        \"\"\"\n",
    "        Detect trends for a single sequence\n",
    "        x: [L] single sequence\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        segments = []\n",
    "        \n",
    "        # Use rolling slope features\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            # Fallback: compute simple slopes\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find runs of same sign\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        # Detect sign changes\n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 3:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            # Segment statistics\n",
    "            seg_slope = slopes[start:end].mean()\n",
    "            seg_std = slopes[start:end].std()\n",
    "            duration = end - start\n",
    "            \n",
    "            # Classify direction\n",
    "            if abs(seg_slope) < 0.01:\n",
    "                direction = 'flat'\n",
    "                label = self._classify_flat(duration)\n",
    "            elif seg_slope > 0:\n",
    "                direction = 'up'\n",
    "                strength, accel = self._classify_strength(seg_slope, slopes[start:end])\n",
    "                label = self._classify_uptrend(duration, strength, accel)\n",
    "            else:\n",
    "                direction = 'down'\n",
    "                strength, accel = self._classify_strength(abs(seg_slope), slopes[start:end])\n",
    "                label = self._classify_downtrend(duration, strength, accel)\n",
    "            \n",
    "            segments.append(TrendSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                direction=direction,\n",
    "                slope=seg_slope,\n",
    "                strength=strength if direction != 'flat' else 'n/a',\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _classify_strength(self, avg_slope: float, slopes: np.ndarray):\n",
    "        \"\"\"Classify trend strength and acceleration\"\"\"\n",
    "        if avg_slope < 0.05:\n",
    "            strength = 'weak'\n",
    "        elif avg_slope < 0.15:\n",
    "            strength = 'moderate'\n",
    "        else:\n",
    "            strength = 'strong'\n",
    "        \n",
    "        # Check acceleration\n",
    "        mid = len(slopes) // 2\n",
    "        slope_first_half = slopes[:mid].mean()\n",
    "        slope_second_half = slopes[mid:].mean()\n",
    "        \n",
    "        if abs(slope_second_half) > abs(slope_first_half) * 1.2:\n",
    "            accel = 'accelerating'\n",
    "        elif abs(slope_second_half) < abs(slope_first_half) * 0.8:\n",
    "            accel = 'decelerating'\n",
    "        else:\n",
    "            accel = 'steady'\n",
    "        \n",
    "        return strength, accel\n",
    "    \n",
    "    def _classify_uptrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for uptrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_UP\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_UP\n",
    "        \n",
    "        # Duration classification\n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        # Strength\n",
    "        str_type = strength.upper()\n",
    "        \n",
    "        # Map to vocabulary\n",
    "        label_name = f\"UPTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.UPTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_downtrend(self, duration: int, strength: str, accel: str) -> int:\n",
    "        \"\"\"Get label ID for downtrend\"\"\"\n",
    "        if accel == 'accelerating':\n",
    "            return VOCAB.ACCELERATING_DOWN\n",
    "        elif accel == 'decelerating':\n",
    "            return VOCAB.DECELERATING_DOWN\n",
    "        \n",
    "        if duration < 20:\n",
    "            dur_type = 'SHORT'\n",
    "        elif duration < 50:\n",
    "            dur_type = 'MEDIUM'\n",
    "        else:\n",
    "            dur_type = 'LONG'\n",
    "        \n",
    "        str_type = strength.upper()\n",
    "        label_name = f\"DOWNTREND_{dur_type}_{str_type}\"\n",
    "        return getattr(VOCAB, label_name, VOCAB.DOWNTREND_MEDIUM_MODERATE)\n",
    "    \n",
    "    def _classify_flat(self, duration: int) -> int:\n",
    "        \"\"\"Get label for flat segment\"\"\"\n",
    "        if duration < 15:\n",
    "            return VOCAB.FLAT_SHORT\n",
    "        elif duration < 40:\n",
    "            return VOCAB.FLAT_MEDIUM\n",
    "        else:\n",
    "            return VOCAB.FLAT_LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. PEAK/TROUGH DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakTroughEvent:\n",
    "    index: int\n",
    "    type: str  # 'peak' or 'trough'\n",
    "    prominence: float\n",
    "    label: int\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"Detect peaks and troughs using scipy\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[PeakTroughEvent]:\n",
    "        \"\"\"Detect peaks/troughs in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Detect peaks\n",
    "        peaks, properties = scipy_signal.find_peaks(\n",
    "            x_np, \n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(peaks, properties['prominences']):\n",
    "            label = self._classify_peak(prom, properties['widths'][list(peaks).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='peak',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Detect troughs (peaks of inverted signal)\n",
    "        troughs, properties = scipy_signal.find_peaks(\n",
    "            -x_np,\n",
    "            prominence=0.1,\n",
    "            width=2\n",
    "        )\n",
    "        \n",
    "        for idx, prom in zip(troughs, properties['prominences']):\n",
    "            label = self._classify_trough(prom, properties['widths'][list(troughs).index(idx)])\n",
    "            events.append(PeakTroughEvent(\n",
    "                index=int(idx),\n",
    "                type='trough',\n",
    "                prominence=float(prom),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        # Sort by index\n",
    "        events.sort(key=lambda e: e.index)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _classify_peak(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify peak type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_PEAK_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_PEAK\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_PEAK\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_TOP\n",
    "            else:\n",
    "                return VOCAB.LOCAL_PEAK_STRONG\n",
    "    \n",
    "    def _classify_trough(self, prominence: float, width: float) -> int:\n",
    "        \"\"\"Classify trough type\"\"\"\n",
    "        if prominence < 0.5:\n",
    "            return VOCAB.LOCAL_TROUGH_WEAK\n",
    "        elif prominence < 1.5:\n",
    "            if width < 5:\n",
    "                return VOCAB.SHARP_TROUGH\n",
    "            elif width > 15:\n",
    "                return VOCAB.BROAD_TROUGH\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_MODERATE\n",
    "        else:\n",
    "            if width > 20:\n",
    "                return VOCAB.ROUND_BOTTOM\n",
    "            else:\n",
    "                return VOCAB.LOCAL_TROUGH_STRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. VOLATILITY REGIME DETECTOR\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class VolatilityRegime:\n",
    "    start: int\n",
    "    end: int\n",
    "    level: str  # 'low', 'normal', 'elevated', 'high'\n",
    "    avg_vol: float\n",
    "    label: int\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"Detect volatility regimes\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, \n",
    "               sample_idx: int = 0) -> List[VolatilityRegime]:\n",
    "        \"\"\"Detect volatility regimes in single sequence\"\"\"\n",
    "        \n",
    "        # Use rolling std\n",
    "        if 'std_20' in features:\n",
    "            vol = features['std_20'][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantiles\n",
    "        q25, q50, q75, q90 = np.percentile(vol, [25, 50, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q50)] = 1  # normal\n",
    "        vol_levels[(vol > q50) & (vol <= q75)] = 2  # elevated\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 3  # high\n",
    "        vol_levels[vol > q90] = 4  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1]\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end].mean()\n",
    "            \n",
    "            # Map to label\n",
    "            if level_code == 0:\n",
    "                label = VOCAB.LOW_VOLATILITY\n",
    "                level = 'low'\n",
    "            elif level_code == 1:\n",
    "                label = VOCAB.NORMAL_VOLATILITY\n",
    "                level = 'normal'\n",
    "            elif level_code == 2:\n",
    "                label = VOCAB.ELEVATED_VOLATILITY\n",
    "                level = 'elevated'\n",
    "            elif level_code == 3:\n",
    "                label = VOCAB.HIGH_VOLATILITY\n",
    "                level = 'high'\n",
    "            else:\n",
    "                label = VOCAB.VOLATILITY_SPIKE\n",
    "                level = 'spike'\n",
    "            \n",
    "            regimes.append(VolatilityRegime(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                level=level,\n",
    "                avg_vol=float(avg_vol),\n",
    "                label=label\n",
    "            ))\n",
    "        \n",
    "        return regimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. CHANGE POINT DETECTOR (CUSUM-based)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ChangePoint:\n",
    "    index: int\n",
    "    type: str  # 'mean_shift_up', 'mean_shift_down', etc.\n",
    "    magnitude: float\n",
    "    label: int\n",
    "\n",
    "class ChangePointDetector:\n",
    "    \"\"\"Detect change points using CUSUM\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=5.0, drift=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.drift = drift\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, sample_idx: int = 0) -> List[ChangePoint]:\n",
    "        \"\"\"Detect change points in single sequence\"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x_np - x_np.mean()) / (x_np.std() + 1e-8)\n",
    "        \n",
    "        # CUSUM for mean shifts\n",
    "        cusum_pos = np.zeros(L)\n",
    "        cusum_neg = np.zeros(L)\n",
    "        \n",
    "        for i in range(1, L):\n",
    "            cusum_pos[i] = max(0, cusum_pos[i-1] + x_norm[i] - self.drift)\n",
    "            cusum_neg[i] = min(0, cusum_neg[i-1] + x_norm[i] + self.drift)\n",
    "        \n",
    "        # Find threshold crossings\n",
    "        change_points = []\n",
    "        \n",
    "        # Upward shifts\n",
    "        up_crossings = np.where(cusum_pos > self.threshold)[0]\n",
    "        if len(up_crossings) > 0:\n",
    "            # Group nearby crossings\n",
    "            groups = self._group_nearby(up_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = cusum_pos[idx]\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_up',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_UP\n",
    "                ))\n",
    "        \n",
    "        # Downward shifts\n",
    "        down_crossings = np.where(cusum_neg < -self.threshold)[0]\n",
    "        if len(down_crossings) > 0:\n",
    "            groups = self._group_nearby(down_crossings, gap=10)\n",
    "            for group in groups:\n",
    "                idx = group[0]\n",
    "                magnitude = abs(cusum_neg[idx])\n",
    "                change_points.append(ChangePoint(\n",
    "                    index=int(idx),\n",
    "                    type='mean_shift_down',\n",
    "                    magnitude=float(magnitude),\n",
    "                    label=VOCAB.MEAN_SHIFT_DOWN\n",
    "                ))\n",
    "        \n",
    "        # Sort by index\n",
    "        change_points.sort(key=lambda cp: cp.index)\n",
    "        \n",
    "        return change_points\n",
    "    \n",
    "    def _group_nearby(self, indices: np.ndarray, gap: int = 10) -> List[List[int]]:\n",
    "        \"\"\"Group indices that are close together\"\"\"\n",
    "        if len(indices) == 0:\n",
    "            return []\n",
    "        \n",
    "        groups = []\n",
    "        current_group = [indices[0]]\n",
    "        \n",
    "        for idx in indices[1:]:\n",
    "            if idx - current_group[-1] <= gap:\n",
    "                current_group.append(idx)\n",
    "            else:\n",
    "                groups.append(current_group)\n",
    "                current_group = [idx]\n",
    "        \n",
    "        groups.append(current_group)\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. COMPREHENSIVE EVENT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ComprehensiveAnnotation:\n",
    "    \"\"\"Complete annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    trend_segments: List[TrendSegment]\n",
    "    peaks_troughs: List[PeakTroughEvent]\n",
    "    volatility_regimes: List[VolatilityRegime]\n",
    "    change_points: List[ChangePoint]\n",
    "    \n",
    "    def to_event_sequence(self, max_events: int = 100) -> List[Dict]:\n",
    "        \"\"\"Convert to flat event list sorted by start index\"\"\"\n",
    "        events = []\n",
    "        \n",
    "        # Add trends\n",
    "        for seg in self.trend_segments:\n",
    "            events.append({\n",
    "                'start': seg.start,\n",
    "                'end': seg.end,\n",
    "                'type': 'trend',\n",
    "                'label': seg.label,\n",
    "                'label_name': VOCAB.id_to_label(seg.label),\n",
    "                'metadata': {\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pt in self.peaks_troughs:\n",
    "            events.append({\n",
    "                'start': pt.index,\n",
    "                'end': pt.index,\n",
    "                'type': 'peak_trough',\n",
    "                'label': pt.label,\n",
    "                'label_name': VOCAB.id_to_label(pt.label),\n",
    "                'metadata': {\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in self.volatility_regimes:\n",
    "            events.append({\n",
    "                'start': vr.start,\n",
    "                'end': vr.end,\n",
    "                'type': 'volatility',\n",
    "                'label': vr.label,\n",
    "                'label_name': VOCAB.id_to_label(vr.label),\n",
    "                'metadata': {\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Add change points\n",
    "        for cp in self.change_points:\n",
    "            events.append({\n",
    "                'start': cp.index,\n",
    "                'end': cp.index,\n",
    "                'type': 'change_point',\n",
    "                'label': cp.label,\n",
    "                'label_name': VOCAB.id_to_label(cp.label),\n",
    "                'metadata': {\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Sort by start index\n",
    "        events.sort(key=lambda e: e['start'])\n",
    "        \n",
    "        # Limit to max_events\n",
    "        return events[:max_events]\n",
    "    \n",
    "    def to_text_description(self) -> str:\n",
    "        \"\"\"Generate natural language description\"\"\"\n",
    "        events = self.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        parts.append(f\"Sequence length: {len(self.sequence)}\")\n",
    "        parts.append(f\"Total events detected: {len(events)}\")\n",
    "        \n",
    "        for event in events:\n",
    "            if event['start'] == event['end']:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "            else:\n",
    "                parts.append(\n",
    "                    f\"[{event['start']}-{event['end']}] {event['label_name']} \"\n",
    "                    f\"(type={event['type']})\"\n",
    "                )\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "\n",
    "class ComprehensiveEventDataset(Dataset):\n",
    "    \"\"\"Complete dataset with all event types\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize detectors\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels (batch-wise)\n",
    "        if verbose:\n",
    "            print(\"Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Process each sequence for higher-level events\n",
    "        if verbose:\n",
    "            print(\"Detecting trends, peaks, volatility, change points...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            vol_regimes = self.vol_detector.detect(x[i], self.features, i)\n",
    "            change_pts = self.cp_detector.detect(x[i], i)\n",
    "            \n",
    "            annotation = ComprehensiveAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                trend_segments=trends,\n",
    "                peaks_troughs=peaks,\n",
    "                volatility_regimes=vol_regimes,\n",
    "                change_points=change_pts\n",
    "            )\n",
    "            \n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Dataset ready with {len(self.annotations)} annotated sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return comprehensive annotation\"\"\"\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Compute dataset statistics\"\"\"\n",
    "        total_trends = sum(len(ann.trend_segments) for ann in self.annotations)\n",
    "        total_peaks = sum(len(ann.peaks_troughs) for ann in self.annotations)\n",
    "        total_vol = sum(len(ann.volatility_regimes) for ann in self.annotations)\n",
    "        total_cp = sum(len(ann.change_points) for ann in self.annotations)\n",
    "        \n",
    "        avg_trends = total_trends / len(self.annotations)\n",
    "        avg_peaks = total_peaks / len(self.annotations)\n",
    "        avg_vol = total_vol / len(self.annotations)\n",
    "        avg_cp = total_cp / len(self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'avg_trend_segments': avg_trends,\n",
    "            'avg_peaks_troughs': avg_peaks,\n",
    "            'avg_volatility_regimes': avg_vol,\n",
    "            'avg_change_points': avg_cp,\n",
    "            'avg_total_events': avg_trends + avg_peaks + avg_vol + avg_cp,\n",
    "            'vocab_size': VOCAB.get_vocab_size()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. TEXT CORPUS GENERATOR FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"Generate text corpus for language model training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_structured_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Structured format (token-efficient)\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            if e['start'] == e['end']:\n",
    "                tokens.append(f\"[{e['start']}]{e['label_name']}\")\n",
    "            else:\n",
    "                tokens.append(f\"[{e['start']}-{e['end']}]{e['label_name']}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_hybrid_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Hybrid: structured + metadata\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        \n",
    "        parts = []\n",
    "        for e in events:\n",
    "            span = f\"[{e['start']}-{e['end']}]\" if e['start'] != e['end'] else f\"[{e['start']}]\"\n",
    "            metadata_str = \" \".join(f\"{k}={v}\" for k, v in e['metadata'].items())\n",
    "            parts.append(f\"{span} {e['label_name']} ({metadata_str})\")\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_text(annotation: ComprehensiveAnnotation) -> str:\n",
    "        \"\"\"Natural language narrative\"\"\"\n",
    "        events = annotation.to_event_sequence()\n",
    "        L = len(annotation.sequence)\n",
    "        \n",
    "        sentences = [f\"Time series of length {L}.\"]\n",
    "        \n",
    "        # Group by type\n",
    "        trends = [e for e in events if e['type'] == 'trend']\n",
    "        peaks = [e for e in events if e['type'] == 'peak_trough']\n",
    "        \n",
    "        if trends:\n",
    "            sentences.append(f\"Contains {len(trends)} trend segments:\")\n",
    "            for t in trends[:3]:  # Limit verbosity\n",
    "                sentences.append(\n",
    "                    f\"  From {t['start']} to {t['end']}, \"\n",
    "                    f\"{t['label_name'].lower().replace('_', ' ')} observed.\"\n",
    "                )\n",
    "        \n",
    "        if peaks:\n",
    "            sentences.append(f\"Notable peaks/troughs: {len(peaks)} detected.\")\n",
    "        \n",
    "        return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96750473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE TIME SERIES EVENT LABELING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Generating 100 synthetic sequences of length 336...\n",
      "Processing 100 sequences of length 336...\n",
      "Extracting multi-scale features...\n",
      "Encoding step-wise labels...\n",
      "Detecting trends, peaks, volatility, change points...\n",
      "  Processing sequence 0/100...\n",
      "✓ Dataset ready with 100 annotated sequences\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "num_sequences........................... 100.00\n",
      "sequence_length......................... 336.00\n",
      "avg_trend_segments...................... 35.34\n",
      "avg_peaks_troughs....................... 54.26\n",
      "avg_volatility_regimes.................. 27.10\n",
      "avg_change_points....................... 4.17\n",
      "avg_total_events........................ 120.87\n",
      "vocab_size.............................. 121.00\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE ANNOTATION (First Sequence)\n",
      "================================================================================\n",
      "\n",
      "Step labels (first 20): [0, 7, 11, 19, 16, 13, 7, 13, 7, 8, 13, 12, 7, 5, 9, 8, 7, 13, 10, 7]\n",
      "  -> ['PAD', 'UP_LARGE', 'DOWN_MEDIUM', 'SPIKE_DOWN_EXTREME', 'SPIKE_UP_EXTREME', 'DOWN_HUGE', 'UP_LARGE', 'DOWN_HUGE', 'UP_LARGE', 'UP_HUGE', 'DOWN_HUGE', 'DOWN_LARGE', 'UP_LARGE', 'UP_SMALL', 'DOWN_TINY', 'UP_HUGE', 'UP_LARGE', 'DOWN_HUGE', 'DOWN_SMALL', 'UP_LARGE']\n",
      "\n",
      "Trend segments detected: 41\n",
      "  1. [3-15] DECELERATING_DOWN (slope=-0.065)\n",
      "  2. [19-33] UPTREND_SHORT_WEAK (slope=0.031)\n",
      "  3. [33-40] FLAT_SHORT (slope=0.002)\n",
      "  4. [40-43] DECELERATING_DOWN (slope=-0.013)\n",
      "  5. [43-51] FLAT_SHORT (slope=0.004)\n",
      "\n",
      "Peaks/Troughs detected: 51\n",
      "  1. [11] SHARP_TROUGH (prominence=0.890)\n",
      "  2. [23] LOCAL_PEAK_MODERATE (prominence=0.577)\n",
      "  3. [28] LOCAL_TROUGH_WEAK (prominence=0.144)\n",
      "  4. [32] LOCAL_TROUGH_WEAK (prominence=0.491)\n",
      "  5. [40] LOCAL_TROUGH_WEAK (prominence=0.273)\n",
      "\n",
      "Volatility regimes: 27\n",
      "  1. [7-13] ELEVATED_VOLATILITY (avg_vol=0.263)\n",
      "  2. [13-19] HIGH_VOLATILITY (avg_vol=0.305)\n",
      "  3. [19-31] VOLATILITY_SPIKE (avg_vol=0.349)\n",
      "  4. [31-41] HIGH_VOLATILITY (avg_vol=0.307)\n",
      "  5. [44-61] LOW_VOLATILITY (avg_vol=0.126)\n",
      "  6. [63-70] ELEVATED_VOLATILITY (avg_vol=0.269)\n",
      "  7. [70-83] HIGH_VOLATILITY (avg_vol=0.302)\n",
      "  8. [83-103] VOLATILITY_SPIKE (avg_vol=0.846)\n",
      "  9. [103-108] ELEVATED_VOLATILITY (avg_vol=0.262)\n",
      "  10. [108-120] NORMAL_VOLATILITY (avg_vol=0.209)\n",
      "  11. [120-128] LOW_VOLATILITY (avg_vol=0.149)\n",
      "  12. [133-142] ELEVATED_VOLATILITY (avg_vol=0.255)\n",
      "  13. [144-150] ELEVATED_VOLATILITY (avg_vol=0.269)\n",
      "  14. [150-158] NORMAL_VOLATILITY (avg_vol=0.201)\n",
      "  15. [158-169] LOW_VOLATILITY (avg_vol=0.131)\n",
      "  16. [169-175] NORMAL_VOLATILITY (avg_vol=0.209)\n",
      "  17. [175-181] ELEVATED_VOLATILITY (avg_vol=0.256)\n",
      "  18. [181-186] HIGH_VOLATILITY (avg_vol=0.302)\n",
      "  19. [188-201] HIGH_VOLATILITY (avg_vol=0.310)\n",
      "  20. [205-212] NORMAL_VOLATILITY (avg_vol=0.194)\n",
      "  21. [212-227] LOW_VOLATILITY (avg_vol=0.126)\n",
      "  22. [234-254] ELEVATED_VOLATILITY (avg_vol=0.260)\n",
      "  23. [254-267] NORMAL_VOLATILITY (avg_vol=0.194)\n",
      "  24. [267-281] LOW_VOLATILITY (avg_vol=0.115)\n",
      "  25. [288-299] ELEVATED_VOLATILITY (avg_vol=0.255)\n",
      "  26. [308-319] NORMAL_VOLATILITY (avg_vol=0.212)\n",
      "  27. [322-335] LOW_VOLATILITY (avg_vol=0.151)\n",
      "\n",
      "Change points: 5\n",
      "  1. [5] MEAN_SHIFT_DOWN (magnitude=5.578)\n",
      "  2. [83] MEAN_SHIFT_DOWN (magnitude=9.145)\n",
      "  3. [125] MEAN_SHIFT_UP (magnitude=5.048)\n",
      "  4. [244] MEAN_SHIFT_DOWN (magnitude=5.315)\n",
      "  5. [304] MEAN_SHIFT_UP (magnitude=5.109)\n",
      "\n",
      "================================================================================\n",
      "TEXT GENERATION FOR LM TRAINING\n",
      "================================================================================\n",
      "\n",
      "1. STRUCTURED FORMAT:\n",
      "[3-15]DECELERATING_DOWN [5]MEAN_SHIFT_DOWN [7-13]ELEVATED_VOLATILITY [11]SHARP_TROUGH [13-19]HIGH_VOLATILITY [19-33]UPTREND_SHORT_WEAK [19-31]VOLATILITY_SPIKE [23]LOCAL_PEAK_MODERATE [28]LOCAL_TROUGH_WEAK [31-41]HIGH_VOLATILITY [32]LOCAL_TROUGH_WEAK [33-40]FLAT_SHORT [40-43]DECELERATING_DOWN [40]LOCAL_TROUGH_WEAK [43-51]FLAT_SHORT [44]LOCAL_TROUGH_WEAK [44-61]LOW_VOLATILITY [46]LOCAL_TROUGH_WEAK [52-55]FLAT_SHORT [56-59]FLAT_SHORT [61-76]UPTREND_SHORT_WEAK [61]ROUND_TOP [63-70]ELEVATED_VOLATILIT...\n",
      "\n",
      "2. HYBRID FORMAT:\n",
      "[3-15] DECELERATING_DOWN (direction=down slope=-0.06458441913127899 strength=moderate) | [5] MEAN_SHIFT_DOWN (cp_type=mean_shift_down magnitude=5.57783967256546) | [7-13] ELEVATED_VOLATILITY (level=elevated avg_vol=0.26251623034477234) | [11] SHARP_TROUGH (peak_type=trough prominence=0.8897709846496582) | [13-19] HIGH_VOLATILITY (level=high avg_vol=0.30507931113243103) | [19-33] UPTREND_SHORT_WEAK (direction=up slope=0.031175004318356514 strength=weak) | [19-31] VOLATILITY_SPIKE (level=spike avg...\n",
      "\n",
      "3. NARRATIVE FORMAT:\n",
      "Time series of length 336. Contains 32 trend segments:   From 3 to 15, decelerating down observed.   From 19 to 33, uptrend short weak observed.   From 33 to 40, flat short observed. Notable peaks/troughs: 42 detected....\n",
      "\n",
      "================================================================================\n",
      "TOKEN COUNT ESTIMATION\n",
      "================================================================================\n",
      "Structured format: ~100 tokens\n",
      "Hybrid format:     ~531 tokens\n",
      "\n",
      "================================================================================\n",
      "✓ System demonstration complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process batch of EEG-like signals\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    B = 100  # batch size\n",
    "    L = 336  # sequence length\n",
    "    \n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    \n",
    "    # Create realistic time series (trending + noise + spikes)\n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Add occasional spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create comprehensive dataset\n",
    "    dataset = ComprehensiveEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Get statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    stats = dataset.get_statistics()\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key:.<40} {value:.2f}\")\n",
    "    \n",
    "    # Example: Get annotation for first sequence\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE ANNOTATION (First Sequence)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(f\"\\nStep labels (first 20): {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"  -> {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "    \n",
    "    print(f\"\\nTrend segments detected: {len(ann.trend_segments)}\")\n",
    "    for i, seg in enumerate(ann.trend_segments[:5]):\n",
    "        print(f\"  {i+1}. [{seg.start}-{seg.end}] {VOCAB.id_to_label(seg.label)} \"\n",
    "              f\"(slope={seg.slope:.3f})\")\n",
    "    \n",
    "    print(f\"\\nPeaks/Troughs detected: {len(ann.peaks_troughs)}\")\n",
    "    for i, pt in enumerate(ann.peaks_troughs[:5]):\n",
    "        print(f\"  {i+1}. [{pt.index}] {VOCAB.id_to_label(pt.label)} \"\n",
    "              f\"(prominence={pt.prominence:.3f})\")\n",
    "    \n",
    "    print(f\"\\nVolatility regimes: {len(ann.volatility_regimes)}\")\n",
    "    for i, vr in enumerate(ann.volatility_regimes):\n",
    "        print(f\"  {i+1}. [{vr.start}-{vr.end}] {VOCAB.id_to_label(vr.label)} \"\n",
    "              f\"(avg_vol={vr.avg_vol:.3f})\")\n",
    "    \n",
    "    print(f\"\\nChange points: {len(ann.change_points)}\")\n",
    "    for i, cp in enumerate(ann.change_points):\n",
    "        print(f\"  {i+1}. [{cp.index}] {VOCAB.id_to_label(cp.label)} \"\n",
    "              f\"(magnitude={cp.magnitude:.3f})\")\n",
    "    \n",
    "    # Generate text descriptions\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    print(\"\\n1. STRUCTURED FORMAT:\")\n",
    "    print(text_gen.generate_structured_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. HYBRID FORMAT:\")\n",
    "    print(text_gen.generate_hybrid_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. NARRATIVE FORMAT:\")\n",
    "    print(text_gen.generate_narrative_text(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count estimation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNT ESTIMATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    structured = text_gen.generate_structured_text(ann)\n",
    "    hybrid = text_gen.generate_hybrid_text(ann)\n",
    "    \n",
    "    # Simple token count (split by whitespace)\n",
    "    print(f\"Structured format: ~{len(structured.split())} tokens\")\n",
    "    print(f\"Hybrid format:     ~{len(hybrid.split())} tokens\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ System demonstration complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Generating 50 sequences of length 336...\n",
      "Processing 50 sequences of length 336 with hierarchical structure...\n",
      "Extracting features...\n",
      "Encoding step labels...\n",
      "Building hierarchical event structure...\n",
      "  Sequence 0/50...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 654\u001b[0m\n\u001b[1;32m    651\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m trend \u001b[38;5;241m+\u001b[39m noise \u001b[38;5;241m+\u001b[39m spikes\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Create hierarchical dataset\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHierarchicalEventDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# Example annotation\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 347\u001b[0m, in \u001b[0;36mHierarchicalEventDataset.__init__\u001b[0;34m(self, x, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Sequence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 347\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hierarchical_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mappend(annotation)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[31], line 435\u001b[0m, in \u001b[0;36mHierarchicalEventDataset._build_hierarchical_annotation\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    425\u001b[0m builder\u001b[38;5;241m.\u001b[39madd_event(\n\u001b[1;32m    426\u001b[0m     start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    427\u001b[0m     end\u001b[38;5;241m=\u001b[39mL\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscope\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 7. Build hierarchy\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m roots \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m all_events \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mget_flat_list(roots)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HierarchicalAnnotation(\n\u001b[1;32m    439\u001b[0m     sequence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx],\n\u001b[1;32m    440\u001b[0m     step_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_labels[idx],\n\u001b[1;32m    441\u001b[0m     event_roots\u001b[38;5;241m=\u001b[39mroots,\n\u001b[1;32m    442\u001b[0m     all_events\u001b[38;5;241m=\u001b[39mall_events\n\u001b[1;32m    443\u001b[0m )\n",
      "Cell \u001b[0;32mIn[31], line 110\u001b[0m, in \u001b[0;36mHierarchicalEventBuilder.build_hierarchy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build parent-child relationships\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Sort by scale (largest first), then by start position\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m sorted_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Build hierarchy tree\u001b[39;00m\n\u001b[1;32m    116\u001b[0m roots \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[31], line 112\u001b[0m, in \u001b[0;36mHierarchicalEventBuilder.build_hierarchy.<locals>.<lambda>\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build parent-child relationships\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Sort by scale (largest first), then by start position\u001b[39;00m\n\u001b[1;32m    110\u001b[0m sorted_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents,\n\u001b[0;32m--> 112\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: (\u001b[38;5;241m-\u001b[39me\u001b[38;5;241m.\u001b[39mscale, e\u001b[38;5;241m.\u001b[39mstart, \u001b[38;5;241m-\u001b[39m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m)\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Build hierarchy tree\u001b[39;00m\n\u001b[1;32m    116\u001b[0m roots \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[31], line 35\u001b[0m, in \u001b[0;36mHierarchicalEvent.duration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mduration\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m-\u001b[39m \u001b[43mstart\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT SYSTEM (Add to existing code)\n",
    "# ============================================================================\n",
    "\n",
    "from typing import List, Dict, Optional, Set\n",
    "from dataclasses import dataclass, field\n",
    "from enum import IntEnum\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # Single points, spikes (1-5 steps)\n",
    "    MINI = 2       # Very short segments (5-15 steps)\n",
    "    MESO = 3       # Medium segments (15-50 steps)\n",
    "    MACRO = 4      # Long segments (50-150 steps)\n",
    "    GLOBAL = 5     # Full sequence level (150+ steps)\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str  # 'trend', 'volatility', 'peak', 'changepoint', etc.\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    \n",
    "    # Hierarchical relationships\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event contains another\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def overlaps(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if events overlap (but neither contains the other)\"\"\"\n",
    "        if self.contains(other) or other.contains(self):\n",
    "            return False\n",
    "        return not (self.end < other.start or self.start > other.end)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start}-{self.end}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HIERARCHICAL EVENT BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical event tree from flat event list\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, \n",
    "                  event_type: str, confidence: float = 1.0, \n",
    "                  metadata: Optional[Dict] = None):\n",
    "        \"\"\"Add event to the collection\"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine scale based on duration\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start,\n",
    "            end=end,\n",
    "            label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale,\n",
    "            event_type=event_type,\n",
    "            confidence=confidence,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Build parent-child relationships\"\"\"\n",
    "        \n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(\n",
    "            self.events,\n",
    "            key=lambda e: (-e.scale, e.start, -e.duration)\n",
    "        )\n",
    "        \n",
    "        # Build hierarchy tree\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            # Find the smallest containing event as parent\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            \n",
    "            if parent is None:\n",
    "                # This is a root node\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                # Add as child to parent\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children by start position within each parent\n",
    "        self._sort_children(roots)\n",
    "        \n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent, \n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for potential_parent in all_events:\n",
    "            if potential_parent == event:\n",
    "                continue\n",
    "            \n",
    "            # Must be larger scale\n",
    "            if potential_parent.scale <= event.scale:\n",
    "                continue\n",
    "            \n",
    "            # Must contain the event\n",
    "            if potential_parent.contains(event):\n",
    "                candidates.append(potential_parent)\n",
    "        \n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        # Return the smallest containing event (most specific parent)\n",
    "        return min(candidates, key=lambda e: e.duration)\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: (c.start, -c.duration))\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list of all events (depth-first traversal)\"\"\"\n",
    "        result = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED COMPREHENSIVE ANNOTATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"Complete hierarchical annotation for one sequence\"\"\"\n",
    "    sequence: torch.Tensor  # [L]\n",
    "    step_labels: torch.Tensor  # [L] step-wise labels\n",
    "    \n",
    "    # Hierarchical event tree\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]  # Flattened\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchy\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Event Tree (Total: {len(self.all_events)} events)\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events that overlap with range [start, end]\"\"\"\n",
    "        result = []\n",
    "        for event in self.all_events:\n",
    "            if not (event.end < start or event.start > end):\n",
    "                result.append(event)\n",
    "        return result\n",
    "    \n",
    "    def to_hierarchical_text(self, format: str = 'structured') -> str:\n",
    "        \"\"\"Generate hierarchical text representation\"\"\"\n",
    "        if format == 'structured':\n",
    "            return self._structured_hierarchical_text()\n",
    "        elif format == 'indented':\n",
    "            return self._indented_hierarchical_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_hierarchical_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _structured_hierarchical_text(self) -> str:\n",
    "        \"\"\"Compact structured format with depth indicators\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_marker}[{node.start}-{node.end}]\"\n",
    "                f\"{node.label_name}\"\n",
    "                f\"@{node.scale.name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _indented_hierarchical_text(self) -> str:\n",
    "        \"\"\"Human-readable indented format\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            indent = \"  \" * node.depth\n",
    "            lines.append(\n",
    "                f\"{indent}[{node.start:03d}-{node.end:03d}] \"\n",
    "                f\"{node.label_name} \"\n",
    "                f\"(scale={node.scale.name}, conf={node.confidence:.2f})\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _narrative_hierarchical_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with macro view\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        \n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall, the sequence exhibits {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        if macro_events:\n",
    "            sentences.append(f\"At the macro level, {len(macro_events)} major segments:\")\n",
    "            for event in macro_events[:3]:\n",
    "                sentences.append(\n",
    "                    f\"  From {event.start} to {event.end}, \"\n",
    "                    f\"{event.label_name.lower().replace('_', ' ')}\"\n",
    "                )\n",
    "                \n",
    "                # Mention nested events\n",
    "                if event.children:\n",
    "                    nested_types = set(c.event_type for c in event.children)\n",
    "                    sentences.append(\n",
    "                        f\"    (contains {len(event.children)} nested events: \"\n",
    "                        f\"{', '.join(nested_types)})\"\n",
    "                    )\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED DATASET WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Dataset with full hierarchical event structure\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        x: [B, L] time series tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L} with hierarchical structure...\")\n",
    "        \n",
    "        # Initialize all detectors (reuse from previous implementation)\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        self.cp_detector = ChangePointDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode step labels\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build hierarchical annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical event structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_hierarchical_annotation(i)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Dataset ready with hierarchical structure\")\n",
    "            self._print_statistics()\n",
    "    \n",
    "    def _build_hierarchical_annotation(self, idx: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # 1. Detect all events (as before)\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        change_pts = self.cp_detector.detect(self.x[idx], idx)\n",
    "        \n",
    "        # 2. Add trend segments to builder\n",
    "        for seg in trends:\n",
    "            builder.add_event(\n",
    "                start=seg.start,\n",
    "                end=seg.end,\n",
    "                label=seg.label,\n",
    "                event_type='trend',\n",
    "                confidence=0.9,\n",
    "                metadata={\n",
    "                    'direction': seg.direction,\n",
    "                    'slope': seg.slope,\n",
    "                    'strength': seg.strength\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 3. Add peaks/troughs\n",
    "        for pt in peaks:\n",
    "            builder.add_event(\n",
    "                start=pt.index,\n",
    "                end=pt.index,\n",
    "                label=pt.label,\n",
    "                event_type='peak_trough',\n",
    "                confidence=0.85,\n",
    "                metadata={\n",
    "                    'peak_type': pt.type,\n",
    "                    'prominence': pt.prominence\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 4. Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(\n",
    "                start=vr.start,\n",
    "                end=vr.end,\n",
    "                label=vr.label,\n",
    "                event_type='volatility',\n",
    "                confidence=0.8,\n",
    "                metadata={\n",
    "                    'level': vr.level,\n",
    "                    'avg_vol': vr.avg_vol\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 5. Add change points\n",
    "        for cp in change_pts:\n",
    "            builder.add_event(\n",
    "                start=cp.index,\n",
    "                end=cp.index,\n",
    "                label=cp.label,\n",
    "                event_type='changepoint',\n",
    "                confidence=0.75,\n",
    "                metadata={\n",
    "                    'cp_type': cp.type,\n",
    "                    'magnitude': cp.magnitude\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # 6. Add global regime (macro-level characterization)\n",
    "        L = len(self.x[idx])\n",
    "        global_regime = self._classify_global_regime(idx, L)\n",
    "        builder.add_event(\n",
    "            start=0,\n",
    "            end=L-1,\n",
    "            label=global_regime,\n",
    "            event_type='global_regime',\n",
    "            confidence=0.7,\n",
    "            metadata={'scope': 'global'}\n",
    "        )\n",
    "        \n",
    "        # 7. Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int, length: int) -> int:\n",
    "        \"\"\"Classify overall regime for entire sequence\"\"\"\n",
    "        # Use net slope across entire sequence\n",
    "        if 'slope_100' in self.features:\n",
    "            avg_slope = self.features['slope_100'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        # Use average volatility\n",
    "        if 'std_20' in self.features:\n",
    "            avg_vol = self.features['std_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_vol = 0.5\n",
    "        \n",
    "        # Classify\n",
    "        if avg_vol > 1.5:\n",
    "            return VOCAB.VOLATILE_REGIME\n",
    "        elif avg_slope > 0.1:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.1:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print hierarchical statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"HIERARCHICAL STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Count events by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        total_events = 0\n",
    "        max_depth = 0\n",
    "        \n",
    "        for ann in self.annotations:\n",
    "            total_events += len(ann.all_events)\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "                max_depth = max(max_depth, event.depth)\n",
    "        \n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        print(f\"Sequences: {len(self.annotations)}\")\n",
    "        print(f\"Avg events per sequence: {avg_events:.1f}\")\n",
    "        print(f\"Max hierarchy depth: {max_depth}\")\n",
    "        print(f\"\\nEvents by scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"  {scale.name:.<20} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalTextGenerator:\n",
    "    \"\"\"Generate training text that preserves hierarchical structure\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_flat_sequential(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Flat list sorted by start (loses hierarchy)\"\"\"\n",
    "        events = sorted(ann.all_events, key=lambda e: e.start)\n",
    "        \n",
    "        tokens = []\n",
    "        for e in events:\n",
    "            tokens.append(f\"[{e.start}-{e.end}]{e.label_name}\")\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_depth_marked(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Include depth markers to indicate nesting\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            depth_str = \">\" * node.depth\n",
    "            parts.append(\n",
    "                f\"{depth_str}[{node.start}-{node.end}]{node.label_name}\"\n",
    "            )\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_xml_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"XML-like nested structure\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent, depth: int = 0):\n",
    "            indent = \"  \" * depth\n",
    "            if node.children:\n",
    "                lines.append(f\"{indent}<event type='{node.label_name}' span='{node.start}-{node.end}'>\")\n",
    "                for child in node.children:\n",
    "                    traverse(child, depth + 1)\n",
    "                lines.append(f\"{indent}</event>\")\n",
    "            else:\n",
    "                lines.append(\n",
    "                    f\"{indent}<event type='{node.label_name}' \"\n",
    "                    f\"span='{node.start}-{node.end}' />\"\n",
    "                )\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_json_style(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"JSON-like hierarchical structure\"\"\"\n",
    "        import json\n",
    "        \n",
    "        def event_to_dict(node: HierarchicalEvent) -> Dict:\n",
    "            result = {\n",
    "                'span': [node.start, node.end],\n",
    "                'label': node.label_name,\n",
    "                'scale': node.scale.name,\n",
    "                'confidence': node.confidence\n",
    "            }\n",
    "            if node.children:\n",
    "                result['children'] = [event_to_dict(child) for child in node.children]\n",
    "            return result\n",
    "        \n",
    "        tree = [event_to_dict(root) for root in ann.event_roots]\n",
    "        return json.dumps(tree, separators=(',', ':'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_narrative_with_context(ann: HierarchicalAnnotation) -> str:\n",
    "        \"\"\"Natural language that mentions parent context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        def traverse(node: HierarchicalEvent):\n",
    "            # Describe the event\n",
    "            if node.start == node.end:\n",
    "                span_desc = f\"At position {node.start}\"\n",
    "            else:\n",
    "                span_desc = f\"From {node.start} to {node.end}\"\n",
    "            \n",
    "            event_desc = node.label_name.lower().replace('_', ' ')\n",
    "            \n",
    "            # Add parent context if exists\n",
    "            if node.parent:\n",
    "                parent_desc = node.parent.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(\n",
    "                    f\"{span_desc}, {event_desc} occurs \"\n",
    "                    f\"(within {parent_desc} [{node.parent.start}-{node.parent.end}]).\"\n",
    "                )\n",
    "            else:\n",
    "                sentences.append(f\"{span_desc}, {event_desc}.\")\n",
    "            \n",
    "            # Recurse\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        \n",
    "        for root in ann.event_roots:\n",
    "            traverse(root)\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED EXAMPLE WITH HIERARCHY\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate test data\n",
    "    B = 50\n",
    "    L = 336\n",
    "    \n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Multi-scale trend\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Volatility clusters\n",
    "        vol = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol\n",
    "        \n",
    "        # Spikes\n",
    "        spike_indices = torch.randint(0, L, (3,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(3) * 2\n",
    "        \n",
    "        # Local corrections (create nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Add dip in middle of uptrend\n",
    "            x[i, 150:180] = x[i, 150:180] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create hierarchical dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Example annotation\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HIERARCHICAL STRUCTURE EXAMPLE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EVENTS BY SCALE\")\n",
    "    print(\"=\" * 80)\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate different text formats\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT GENERATION FORMATS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text_gen = HierarchicalTextGenerator()\n",
    "    \n",
    "    print(\"\\n1. DEPTH-MARKED FORMAT:\")\n",
    "    print(text_gen.generate_depth_marked(ann)[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n2. NARRATIVE WITH CONTEXT:\")\n",
    "    narrative = text_gen.generate_narrative_with_context(ann)\n",
    "    print(narrative[:400] + \"...\")\n",
    "    \n",
    "    print(\"\\n3. JSON-STYLE (first 500 chars):\")\n",
    "    print(text_gen.generate_json_style(ann)[:500] + \"...\")\n",
    "    \n",
    "    # Token count comparison\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOKEN COUNTS BY FORMAT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    formats = {\n",
    "        'Flat sequential': text_gen.generate_flat_sequential(ann),\n",
    "        'Depth-marked': text_gen.generate_depth_marked(ann),\n",
    "        'Narrative': text_gen.generate_narrative_with_context(ann),\n",
    "        'JSON-style': text_gen.generate_json_style(ann)\n",
    "    }\n",
    "    \n",
    "    for name, text in formats.items():\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"{name:.<30} {tokens:>6} tokens, {chars:>6} chars\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ Hierarchical system complete!\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0226c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## 🎯 **What's New in This Version**\\n\\n### **1. True Hierarchical Structure** ✅\\n```\\n[0-335] SIDEWAYS_REGIME (GLOBAL)\\n  ├─ [0-120] UPTREND_LONG_MODERATE (MACRO)\\n  │   ├─ [30-45] DOWNTREND_SHORT_WEAK (MESO)  ← NESTED!\\n  │   │   └─ [38] SPIKE_DOWN_STRONG (MICRO)\\n  │   └─ [50-55] VOLATILITY_SPIKE (MINI)\\n  ├─ [121-200] FLAT_LONG (MACRO)\\n  └─ [201-335] DOWNTREND_LONG_STRONG (MACRO)\\n      └─ [250-265] LOCAL_PEAK_MODERATE (MESO)\\n```\\n\\n### **2. Event Scale Classification**\\n- **MICRO** (1-5 steps): Spikes, single peaks\\n- **MINI** (5-15 steps): Very short segments\\n- **MESO** (15-50 steps): Medium trends, local corrections\\n- **MACRO** (50-150 steps): Major trends\\n- **GLOBAL** (150+ steps): Overall regime\\n\\n### **3. Parent-Child Relationships**\\n- Automatic detection of containment\\n- Smallest containing event becomes parent\\n- Children sorted by start position\\n\\n### **4. Multiple Text Formats**\\n\\n**Depth-Marked (Token-Efficient):**\\n```\\n[0-335]SIDEWAYS_REGIME@GLOBAL >[0-120]UPTREND_LONG_MODERATE@MACRO >>[30-45]DOWNTREND_SHORT_WEAK@MESO >>>[38]SPIKE_DOWN_STRONG@MICRO\\n```\\n\\n**Narrative with Context:**\\n```\\nFrom 0 to 335, sideways regime. From 0 to 120, uptrend long moderate. \\nFrom 30 to 45, downtrend short weak (within uptrend long moderate [0-120]). \\nAt position 38, spike down strong (within downtrend short weak [30-45]). \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## 🎯 **What's New in This Version**\n",
    "\n",
    "### **1. True Hierarchical Structure** ✅\n",
    "```\n",
    "[0-335] SIDEWAYS_REGIME (GLOBAL)\n",
    "  ├─ [0-120] UPTREND_LONG_MODERATE (MACRO)\n",
    "  │   ├─ [30-45] DOWNTREND_SHORT_WEAK (MESO)  ← NESTED!\n",
    "  │   │   └─ [38] SPIKE_DOWN_STRONG (MICRO)\n",
    "  │   └─ [50-55] VOLATILITY_SPIKE (MINI)\n",
    "  ├─ [121-200] FLAT_LONG (MACRO)\n",
    "  └─ [201-335] DOWNTREND_LONG_STRONG (MACRO)\n",
    "      └─ [250-265] LOCAL_PEAK_MODERATE (MESO)\n",
    "```\n",
    "\n",
    "### **2. Event Scale Classification**\n",
    "- **MICRO** (1-5 steps): Spikes, single peaks\n",
    "- **MINI** (5-15 steps): Very short segments\n",
    "- **MESO** (15-50 steps): Medium trends, local corrections\n",
    "- **MACRO** (50-150 steps): Major trends\n",
    "- **GLOBAL** (150+ steps): Overall regime\n",
    "\n",
    "### **3. Parent-Child Relationships**\n",
    "- Automatic detection of containment\n",
    "- Smallest containing event becomes parent\n",
    "- Children sorted by start position\n",
    "\n",
    "### **4. Multiple Text Formats**\n",
    "\n",
    "**Depth-Marked (Token-Efficient):**\n",
    "```\n",
    "[0-335]SIDEWAYS_REGIME@GLOBAL >[0-120]UPTREND_LONG_MODERATE@MACRO >>[30-45]DOWNTREND_SHORT_WEAK@MESO >>>[38]SPIKE_DOWN_STRONG@MICRO\n",
    "```\n",
    "\n",
    "**Narrative with Context:**\n",
    "```\n",
    "From 0 to 335, sideways regime. From 0 to 120, uptrend long moderate. \n",
    "From 30 to 45, downtrend short weak (within uptrend long moderate [0-120]). \n",
    "At position 38, spike down strong (within downtrend short weak [30-45]). \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a1bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HIERARCHICAL EVENT LABELING SYSTEM - FIXED VERSION\n",
      "================================================================================\n",
      "\n",
      "Generating 20 sequences of length 336...\n",
      "Processing 20 sequences of length 336...\n",
      "Extracting features...\n",
      "Encoding step labels...\n",
      "Building hierarchical structure...\n",
      "  Sequence 0/20...\n",
      "✓ Complete! Avg events per sequence: 89.0\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE HIERARCHICAL ANNOTATION\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Events (Total: 85)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (61 children)\n",
      "  [000-020] FLAT_SEGMENT (scale=MESO) (3 children)\n",
      "    [003-003] LOCAL_TROUGH (scale=MICRO)\n",
      "    [009-009] LOCAL_PEAK (scale=MICRO)\n",
      "    [011-011] LOCAL_TROUGH (scale=MICRO)\n",
      "  [021-027] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "    [027-027] LOCAL_PEAK (scale=MICRO)\n",
      "  [032-032] LOCAL_TROUGH (scale=MICRO)\n",
      "  [035-035] LOCAL_PEAK (scale=MICRO)\n",
      "  [040-040] LOCAL_TROUGH (scale=MICRO)\n",
      "  [045-045] LOCAL_PEAK (scale=MICRO)\n",
      "  [046-046] LOCAL_TROUGH (scale=MICRO)\n",
      "  [053-053] LOCAL_PEAK (scale=MICRO)\n",
      "  [056-056] LOCAL_TROUGH (scale=MICRO)\n",
      "  [061-061] LOCAL_PEAK (scale=MICRO)\n",
      "  [062-062] LOCAL_TROUGH (scale=MICRO)\n",
      "  [066-066] LOCAL_PEAK (scale=MICRO)\n",
      "  [070-070] LOCAL_TROUGH (scale=MICRO)\n",
      "  [071-071] LOCAL_PEAK (scale=MICRO)\n",
      "  [076-096] FLAT_SEGMENT (scale=MESO) (5 children)\n",
      "    [078-078] LOCAL_TROUGH (scale=MICRO)\n",
      "    [086-086] LOCAL_PEAK (scale=MICRO)\n",
      "    [092-092] LOCAL_TROUGH (scale=MICRO)\n",
      "    [094-094] LOCAL_PEAK (scale=MICRO)\n",
      "    [096-096] LOCAL_TROUGH (scale=MICRO)\n",
      "  [103-108] FLAT_SEGMENT (scale=MINI)\n",
      "  [109-109] LOCAL_PEAK (scale=MICRO)\n",
      "  [112-112] LOCAL_TROUGH (scale=MICRO)\n",
      "  [116-137] FLAT_SEGMENT (scale=MESO) (2 children)\n",
      "    [128-128] LOCAL_PEAK (scale=MICRO)\n",
      "    [130-130] LOCAL_TROUGH (scale=MICRO)\n",
      "  [138-138] SHARP_PEAK (scale=MICRO)\n",
      "  [144-144] LOCAL_TROUGH (scale=MICRO)\n",
      "  [145-151] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "    [146-146] LOCAL_PEAK (scale=MICRO)\n",
      "    [150-150] LOCAL_TROUGH (scale=MICRO)\n",
      "  [155-155] LOCAL_PEAK (scale=MICRO)\n",
      "  [156-156] LOCAL_TROUGH (scale=MICRO)\n",
      "  [159-159] LOCAL_PEAK (scale=MICRO)\n",
      "  [160-160] LOCAL_TROUGH (scale=MICRO)\n",
      "  [161-161] LOCAL_PEAK (scale=MICRO)\n",
      "  [168-168] LOCAL_TROUGH (scale=MICRO)\n",
      "  [169-169] LOCAL_PEAK (scale=MICRO)\n",
      "  [172-172] LOCAL_TROUGH (scale=MICRO)\n",
      "  [175-175] LOCAL_PEAK (scale=MICRO)\n",
      "  [186-186] LOCAL_TROUGH (scale=MICRO)\n",
      "  [192-192] LOCAL_PEAK (scale=MICRO)\n",
      "  [196-196] LOCAL_TROUGH (scale=MICRO)\n",
      "  [201-201] LOCAL_PEAK (scale=MICRO)\n",
      "  [203-203] LOCAL_TROUGH (scale=MICRO)\n",
      "  [206-206] LOCAL_PEAK (scale=MICRO)\n",
      "  [212-217] FLAT_SEGMENT (scale=MINI) (2 children)\n",
      "    [215-215] SHARP_TROUGH (scale=MICRO)\n",
      "    [217-217] LOCAL_PEAK (scale=MICRO)\n",
      "  [218-218] LOCAL_TROUGH (scale=MICRO)\n",
      "  [219-219] LOCAL_PEAK (scale=MICRO)\n",
      "  [221-239] FLAT_SEGMENT (scale=MESO) (5 children)\n",
      "    [221-221] LOCAL_TROUGH (scale=MICRO)\n",
      "    [223-223] LOCAL_PEAK (scale=MICRO)\n",
      "    [227-227] LOCAL_TROUGH (scale=MICRO)\n",
      "    [231-231] LOCAL_PEAK (scale=MICRO)\n",
      "    [234-234] LOCAL_TROUGH (scale=MICRO)\n",
      "  [240-240] SHARP_PEAK (scale=MICRO)\n",
      "  [244-244] LOCAL_TROUGH (scale=MICRO)\n",
      "  [247-247] LOCAL_PEAK (scale=MICRO)\n",
      "  [259-259] LOCAL_TROUGH (scale=MICRO)\n",
      "  [263-263] LOCAL_PEAK (scale=MICRO)\n",
      "  [264-264] LOCAL_TROUGH (scale=MICRO)\n",
      "  [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "  [269-276] FLAT_SEGMENT (scale=MINI) (3 children)\n",
      "    [270-270] LOCAL_TROUGH (scale=MICRO)\n",
      "    [273-273] LOCAL_PEAK (scale=MICRO)\n",
      "    [274-274] SHARP_TROUGH (scale=MICRO)\n",
      "  [281-281] LOCAL_PEAK (scale=MICRO)\n",
      "  [289-289] LOCAL_TROUGH (scale=MICRO)\n",
      "  [309-309] LOCAL_PEAK (scale=MICRO)\n",
      "  [310-310] LOCAL_TROUGH (scale=MICRO)\n",
      "  [312-312] LOCAL_PEAK (scale=MICRO)\n",
      "  [313-313] LOCAL_TROUGH (scale=MICRO)\n",
      "  [318-318] LOCAL_PEAK (scale=MICRO)\n",
      "  [319-319] LOCAL_TROUGH (scale=MICRO)\n",
      "  [322-322] LOCAL_PEAK (scale=MICRO)\n",
      "  [323-323] LOCAL_TROUGH (scale=MICRO)\n",
      "  [329-329] LOCAL_PEAK (scale=MICRO)\n",
      "  [330-330] LOCAL_TROUGH (scale=MICRO)\n",
      "\n",
      "================================================================================\n",
      "TEXT OUTPUT\n",
      "================================================================================\n",
      "[0-335]SIDEWAYS_REGIME >[0-20]FLAT_SEGMENT >>[3-3]LOCAL_TROUGH >>[9-9]LOCAL_PEAK >>[11-11]LOCAL_TROUGH >[21-27]UPTREND_SHORT >>[27-27]LOCAL_PEAK >[32-32]LOCAL_TROUGH >[35-35]LOCAL_PEAK >[40-40]LOCAL_TROUGH >[45-45]LOCAL_PEAK >[46-46]LOCAL_TROUGH >[53-53]LOCAL_PEAK >[56-56]LOCAL_TROUGH >[61-61]LOCAL_PEAK >[62-62]LOCAL_TROUGH >[66-66]LOCAL_PEAK >[70-70]LOCAL_TROUGH >[71-71]LOCAL_PEAK >[76-96]FLAT_SEGMENT >>[78-78]LOCAL_TROUGH >>[86-86]LOCAL_PEAK >>[92-92]LOCAL_TROUGH >>[94-94]LOCAL_PEAK >>[96-96]L...\n",
      "\n",
      "Total tokens: ~85\n",
      "\n",
      "================================================================================\n",
      "✓ System working correctly!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COMPLETE HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "========================================================\n",
    "Fixed and production-ready version\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VOCABULARY (same as before)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"Complete event vocabulary\"\"\"\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trends\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls):\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HIERARCHICAL STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels\"\"\"\n",
    "    MICRO = 1      # 1-5 steps\n",
    "    MINI = 2       # 5-15 steps\n",
    "    MESO = 3       # 15-50 steps\n",
    "    MACRO = 4      # 50-150 steps\n",
    "    GLOBAL = 5     # 150+ steps\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"Event node in hierarchy tree\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FIXED FEATURE EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"Extract features at multiple temporal scales - FIXED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, scales=[5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B, L] time series\n",
    "        Returns: dict of features at multiple scales\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # First derivative\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling std\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Simple rolling slope\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"Simple slope computation\"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            # Slope from start to end of window\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "# ============================================================================\n",
    "# 4. STEP ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"Encode each timestep - SIMPLIFIED\"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Quantiles\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Classify\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SIMPLE DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "class SimpleTrendDetector:\n",
    "    \"\"\"Simplified trend detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Use slope if available\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "class SimplePeakDetector:\n",
    "    \"\"\"Simplified peak detector\"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        x_np = x.cpu().numpy()\n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(x_np, prominence=0.2)\n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                label = VOCAB.SHARP_PEAK if prom > 0.5 else VOCAB.LOCAL_PEAK\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(-x_np, prominence=0.2)\n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                label = VOCAB.SHARP_TROUGH if prom > 0.5 else VOCAB.LOCAL_TROUGH\n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={'prominence': float(prom)}\n",
    "                ))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return events\n",
    "\n",
    "# ============================================================================\n",
    "# 6. HIERARCHY BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"Build hierarchical structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# 7. HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"Generate hierarchical text\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"Main dataset class - FIXED\"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {B} sequences of length {L}...\")\n",
    "        \n",
    "        # Initialize\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = SimpleTrendDetector()\n",
    "        self.peak_detector = SimplePeakDetector()\n",
    "        \n",
    "        # Extract features\n",
    "        if verbose:\n",
    "            print(\"Extracting features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        \n",
    "        # Encode steps\n",
    "        if verbose:\n",
    "            print(\"Encoding step labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        \n",
    "        # Build annotations\n",
    "        if verbose:\n",
    "            print(\"Building hierarchical structure...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"  Sequence {i}/{B}...\")\n",
    "            \n",
    "            builder = HierarchicalEventBuilder()\n",
    "            \n",
    "            # Detect trends\n",
    "            trends = self.trend_detector.detect(x[i], self.features, i)\n",
    "            for seg in trends:\n",
    "                builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                                metadata=seg.metadata)\n",
    "            \n",
    "            # Detect peaks\n",
    "            peaks = self.peak_detector.detect(x[i], i)\n",
    "            for pk in peaks:\n",
    "                builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                                metadata=pk.metadata)\n",
    "            \n",
    "            # Add global regime\n",
    "            if 'slope_20' in self.features:\n",
    "                avg_slope = self.features['slope_20'][i].mean().item()\n",
    "            else:\n",
    "                avg_slope = 0\n",
    "            \n",
    "            if avg_slope > 0.05:\n",
    "                global_label = VOCAB.BULLISH_REGIME\n",
    "            elif avg_slope < -0.05:\n",
    "                global_label = VOCAB.BEARISH_REGIME\n",
    "            else:\n",
    "                global_label = VOCAB.SIDEWAYS_REGIME\n",
    "            \n",
    "            builder.add_event(0, L-1, global_label, 'regime')\n",
    "            \n",
    "            # Build hierarchy\n",
    "            roots = builder.build_hierarchy()\n",
    "            all_events = builder.get_flat_list(roots)\n",
    "            \n",
    "            self.annotations.append(HierarchicalAnnotation(\n",
    "                sequence=x[i],\n",
    "                step_labels=self.step_labels[i],\n",
    "                event_roots=roots,\n",
    "                all_events=all_events\n",
    "            ))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Complete! Avg events per sequence: {self._avg_events():.1f}\")\n",
    "    \n",
    "    def _avg_events(self):\n",
    "        return sum(len(a.all_events) for a in self.annotations) / len(self.annotations)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "\n",
    "# ============================================================================\n",
    "# 9. TEST\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"HIERARCHICAL EVENT LABELING SYSTEM - FIXED VERSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} sequences of length {L}...\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        noise = 0.1 * torch.randn(L)\n",
    "        spikes = torch.zeros(L)\n",
    "        spike_idx = torch.randint(50, L-50, (2,))\n",
    "        spikes[spike_idx] = torch.randn(2) * 2\n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE HIERARCHICAL ANNOTATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEXT OUTPUT\")\n",
    "    print(\"=\" * 80)\n",
    "    text = ann.to_text()\n",
    "    print(text[:500] + \"...\")\n",
    "    print(f\"\\nTotal tokens: ~{len(text.split())}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ System working correctly!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf68f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
      "Demonstration\n",
      "================================================================================\n",
      "\n",
      "Generating 20 synthetic sequences of length 336...\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 20\n",
      "Length: 336\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 6720 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/20...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 998\n",
      "      Avg per sequence: 49.9\n",
      "      By scale:\n",
      "        MICRO.......   23.2 per sequence\n",
      "        MINI........   17.6 per sequence\n",
      "        MESO........    8.1 per sequence\n",
      "        MACRO.......    0.0 per sequence\n",
      "        GLOBAL......    1.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: HIERARCHICAL STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Events (Total: 54)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (26 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO)\n",
      "  [003-018] NORMAL_VOLATILITY (scale=MESO) (2 children)\n",
      "    [003-003] SHARP_TROUGH (scale=MICRO)\n",
      "    [009-009] SHARP_PEAK (scale=MICRO)\n",
      "  [019-030] HIGH_VOLATILITY (scale=MINI)\n",
      "  [021-027] UPTREND_SHORT (scale=MINI)\n",
      "  [032-032] LOCAL_TROUGH (scale=MICRO)\n",
      "  [034-044] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [045-060] LOW_VOLATILITY (scale=MESO) (2 children)\n",
      "    [045-045] LOCAL_PEAK (scale=MICRO)\n",
      "    [056-056] SHARP_TROUGH (scale=MICRO)\n",
      "  [061-077] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [063-068] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "      [066-066] SHARP_PEAK (scale=MICRO)\n",
      "  [078-078] SHARP_TROUGH (scale=MICRO)\n",
      "  [081-097] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [094-094] LOCAL_PEAK (scale=MICRO)\n",
      "  [098-127] LOW_VOLATILITY (scale=MESO) (3 children)\n",
      "    [103-108] FLAT_SEGMENT (scale=MINI)\n",
      "    [112-112] SHARP_TROUGH (scale=MICRO)\n",
      "    [116-122] FLAT_SEGMENT (scale=MINI)\n",
      "  [128-137] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [128-128] SHARP_PEAK (scale=MICRO)\n",
      "  [138-157] HIGH_VOLATILITY (scale=MESO) (1 children)\n",
      "    [138-138] SHARP_TROUGH (scale=MICRO)\n",
      "  [158-168] LOW_VOLATILITY (scale=MINI)\n",
      "  [169-185] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [175-175] SHARP_PEAK (scale=MICRO)\n",
      "  [186-186] SHARP_TROUGH (scale=MICRO)\n",
      "  [188-211] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [192-192] SHARP_PEAK (scale=MICRO)\n",
      "  [213-218] HIGH_VOLATILITY (scale=MINI) (1 children)\n",
      "    [213-213] SHARP_TROUGH (scale=MICRO)\n",
      "  [219-232] VOLATILITY_SPIKE (scale=MINI) (1 children)\n",
      "    [219-219] LOCAL_PEAK (scale=MICRO)\n",
      "  [240-259] VOLATILITY_SPIKE (scale=MESO) (3 children)\n",
      "    [240-240] SHARP_TROUGH (scale=MICRO)\n",
      "    [247-247] SHARP_PEAK (scale=MICRO)\n",
      "    [259-259] LOCAL_TROUGH (scale=MICRO)\n",
      "  [260-270] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "  [271-276] FLAT_SEGMENT (scale=MINI) (1 children)\n",
      "    [274-274] LOCAL_TROUGH (scale=MICRO)\n",
      "  [271-279] LOW_VOLATILITY (scale=MINI)\n",
      "  [277-286] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "    [281-281] SHARP_PEAK (scale=MICRO)\n",
      "  [280-321] NORMAL_VOLATILITY (scale=MESO) (3 children)\n",
      "    [289-289] SHARP_TROUGH (scale=MICRO)\n",
      "    [296-296] SHARP_PEAK (scale=MICRO)\n",
      "    [307-307] SHARP_TROUGH (scale=MICRO)\n",
      "  [322-335] LOW_VOLATILITY (scale=MINI) (2 children)\n",
      "    [322-322] LOCAL_PEAK (scale=MICRO)\n",
      "    [330-330] LOCAL_TROUGH (scale=MICRO)\n",
      "\n",
      "================================================================================\n",
      "EVENTS BY HIERARCHICAL SCALE\n",
      "================================================================================\n",
      "\n",
      "MICRO (27 events):\n",
      "  [003-003] SHARP_TROUGH\n",
      "  [009-009] SHARP_PEAK\n",
      "  [032-032] LOCAL_TROUGH\n",
      "  [045-045] LOCAL_PEAK\n",
      "  [056-056] SHARP_TROUGH\n",
      "\n",
      "MINI (15 events):\n",
      "  [019-030] HIGH_VOLATILITY\n",
      "  [021-027] UPTREND_SHORT\n",
      "  [034-044] NORMAL_VOLATILITY\n",
      "  [063-068] UPTREND_SHORT\n",
      "  [103-108] FLAT_SEGMENT\n",
      "\n",
      "MESO (11 events):\n",
      "  [000-019] FLAT_SEGMENT\n",
      "  [003-018] NORMAL_VOLATILITY\n",
      "  [045-060] LOW_VOLATILITY\n",
      "  [061-077] NORMAL_VOLATILITY\n",
      "  [081-097] NORMAL_VOLATILITY\n",
      "\n",
      "MACRO (0 events):\n",
      "\n",
      "GLOBAL (1 events):\n",
      "  [000-335] SIDEWAYS_REGIME\n",
      "\n",
      "================================================================================\n",
      "TEXT GENERATION FOR LM TRAINING\n",
      "================================================================================\n",
      "\n",
      "DEPTH_MARKED:\n",
      "  Tokens: 54, Chars: 1277\n",
      "  Preview: [0-335]SIDEWAYS_REGIME >[0-19]FLAT_SEGMENT >[3-18]NORMAL_VOLATILITY >>[3-3]SHARP_TROUGH >>[9-9]SHARP_PEAK >[19-30]HIGH_VOLATILITY >[21-27]UPTREND_SHORT >[32-32]LOCAL_TROUGH >[34-44]NORMAL_VOLATILITY >...\n",
      "\n",
      "FLAT:\n",
      "  Tokens: 54, Chars: 1196\n",
      "  Preview: [0-335]SIDEWAYS_REGIME [0-19]FLAT_SEGMENT [3-18]NORMAL_VOLATILITY [3-3]SHARP_TROUGH [9-9]SHARP_PEAK [19-30]HIGH_VOLATILITY [21-27]UPTREND_SHORT [32-32]LOCAL_TROUGH [34-44]NORMAL_VOLATILITY [45-60]LOW_...\n",
      "\n",
      "NARRATIVE:\n",
      "  Tokens: 3, Chars: 25\n",
      "  Preview: Overall: sideways regime....\n",
      "\n",
      "================================================================================\n",
      "FULL CORPUS STATISTICS\n",
      "================================================================================\n",
      "  num_documents: 20\n",
      "  total_tokens: 998\n",
      "  total_chars: 23,894\n",
      "  avg_tokens_per_doc: 49.9\n",
      "  avg_chars_per_doc: 1,194.7\n",
      "\n",
      "================================================================================\n",
      "✓ DEMONSTRATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "The system is ready for processing real time series data.\n",
      "Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "===============================================\n",
    "\n",
    "A comprehensive system for detecting and labeling events in time series data\n",
    "with hierarchical structure preservation.\n",
    "\n",
    "Workflow:\n",
    "    1. Define vocabulary and hierarchical structures\n",
    "    2. Extract multi-scale features from raw time series\n",
    "    3. Encode step-wise labels for each timestep\n",
    "    4. Detect higher-level events (trends, peaks, volatility, change points)\n",
    "    5. Build hierarchical event tree\n",
    "    6. Generate training text in various formats\n",
    "\n",
    "Author: Sachith\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels for events\"\"\"\n",
    "    MICRO = 1      # 1-5 timesteps (spikes, single points)\n",
    "    MINI = 2       # 5-15 timesteps (very short segments)\n",
    "    MESO = 3       # 15-50 timesteps (medium segments, local patterns)\n",
    "    MACRO = 4      # 50-150 timesteps (major trends)\n",
    "    GLOBAL = 5     # 150+ timesteps (full sequence characteristics)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"\n",
    "    Complete event vocabulary with 64 distinct labels.\n",
    "    \n",
    "    Categories:\n",
    "        - Special tokens (0-2)\n",
    "        - Step movements (3-10)\n",
    "        - Trend segments (20-26)\n",
    "        - Peaks/troughs (30-33)\n",
    "        - Volatility regimes (40-43)\n",
    "        - Change points (50-51)\n",
    "        - Global regimes (60-63)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    \n",
    "    # Step-level movements\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trend segments\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks and troughs\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility regimes\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Global regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls) -> int:\n",
    "        \"\"\"Return total vocabulary size\"\"\"\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        \"\"\"Convert label ID to string name\"\"\"\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"\n",
    "    Event node in hierarchical tree structure.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting timestep index\n",
    "        end: Ending timestep index\n",
    "        label: Vocabulary ID\n",
    "        label_name: Human-readable label\n",
    "        scale: Hierarchical scale level\n",
    "        event_type: Category (trend/peak/volatility/changepoint/regime)\n",
    "        confidence: Detection confidence score\n",
    "        metadata: Additional event-specific information\n",
    "        parent: Parent event in hierarchy (None for root)\n",
    "        children: List of child events\n",
    "    \"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        \"\"\"Duration in timesteps\"\"\"\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy tree (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event fully contains another event\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "\n",
    "# Global vocabulary instance\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features at multiple temporal scales using efficient convolutions.\n",
    "    \n",
    "    Features computed:\n",
    "        - First derivative (dx)\n",
    "        - Rolling mean at multiple window sizes\n",
    "        - Rolling standard deviation (volatility)\n",
    "        - Rolling slope (trend strength)\n",
    "        - Z-scores for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        scales: List of window sizes for rolling features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scales: List[int] = [5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from time series batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor of shape [B, L]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature tensors, each shape [B, L]:\n",
    "                - 'dx': First derivative\n",
    "                - 'mean_{w}': Rolling mean with window w\n",
    "                - 'std_{w}': Rolling std with window w\n",
    "                - 'slope_{w}': Rolling slope with window w\n",
    "                - 'zscore': Normalized z-scores\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        features = {}\n",
    "        \n",
    "        # First derivative (rate of change)\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean using efficient convolution\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling standard deviation (volatility measure)\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (trend direction and strength)\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores for outlier detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rolling linear slope over window.\n",
    "        \n",
    "        Simple implementation: slope = (end - start) / window\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STEP-WISE LABEL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"\n",
    "    Encode each timestep with symbolic movement labels.\n",
    "    \n",
    "    Labels based on magnitude of first derivative:\n",
    "        - FLAT: negligible change\n",
    "        - UP/DOWN_SMALL/MEDIUM/LARGE: quantile-based magnitude bins\n",
    "        - SPIKE_UP/DOWN: extreme changes (>90th percentile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode step-wise movement labels for entire batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor [B, L]\n",
    "            features: Feature dictionary from MultiScaleFeatureExtractor\n",
    "        \n",
    "        Returns:\n",
    "            Label tensor [B, L] with vocabulary IDs\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padded value\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Compute quantiles for adaptive thresholding\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33  # Flat threshold\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat (negligible change)\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: EVENT DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    \"\"\"Simple segment representation for detector outputs\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"\n",
    "    Detect trend segments using slope sign changes.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Compute rolling slopes\n",
    "        2. Find sign changes (trend reversals)\n",
    "        3. Classify segments by direction and duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect trend segments in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of trend segments\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Get slopes\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend direction changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify segment\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs using scipy's find_peaks with proper filtering.\n",
    "    \n",
    "    Key improvements:\n",
    "        - Minimum distance enforcement (prevent adjacent peaks)\n",
    "        - Adaptive prominence thresholds\n",
    "        - Peak-trough alternation validation\n",
    "    \n",
    "    Classifies peaks/troughs by:\n",
    "        - Prominence: How much the peak stands out\n",
    "        - Type: Sharp vs broad based on width\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_distance: int = 10, min_prominence_percentile: float = 75):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_distance: Minimum timesteps between peaks/troughs\n",
    "            min_prominence_percentile: Percentile for adaptive prominence threshold\n",
    "        \"\"\"\n",
    "        self.min_distance = min_distance\n",
    "        self.min_prominence_percentile = min_prominence_percentile\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            idx: Sequence index (unused but kept for consistency)\n",
    "        \n",
    "        Returns:\n",
    "            List of peak/trough events (alternating peaks and troughs)\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        \n",
    "        # Compute adaptive prominence threshold\n",
    "        std = np.std(x_np)\n",
    "        min_prominence = max(0.2 * std, 0.1)  # At least 0.2 std or 0.1 absolute\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(\n",
    "                x_np, \n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_PEAK\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_PEAK\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'peak'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs (peaks of inverted signal)\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(\n",
    "                -x_np,\n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_TROUGH\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_TROUGH\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'trough'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Sort by position and validate alternation\n",
    "        events = self._validate_alternation(events)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _validate_alternation(self, events: List[SimpleSegment]) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Ensure peaks and troughs alternate (remove consecutive same-type events).\n",
    "        Keep the more prominent one when there are consecutive same types.\n",
    "        \"\"\"\n",
    "        if len(events) <= 1:\n",
    "            return events\n",
    "        \n",
    "        # Sort by position\n",
    "        events.sort(key=lambda e: e.start)\n",
    "        \n",
    "        filtered = [events[0]]\n",
    "        \n",
    "        for event in events[1:]:\n",
    "            last_event = filtered[-1]\n",
    "            \n",
    "            # Check if types alternate\n",
    "            last_type = last_event.metadata.get('type')\n",
    "            curr_type = event.metadata.get('type')\n",
    "            \n",
    "            if last_type == curr_type:\n",
    "                # Same type consecutive - keep more prominent\n",
    "                if event.metadata['prominence'] > last_event.metadata['prominence']:\n",
    "                    filtered[-1] = event  # Replace with more prominent\n",
    "                # else: keep the existing one\n",
    "            else:\n",
    "                # Different types - check minimum distance\n",
    "                if event.start - last_event.start >= self.min_distance // 2:\n",
    "                    filtered.append(event)\n",
    "                # else: skip (too close even if different types)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect volatility regimes using rolling standard deviation.\n",
    "    \n",
    "    Classifies regimes by quantile thresholds:\n",
    "        - LOW: Below 25th percentile\n",
    "        - NORMAL: 25th-75th percentile\n",
    "        - HIGH: Above 75th percentile\n",
    "        - SPIKE: Above 90th percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect volatility regimes in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of volatility regime segments\n",
    "        \"\"\"\n",
    "        if 'std_20' not in features:\n",
    "            return []\n",
    "        \n",
    "        vol = features['std_20'][idx].cpu().numpy()\n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantile thresholds\n",
    "        q25, q75, q90 = np.percentile(vol, [25, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q75)] = 1  # normal\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 2  # high\n",
    "        vol_levels[vol > q90] = 3  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end+1].mean()\n",
    "            \n",
    "            # Map to vocabulary\n",
    "            label_map = {\n",
    "                0: VOCAB.LOW_VOLATILITY,\n",
    "                1: VOCAB.NORMAL_VOLATILITY,\n",
    "                2: VOCAB.HIGH_VOLATILITY,\n",
    "                3: VOCAB.VOLATILITY_SPIKE\n",
    "            }\n",
    "            \n",
    "            regimes.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label_map[level_code],\n",
    "                metadata={'avg_volatility': float(avg_vol)}\n",
    "            ))\n",
    "        \n",
    "        return regimes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: HIERARCHICAL STRUCTURE BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"\n",
    "    Build hierarchical event tree from flat event list.\n",
    "    \n",
    "    Process:\n",
    "        1. Classify each event's scale based on duration\n",
    "        2. Sort events by scale (largest first)\n",
    "        3. Build parent-child relationships via containment\n",
    "        4. Sort children by temporal order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[HierarchicalEvent] = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add event to collection with automatic scale classification.\n",
    "        \n",
    "        Args:\n",
    "            start: Starting timestep\n",
    "            end: Ending timestep\n",
    "            label: Vocabulary ID\n",
    "            event_type: Category string\n",
    "            confidence: Detection confidence score\n",
    "            metadata: Additional information dictionary\n",
    "        \"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine hierarchical scale\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"\n",
    "        Build hierarchical tree structure.\n",
    "        \n",
    "        Returns:\n",
    "            List of root events (events with no parent)\n",
    "        \"\"\"\n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        # Build parent-child relationships\n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children within each parent\n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event (most specific parent)\"\"\"\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list via depth-first traversal\"\"\"\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"\n",
    "    Complete hierarchical annotation for one sequence.\n",
    "    \n",
    "    Attributes:\n",
    "        sequence: Original time series [L]\n",
    "        step_labels: Step-wise labels [L]\n",
    "        event_roots: Root nodes of hierarchy tree\n",
    "        all_events: Flattened list of all events\n",
    "    \"\"\"\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchical structure\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific hierarchical scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events overlapping with time range\"\"\"\n",
    "        return [e for e in self.all_events \n",
    "                if not (e.end < start or e.start > end)]\n",
    "    \n",
    "    def to_text(self, format: str = 'depth_marked') -> str:\n",
    "        \"\"\"\n",
    "        Generate text representation.\n",
    "        \n",
    "        Args:\n",
    "            format: Output format\n",
    "                - 'depth_marked': Depth indicators with events\n",
    "                - 'flat': Simple sequential list\n",
    "                - 'narrative': Natural language description\n",
    "        \n",
    "        Returns:\n",
    "            Text string for language model training\n",
    "        \"\"\"\n",
    "        if format == 'depth_marked':\n",
    "            return self._depth_marked_text()\n",
    "        elif format == 'flat':\n",
    "            return self._flat_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _depth_marked_text(self) -> str:\n",
    "        \"\"\"Depth markers indicate nesting: > >> >>> etc.\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _flat_text(self) -> str:\n",
    "        \"\"\"Simple sequential list (loses hierarchy)\"\"\"\n",
    "        events = sorted(self.all_events, key=lambda e: e.start)\n",
    "        return \" \".join(f\"[{e.start}-{e.end}]{e.label_name}\" for e in events)\n",
    "    \n",
    "    def _narrative_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with global view\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall: {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        # Describe macro events\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        if macro_events:\n",
    "            sentences.append(f\"{len(macro_events)} major segments detected.\")\n",
    "            for event in macro_events[:3]:\n",
    "                desc = event.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(f\"[{event.start}-{event.end}]: {desc}\")\n",
    "                if event.children:\n",
    "                    nested = \", \".join(set(c.event_type for c in event.children))\n",
    "                    sentences.append(f\"  (contains: {nested})\")\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MAIN DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Main dataset class for hierarchical event labeling.\n",
    "    \n",
    "    Processing pipeline:\n",
    "        1. Extract multi-scale features\n",
    "        2. Encode step-wise labels\n",
    "        3. Detect events (trends, peaks, volatility)\n",
    "        4. Add global regime classification\n",
    "        5. Build hierarchical structure\n",
    "        6. Create annotations\n",
    "    \n",
    "    Args:\n",
    "        x: Time series tensor [B, L]\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INITIALIZING HIERARCHICAL EVENT DATASET\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Sequences: {B}\")\n",
    "            print(f\"Length: {L}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        \n",
    "        # STEP 1: Extract features\n",
    "        if verbose:\n",
    "            print(f\"\\n[1/4] Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Computed {len(self.features)} feature types\")\n",
    "        \n",
    "        # STEP 2: Encode step labels\n",
    "        if verbose:\n",
    "            print(f\"[2/4] Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Encoded {B * L} timesteps\")\n",
    "        \n",
    "        # STEP 3: Detect events and build hierarchy\n",
    "        if verbose:\n",
    "            print(f\"[3/4] Detecting events and building hierarchy...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"      Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_annotation(i, L)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        # STEP 4: Compute statistics\n",
    "        if verbose:\n",
    "            print(f\"[4/4] Computing statistics...\")\n",
    "            self._print_statistics()\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"✓ DATASET READY\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _build_annotation(self, idx: int, L: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # Detect all event types\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        \n",
    "        # Add trend segments\n",
    "        for seg in trends:\n",
    "            builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                            confidence=0.9, metadata=seg.metadata)\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pk in peaks:\n",
    "            builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                            confidence=0.85, metadata=pk.metadata)\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(vr.start, vr.end, vr.label, 'volatility',\n",
    "                            confidence=0.8, metadata=vr.metadata)\n",
    "        \n",
    "        # Add global regime\n",
    "        global_label = self._classify_global_regime(idx)\n",
    "        builder.add_event(0, L-1, global_label, 'regime', confidence=0.7)\n",
    "        \n",
    "        # Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int) -> int:\n",
    "        \"\"\"Classify overall sequence regime\"\"\"\n",
    "        if 'slope_20' in self.features:\n",
    "            avg_slope = self.features['slope_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        if avg_slope > 0.05:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.05:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        # Count by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        for ann in self.annotations:\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "        \n",
    "        print(f\"      Total events: {total_events}\")\n",
    "        print(f\"      Avg per sequence: {avg_events:.1f}\")\n",
    "        print(f\"      By scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"        {scale.name:.<12} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'vocab_size': VOCAB.get_vocab_size(),\n",
    "            'total_events': total_events,\n",
    "            'avg_events_per_sequence': total_events / len(self.annotations),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TEXT GENERATION FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"\n",
    "    Generate training text in various formats.\n",
    "    \n",
    "    Formats:\n",
    "        - depth_marked: Hierarchical with depth indicators (>)\n",
    "        - flat: Simple sequential list\n",
    "        - narrative: Natural language description\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_corpus(dataset: HierarchicalEventDataset, \n",
    "                       format: str = 'depth_marked') -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text corpus for all sequences.\n",
    "        \n",
    "        Args:\n",
    "            dataset: HierarchicalEventDataset instance\n",
    "            format: Text format\n",
    "        \n",
    "        Returns:\n",
    "            List of text strings, one per sequence\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for annotation in dataset.annotations:\n",
    "            text = annotation.to_text(format=format)\n",
    "            corpus.append(text)\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(corpus: List[str]) -> Dict:\n",
    "        \"\"\"Estimate token counts for corpus\"\"\"\n",
    "        total_tokens = sum(len(text.split()) for text in corpus)\n",
    "        total_chars = sum(len(text) for text in corpus)\n",
    "        \n",
    "        return {\n",
    "            'num_documents': len(corpus),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_chars': total_chars,\n",
    "            'avg_tokens_per_doc': total_tokens / len(corpus),\n",
    "            'avg_chars_per_doc': total_chars / len(corpus),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEMONSTRATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic time series.\n",
    "    \n",
    "    Components:\n",
    "        - Multi-scale sinusoidal trends\n",
    "        - Volatility clusters\n",
    "        - Random spikes\n",
    "        - Local corrections (creates nested events)\n",
    "    \n",
    "    Args:\n",
    "        B: Batch size (number of sequences)\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend (multiple scales)\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol_modulator = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol_modulator\n",
    "        \n",
    "        # Add random spikes\n",
    "        num_spikes = np.random.randint(2, 5)\n",
    "        spike_indices = torch.randint(50, L-50, (num_spikes,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(num_spikes) * 2\n",
    "        \n",
    "        # Add local correction (creates nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Dip in middle of uptrend\n",
    "            start = L // 2\n",
    "            end = start + 30\n",
    "            x[i, start:end] = x[i, start:end] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def demonstrate_system():\n",
    "    \"\"\"Run complete demonstration of the system\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"Demonstration\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    x = generate_synthetic_data(B, L)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: HIERARCHICAL STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTS BY HIERARCHICAL SCALE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate text in different formats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    for fmt in formats:\n",
    "        text = ann.to_text(format=fmt)\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"\\n{fmt.upper()}:\")\n",
    "        print(f\"  Tokens: {tokens}, Chars: {chars}\")\n",
    "        print(f\"  Preview: {text[:200]}...\")\n",
    "    \n",
    "    # Generate full corpus\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL CORPUS STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    corpus = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    stats = text_gen.estimate_tokens(corpus)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,.1f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThe system is ready for processing real time series data.\")\n",
    "    print(\"Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE WORKFLOW EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "\n",
      "[2/6] Creating hierarchical dataset...\n",
      "  ✓ Processed 100 sequences\n",
      "\n",
      "[3/6] Exploring example annotation...\n",
      "================================================================================\n",
      "ANNOTATION EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "1. HIERARCHICAL TREE:\n",
      "\n",
      "Hierarchical Events (Total: 223)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (52 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO) (6 children)\n",
      "    [001-001] SHARP_PEAK (scale=MICRO)\n",
      "    [003-012] NORMAL_VOLATILITY (scale=MINI) (7 children)\n",
      "      [003-003] SHARP_TROUGH (scale=MICRO)\n",
      "      [004-004] SHARP_PEAK (scale=MICRO)\n",
      "      [005-005] SHARP_TROUGH (scale=MICRO)\n",
      "      [007-007] SHARP_PEAK (scale=MICRO)\n",
      "      [010-010] SHARP_TROUGH (scale=MICRO)\n",
      "      [011-011] SHARP_PEAK (scale=MICRO)\n",
      "      [012-012] SHARP_TROUGH (scale=MICRO)\n",
      "    [013-013] SHARP_PEAK (scale=MICRO)\n",
      "    [016-016] SHARP_TROUGH (scale=MICRO)\n",
      "    [017-017] SHARP_PEAK (scale=MICRO)\n",
      "    [019-019] SHARP_TROUGH (scale=MICRO)\n",
      "  [020-020] SHARP_PEAK (scale=MICRO)\n",
      "  [021-032] HIGH_VOLATILITY (scale=MINI) (7 children)\n",
      "    [022-022] SHARP_TROUGH (scale=MICRO)\n",
      "    [024-024] SHARP_PEAK (scale=MICRO)\n",
      "    [025-025] SHARP_TROUGH (scale=MICRO)\n",
      "    [026-026] SHARP_PEAK (scale=MICRO)\n",
      "    [028-028] SHARP_TROUGH (scale=MICRO)\n",
      "    [031-031] SHARP_PEAK (scale=MICRO)\n",
      "    [032-032] SHARP_TROUGH (scale=MICRO)\n",
      "  [033-038] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [033-033] SHARP_PEAK (scale=MICRO)\n",
      "    [035-035] SHARP_TROUGH (scale=MICRO)\n",
      "    [036-036] SHARP_PEAK (scale=MICRO)\n",
      "  [033-038] VOLATILITY_SPIKE (scale=MINI)\n",
      "  [039-039] SHARP_TROUGH (scale=MICRO)\n",
      "  [042-082] NORMAL_VOLATILITY (scale=MESO) (19 children)\n",
      "    [042-042] SHARP_PEAK (scale=MICRO)\n",
      "    [043-043] SHARP_TROUGH (scale=MICRO)\n",
      "    [045-045] SHARP_PEAK (scale=MICRO)\n",
      "    [046-052] DOWNTREND_SHORT (scale=MINI) (6 children)\n",
      "      [047-047] SHARP_TROUGH (scale=MICRO)\n",
      "      [048-048] SHARP_PEAK (scale=MICRO)\n",
      "      [049-049] SHARP_TROUGH (scale=MICRO)\n",
      "      [050-050] LOCAL_PEAK (scale=MICRO)\n",
      "      [051-051] LOCAL_TROUGH (scale=MICRO)\n",
      "      [052-052] SHARP_PEAK (scale=MICRO)\n",
      "    [054-054] SHARP_TROUGH (scale=MICRO)\n",
      "    [056-056] SHARP_PEAK (scale=MICRO)\n",
      "    [058-058] SHARP_TROUGH (scale=MICRO)\n",
      "    [059-059] SHARP_PEAK (scale=MICRO)\n",
      "    [060-060] SHARP_TROUGH (scale=MICRO)\n",
      "    [062-062] SHARP_PEAK (scale=MICRO)\n",
      "    [064-064] SHARP_TROUGH (scale=MICRO)\n",
      "    [066-066] SHARP_PEAK (scale=MICRO)\n",
      "    [068-068] SHARP_TROUGH (scale=MICRO)\n",
      "    [070-070] SHARP_PEAK (scale=MICRO)\n",
      "    [071-071] SHARP_TROUGH (scale=MICRO)\n",
      "    [074-074] SHARP_PEAK (scale=MICRO)\n",
      "    [076-076] SHARP_TROUGH (scale=MICRO)\n",
      "    [079-079] SHARP_PEAK (scale=MICRO)\n",
      "    [081-081] SHARP_TROUGH (scale=MICRO)\n",
      "  [083-083] SHARP_PEAK (scale=MICRO)\n",
      "  [087-087] SHARP_TROUGH (scale=MICRO)\n",
      "  [088-088] SHARP_PEAK (scale=MICRO)\n",
      "  [090-090] SHARP_TROUGH (scale=MICRO)\n",
      "  [091-091] SHARP_PEAK (scale=MICRO)\n",
      "  [092-102] VOLATILITY_SPIKE (scale=MINI) (6 children)\n",
      "    [094-094] SHARP_TROUGH (scale=MICRO)\n",
      "    [095-095] SHARP_PEAK (scale=MICRO)\n",
      "    [097-097] SHARP_TROUGH (scale=MICRO)\n",
      "    [098-098] SHARP_PEAK (scale=MICRO)\n",
      "    [099-099] SHARP_TROUGH (scale=MICRO)\n",
      "    [101-101] SHARP_PEAK (scale=MICRO)\n",
      "  [103-103] SHARP_TROUGH (scale=MICRO)\n",
      "  [104-104] SHARP_PEAK (scale=MICRO)\n",
      "  [105-105] SHARP_TROUGH (scale=MICRO)\n",
      "  [106-106] SHARP_PEAK (scale=MICRO)\n",
      "  [107-107] SHARP_TROUGH (scale=MICRO)\n",
      "  [111-111] SHARP_PEAK (scale=MICRO)\n",
      "  [112-124] NORMAL_VOLATILITY (scale=MINI) (8 children)\n",
      "    [113-113] SHARP_TROUGH (scale=MICRO)\n",
      "    [115-115] SHARP_PEAK (scale=MICRO)\n",
      "    [117-117] SHARP_TROUGH (scale=MICRO)\n",
      "    [119-119] SHARP_PEAK (scale=MICRO)\n",
      "    [120-120] SHARP_TROUGH (scale=MICRO)\n",
      "    [121-121] SHARP_PEAK (scale=MICRO)\n",
      "    [122-122] SHARP_TROUGH (scale=MICRO)\n",
      "    [124-124] SHARP_PEAK (scale=MICRO)\n",
      "  [125-133] LOW_VOLATILITY (scale=MINI) (4 children)\n",
      "    [125-125] SHARP_TROUGH (scale=MICRO)\n",
      "    [128-128] SHARP_PEAK (scale=MICRO)\n",
      "    [131-131] SHARP_TROUGH (scale=MICRO)\n",
      "    [132-132] SHARP_PEAK (scale=MICRO)\n",
      "  [134-140] NORMAL_VOLATILITY (scale=MINI) (6 children)\n",
      "    [134-134] SHARP_TROUGH (scale=MICRO)\n",
      "    [135-135] SHARP_PEAK (scale=MICRO)\n",
      "    [136-136] SHARP_TROUGH (scale=MICRO)\n",
      "    [137-137] SHARP_PEAK (scale=MICRO)\n",
      "    [139-139] SHARP_TROUGH (scale=MICRO)\n",
      "    [140-140] LOCAL_PEAK (scale=MICRO)\n",
      "  [141-141] LOCAL_TROUGH (scale=MICRO)\n",
      "  [142-155] NORMAL_VOLATILITY (scale=MINI) (6 children)\n",
      "    [142-142] SHARP_PEAK (scale=MICRO)\n",
      "    [150-150] SHARP_PEAK (scale=MICRO)\n",
      "    [151-151] SHARP_TROUGH (scale=MICRO)\n",
      "    [153-153] SHARP_PEAK (scale=MICRO)\n",
      "    [154-154] SHARP_TROUGH (scale=MICRO)\n",
      "    [155-155] SHARP_PEAK (scale=MICRO)\n",
      "  [143-148] DOWNTREND_SHORT (scale=MINI) (3 children)\n",
      "    [144-144] SHARP_TROUGH (scale=MICRO)\n",
      "    [147-147] SHARP_PEAK (scale=MICRO)\n",
      "    [148-148] SHARP_TROUGH (scale=MICRO)\n",
      "  [156-156] SHARP_TROUGH (scale=MICRO)\n",
      "  [157-171] NORMAL_VOLATILITY (scale=MINI) (9 children)\n",
      "    [157-157] SHARP_PEAK (scale=MICRO)\n",
      "    [159-159] SHARP_TROUGH (scale=MICRO)\n",
      "    [160-160] SHARP_PEAK (scale=MICRO)\n",
      "    [161-161] SHARP_TROUGH (scale=MICRO)\n",
      "    [162-162] SHARP_PEAK (scale=MICRO)\n",
      "    [164-164] SHARP_TROUGH (scale=MICRO)\n",
      "    [165-165] SHARP_PEAK (scale=MICRO)\n",
      "    [167-167] SHARP_TROUGH (scale=MICRO)\n",
      "    [170-170] SHARP_PEAK (scale=MICRO)\n",
      "  [173-173] SHARP_TROUGH (scale=MICRO)\n",
      "  [174-174] SHARP_PEAK (scale=MICRO)\n",
      "  [176-176] SHARP_TROUGH (scale=MICRO)\n",
      "  [177-189] NORMAL_VOLATILITY (scale=MINI) (10 children)\n",
      "    [177-177] SHARP_PEAK (scale=MICRO)\n",
      "    [178-178] SHARP_TROUGH (scale=MICRO)\n",
      "    [179-179] SHARP_PEAK (scale=MICRO)\n",
      "    [180-180] SHARP_TROUGH (scale=MICRO)\n",
      "    [181-181] SHARP_PEAK (scale=MICRO)\n",
      "    [183-183] SHARP_TROUGH (scale=MICRO)\n",
      "    [184-184] SHARP_PEAK (scale=MICRO)\n",
      "    [185-185] SHARP_TROUGH (scale=MICRO)\n",
      "    [186-186] SHARP_PEAK (scale=MICRO)\n",
      "    [188-188] SHARP_TROUGH (scale=MICRO)\n",
      "  [192-192] SHARP_PEAK (scale=MICRO)\n",
      "  [193-209] NORMAL_VOLATILITY (scale=MESO) (10 children)\n",
      "    [193-193] SHARP_TROUGH (scale=MICRO)\n",
      "    [196-196] SHARP_PEAK (scale=MICRO)\n",
      "    [198-198] SHARP_TROUGH (scale=MICRO)\n",
      "    [199-199] SHARP_PEAK (scale=MICRO)\n",
      "    [201-201] SHARP_TROUGH (scale=MICRO)\n",
      "    [203-203] SHARP_PEAK (scale=MICRO)\n",
      "    [206-206] SHARP_TROUGH (scale=MICRO)\n",
      "    [207-207] LOCAL_PEAK (scale=MICRO)\n",
      "    [208-208] LOCAL_TROUGH (scale=MICRO)\n",
      "    [209-209] SHARP_PEAK (scale=MICRO)\n",
      "  [210-210] SHARP_TROUGH (scale=MICRO)\n",
      "  [212-212] SHARP_PEAK (scale=MICRO)\n",
      "  [213-220] NORMAL_VOLATILITY (scale=MINI) (3 children)\n",
      "    [214-214] SHARP_TROUGH (scale=MICRO)\n",
      "    [216-216] SHARP_PEAK (scale=MICRO)\n",
      "    [217-217] SHARP_TROUGH (scale=MICRO)\n",
      "  [221-221] SHARP_PEAK (scale=MICRO)\n",
      "  [222-222] SHARP_TROUGH (scale=MICRO)\n",
      "  [224-224] SHARP_PEAK (scale=MICRO)\n",
      "  [226-235] LOW_VOLATILITY (scale=MINI) (6 children)\n",
      "    [226-226] SHARP_TROUGH (scale=MICRO)\n",
      "    [227-227] LOCAL_PEAK (scale=MICRO)\n",
      "    [228-228] LOCAL_TROUGH (scale=MICRO)\n",
      "    [230-230] SHARP_PEAK (scale=MICRO)\n",
      "    [233-233] SHARP_TROUGH (scale=MICRO)\n",
      "    [234-234] SHARP_PEAK (scale=MICRO)\n",
      "  [236-244] NORMAL_VOLATILITY (scale=MINI) (5 children)\n",
      "    [237-237] SHARP_TROUGH (scale=MICRO)\n",
      "    [238-238] SHARP_PEAK (scale=MICRO)\n",
      "    [240-240] SHARP_TROUGH (scale=MICRO)\n",
      "    [241-241] SHARP_PEAK (scale=MICRO)\n",
      "    [244-244] SHARP_TROUGH (scale=MICRO)\n",
      "  [245-245] SHARP_PEAK (scale=MICRO)\n",
      "  [248-253] DOWNTREND_SHORT (scale=MINI) (3 children)\n",
      "    [248-248] SHARP_TROUGH (scale=MICRO)\n",
      "    [251-251] SHARP_PEAK (scale=MICRO)\n",
      "    [253-253] SHARP_TROUGH (scale=MICRO)\n",
      "  [248-255] VOLATILITY_SPIKE (scale=MINI) (2 children)\n",
      "    [254-254] SHARP_PEAK (scale=MICRO)\n",
      "    [255-255] SHARP_TROUGH (scale=MICRO)\n",
      "  [256-262] HIGH_VOLATILITY (scale=MINI) (3 children)\n",
      "    [259-259] SHARP_PEAK (scale=MICRO)\n",
      "    [261-261] SHARP_TROUGH (scale=MICRO)\n",
      "    [262-262] SHARP_PEAK (scale=MICRO)\n",
      "  [263-263] SHARP_TROUGH (scale=MICRO)\n",
      "  [265-265] SHARP_PEAK (scale=MICRO)\n",
      "  [267-267] SHARP_TROUGH (scale=MICRO)\n",
      "  [268-298] LOW_VOLATILITY (scale=MESO) (20 children)\n",
      "    [269-269] SHARP_PEAK (scale=MICRO)\n",
      "    [271-271] SHARP_TROUGH (scale=MICRO)\n",
      "    [272-272] SHARP_PEAK (scale=MICRO)\n",
      "    [274-274] SHARP_TROUGH (scale=MICRO)\n",
      "    [277-277] SHARP_PEAK (scale=MICRO)\n",
      "    [280-280] SHARP_TROUGH (scale=MICRO)\n",
      "    [281-281] SHARP_PEAK (scale=MICRO)\n",
      "    [283-283] SHARP_TROUGH (scale=MICRO)\n",
      "    [284-284] SHARP_PEAK (scale=MICRO)\n",
      "    [285-285] SHARP_TROUGH (scale=MICRO)\n",
      "    [286-286] LOCAL_PEAK (scale=MICRO)\n",
      "    [287-287] LOCAL_TROUGH (scale=MICRO)\n",
      "    [288-288] SHARP_PEAK (scale=MICRO)\n",
      "    [289-289] SHARP_TROUGH (scale=MICRO)\n",
      "    [291-291] SHARP_PEAK (scale=MICRO)\n",
      "    [292-292] SHARP_TROUGH (scale=MICRO)\n",
      "    [294-294] SHARP_PEAK (scale=MICRO)\n",
      "    [295-295] SHARP_TROUGH (scale=MICRO)\n",
      "    [296-296] SHARP_PEAK (scale=MICRO)\n",
      "    [297-297] SHARP_TROUGH (scale=MICRO)\n",
      "  [299-305] NORMAL_VOLATILITY (scale=MINI) (4 children)\n",
      "    [299-299] SHARP_PEAK (scale=MICRO)\n",
      "    [303-303] SHARP_TROUGH (scale=MICRO)\n",
      "    [304-304] SHARP_PEAK (scale=MICRO)\n",
      "    [305-305] SHARP_TROUGH (scale=MICRO)\n",
      "  [306-328] LOW_VOLATILITY (scale=MESO) (11 children)\n",
      "    [306-306] LOCAL_PEAK (scale=MICRO)\n",
      "    [307-307] LOCAL_TROUGH (scale=MICRO)\n",
      "    [308-308] SHARP_PEAK (scale=MICRO)\n",
      "    [313-313] SHARP_TROUGH (scale=MICRO)\n",
      "    [314-314] SHARP_PEAK (scale=MICRO)\n",
      "    [317-317] SHARP_TROUGH (scale=MICRO)\n",
      "    [318-318] SHARP_PEAK (scale=MICRO)\n",
      "    [321-321] SHARP_TROUGH (scale=MICRO)\n",
      "    [324-324] SHARP_PEAK (scale=MICRO)\n",
      "    [327-327] SHARP_TROUGH (scale=MICRO)\n",
      "    [328-328] SHARP_PEAK (scale=MICRO)\n",
      "  [329-335] NORMAL_VOLATILITY (scale=MINI) (3 children)\n",
      "    [331-331] SHARP_TROUGH (scale=MICRO)\n",
      "    [333-333] SHARP_PEAK (scale=MICRO)\n",
      "    [334-334] SHARP_TROUGH (scale=MICRO)\n",
      "\n",
      "2. EVENTS BY SCALE:\n",
      "MICRO: 196 events\n",
      "MINI: 21 events\n",
      "MESO: 5 events\n",
      "MACRO: 0 events\n",
      "GLOBAL: 1 events\n",
      "\n",
      "3. EVENTS IN TIME RANGE [100-200]:\n",
      "  [000-335] SIDEWAYS_REGIME (GLOBAL)\n",
      "  [092-102] VOLATILITY_SPIKE (MINI)\n",
      "  [101-101] SHARP_PEAK (MICRO)\n",
      "  [103-103] SHARP_TROUGH (MICRO)\n",
      "  [104-104] SHARP_PEAK (MICRO)\n",
      "  [105-105] SHARP_TROUGH (MICRO)\n",
      "  [106-106] SHARP_PEAK (MICRO)\n",
      "  [107-107] SHARP_TROUGH (MICRO)\n",
      "  [111-111] SHARP_PEAK (MICRO)\n",
      "  [112-124] NORMAL_VOLATILITY (MINI)\n",
      "  [113-113] SHARP_TROUGH (MICRO)\n",
      "  [115-115] SHARP_PEAK (MICRO)\n",
      "  [117-117] SHARP_TROUGH (MICRO)\n",
      "  [119-119] SHARP_PEAK (MICRO)\n",
      "  [120-120] SHARP_TROUGH (MICRO)\n",
      "  [121-121] SHARP_PEAK (MICRO)\n",
      "  [122-122] SHARP_TROUGH (MICRO)\n",
      "  [124-124] SHARP_PEAK (MICRO)\n",
      "  [125-133] LOW_VOLATILITY (MINI)\n",
      "  [125-125] SHARP_TROUGH (MICRO)\n",
      "  [128-128] SHARP_PEAK (MICRO)\n",
      "  [131-131] SHARP_TROUGH (MICRO)\n",
      "  [132-132] SHARP_PEAK (MICRO)\n",
      "  [134-140] NORMAL_VOLATILITY (MINI)\n",
      "  [134-134] SHARP_TROUGH (MICRO)\n",
      "  [135-135] SHARP_PEAK (MICRO)\n",
      "  [136-136] SHARP_TROUGH (MICRO)\n",
      "  [137-137] SHARP_PEAK (MICRO)\n",
      "  [139-139] SHARP_TROUGH (MICRO)\n",
      "  [140-140] LOCAL_PEAK (MICRO)\n",
      "  [141-141] LOCAL_TROUGH (MICRO)\n",
      "  [142-155] NORMAL_VOLATILITY (MINI)\n",
      "  [142-142] SHARP_PEAK (MICRO)\n",
      "  [150-150] SHARP_PEAK (MICRO)\n",
      "  [151-151] SHARP_TROUGH (MICRO)\n",
      "  [153-153] SHARP_PEAK (MICRO)\n",
      "  [154-154] SHARP_TROUGH (MICRO)\n",
      "  [155-155] SHARP_PEAK (MICRO)\n",
      "  [143-148] DOWNTREND_SHORT (MINI)\n",
      "  [144-144] SHARP_TROUGH (MICRO)\n",
      "  [147-147] SHARP_PEAK (MICRO)\n",
      "  [148-148] SHARP_TROUGH (MICRO)\n",
      "  [156-156] SHARP_TROUGH (MICRO)\n",
      "  [157-171] NORMAL_VOLATILITY (MINI)\n",
      "  [157-157] SHARP_PEAK (MICRO)\n",
      "  [159-159] SHARP_TROUGH (MICRO)\n",
      "  [160-160] SHARP_PEAK (MICRO)\n",
      "  [161-161] SHARP_TROUGH (MICRO)\n",
      "  [162-162] SHARP_PEAK (MICRO)\n",
      "  [164-164] SHARP_TROUGH (MICRO)\n",
      "  [165-165] SHARP_PEAK (MICRO)\n",
      "  [167-167] SHARP_TROUGH (MICRO)\n",
      "  [170-170] SHARP_PEAK (MICRO)\n",
      "  [173-173] SHARP_TROUGH (MICRO)\n",
      "  [174-174] SHARP_PEAK (MICRO)\n",
      "  [176-176] SHARP_TROUGH (MICRO)\n",
      "  [177-189] NORMAL_VOLATILITY (MINI)\n",
      "  [177-177] SHARP_PEAK (MICRO)\n",
      "  [178-178] SHARP_TROUGH (MICRO)\n",
      "  [179-179] SHARP_PEAK (MICRO)\n",
      "  [180-180] SHARP_TROUGH (MICRO)\n",
      "  [181-181] SHARP_PEAK (MICRO)\n",
      "  [183-183] SHARP_TROUGH (MICRO)\n",
      "  [184-184] SHARP_PEAK (MICRO)\n",
      "  [185-185] SHARP_TROUGH (MICRO)\n",
      "  [186-186] SHARP_PEAK (MICRO)\n",
      "  [188-188] SHARP_TROUGH (MICRO)\n",
      "  [192-192] SHARP_PEAK (MICRO)\n",
      "  [193-209] NORMAL_VOLATILITY (MESO)\n",
      "  [193-193] SHARP_TROUGH (MICRO)\n",
      "  [196-196] SHARP_PEAK (MICRO)\n",
      "  [198-198] SHARP_TROUGH (MICRO)\n",
      "  [199-199] SHARP_PEAK (MICRO)\n",
      "\n",
      "4. EVENT METADATA:\n",
      "SIDEWAYS_REGIME: {}\n",
      "FLAT_SEGMENT: {'slope': 0.0}\n",
      "SHARP_PEAK: {'prominence': 2.7038753628730774}\n",
      "NORMAL_VOLATILITY: {'avg_volatility': 0.9497693777084351}\n",
      "SHARP_TROUGH: {'prominence': 3.990684390068054}\n",
      "\n",
      "5. STEP-WISE LABELS (first 20):\n",
      "IDs: [0, 9, 6, 10, 9, 7, 4, 5, 8, 6, 6, 4, 8, 5, 6, 7, 7, 5, 8, 7]\n",
      "Names: ['PAD', 'SPIKE_UP', 'DOWN_SMALL', 'SPIKE_DOWN', 'SPIKE_UP', 'DOWN_MEDIUM', 'UP_MEDIUM', 'UP_LARGE', 'DOWN_LARGE', 'DOWN_SMALL', 'DOWN_SMALL', 'UP_MEDIUM', 'DOWN_LARGE', 'UP_LARGE', 'DOWN_SMALL', 'DOWN_MEDIUM', 'DOWN_MEDIUM', 'UP_LARGE', 'DOWN_LARGE', 'DOWN_MEDIUM']\n",
      "\n",
      "[4/6] Computing statistics...\n",
      "\n",
      "================================================================================\n",
      "DETAILED EVENT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "By Event Type:\n",
      "  peak................  20800 (208.00 per sequence)\n",
      "  regime..............    100 (1.00 per sequence)\n",
      "  trend...............    376 (3.76 per sequence)\n",
      "  volatility..........   1980 (19.80 per sequence)\n",
      "\n",
      "By Scale:\n",
      "  GLOBAL..............    100 (1.00 per sequence)\n",
      "  MESO................    596 (5.96 per sequence)\n",
      "  MICRO...............  20800 (208.00 per sequence)\n",
      "  MINI................   1760 (17.60 per sequence)\n",
      "\n",
      "Top 10 Labels:\n",
      "  SHARP_PEAK....................   9247 (92.47 per sequence)\n",
      "  SHARP_TROUGH..................   9245 (92.45 per sequence)\n",
      "  LOCAL_TROUGH..................   1155 (11.55 per sequence)\n",
      "  LOCAL_PEAK....................   1153 (11.53 per sequence)\n",
      "  NORMAL_VOLATILITY.............   1083 (10.83 per sequence)\n",
      "  LOW_VOLATILITY................    470 (4.70 per sequence)\n",
      "  HIGH_VOLATILITY...............    229 (2.29 per sequence)\n",
      "  VOLATILITY_SPIKE..............    198 (1.98 per sequence)\n",
      "  UPTREND_SHORT.................    143 (1.43 per sequence)\n",
      "  DOWNTREND_SHORT...............    133 (1.33 per sequence)\n",
      "\n",
      "[5/6] Generating training corpus...\n",
      "\n",
      "Generating depth_marked format...\n",
      "Saved to depth_marked_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 23,256\n",
      "  Avg tokens/doc: 232.6\n",
      "\n",
      "Generating flat format...\n",
      "Saved to flat_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 23,256\n",
      "  Avg tokens/doc: 232.6\n",
      "\n",
      "Generating narrative format...\n",
      "Saved to narrative_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 300\n",
      "  Avg tokens/doc: 3.0\n",
      "\n",
      "[6/6] Creating DataLoader...\n",
      "  ✓ DataLoader ready with 7 batches\n",
      "\n",
      "================================================================================\n",
      "✓ WORKFLOW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "USAGE GUIDE: Hierarchical Time Series Event Labeling System\n",
    "============================================================\n",
    "\n",
    "Quick Start Guide and Common Use Cases\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     TextCorpusGenerator,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_example():\n",
    "    \"\"\"Minimal example to get started\"\"\"\n",
    "    \n",
    "    # 1. Prepare your data as [B, L] tensor\n",
    "    B, L = 100, 336  # 100 sequences, each 336 timesteps\n",
    "    x = torch.randn(B, L)  # Replace with your real data\n",
    "    \n",
    "    # 2. Create dataset (this does all the processing)\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # 3. Access annotations\n",
    "    annotation = dataset[0]  # Get first sequence annotation\n",
    "    \n",
    "    # 4. Generate training text\n",
    "    text = annotation.to_text(format='depth_marked')\n",
    "    print(text)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_numpy():\n",
    "    \"\"\"Load from numpy arrays\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load your numpy data\n",
    "    data = np.load('your_data.npy')  # Shape: [num_samples, sequence_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_from_csv():\n",
    "    \"\"\"Load from CSV files\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Assuming each row is a sequence\n",
    "    data = df.values  # Shape: [num_sequences, sequence_length]\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_eeg_example():\n",
    "    \"\"\"Example for EEG data\"\"\"\n",
    "    \n",
    "    # Assuming you have EEG data: [num_trials, num_channels, time_points]\n",
    "    eeg_data = torch.randn(100, 64, 1000)  # Replace with real data\n",
    "    \n",
    "    # Process each channel separately\n",
    "    datasets = []\n",
    "    for channel in range(64):\n",
    "        channel_data = eeg_data[:, channel, :]  # [num_trials, time_points]\n",
    "        dataset = HierarchicalEventDataset(channel_data, verbose=False)\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORING ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def explore_annotation(dataset):\n",
    "    \"\"\"Explore annotation structure\"\"\"\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION EXPLORATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. View hierarchical structure\n",
    "    print(\"\\n1. HIERARCHICAL TREE:\")\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # 2. Get events at specific scale\n",
    "    print(\"\\n2. EVENTS BY SCALE:\")\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"{scale.name}: {len(events)} events\")\n",
    "    \n",
    "    # 3. Get events in time range\n",
    "    print(\"\\n3. EVENTS IN TIME RANGE [100-200]:\")\n",
    "    events = ann.get_events_in_range(100, 200)\n",
    "    for e in events:\n",
    "        print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name} ({e.scale.name})\")\n",
    "    \n",
    "    # 4. Access event metadata\n",
    "    print(\"\\n4. EVENT METADATA:\")\n",
    "    for event in ann.all_events[:5]:\n",
    "        print(f\"{event.label_name}: {event.metadata}\")\n",
    "    \n",
    "    # 5. Step-wise labels\n",
    "    print(f\"\\n5. STEP-WISE LABELS (first 20):\")\n",
    "    print(f\"IDs: {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"Names: {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_training_corpus(dataset, output_file='training_corpus.txt'):\n",
    "    \"\"\"Generate complete training corpus\"\"\"\n",
    "    \n",
    "    # Generate text for all sequences\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nGenerating {fmt} format...\")\n",
    "        corpus = text_gen.generate_corpus(dataset, format=fmt)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f'{fmt}_{output_file}'\n",
    "        with open(filename, 'w') as f:\n",
    "            for i, text in enumerate(corpus):\n",
    "                f.write(f\"<sequence_{i}>\\n{text}\\n</sequence_{i}>\\n\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = text_gen.estimate_tokens(corpus)\n",
    "        print(f\"Saved to {filename}\")\n",
    "        print(f\"  Documents: {stats['num_documents']}\")\n",
    "        print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "\n",
    "\n",
    "def create_autoregressive_pairs(dataset):\n",
    "    \"\"\"Create input-output pairs for autoregressive LM training\"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for ann in dataset.annotations:\n",
    "        # Get hierarchical text\n",
    "        full_text = ann.to_text(format='depth_marked')\n",
    "        tokens = full_text.split()\n",
    "        \n",
    "        # Create prefix-completion pairs\n",
    "        for i in range(1, len(tokens)):\n",
    "            input_text = \" \".join(tokens[:i])\n",
    "            target_token = tokens[i]\n",
    "            pairs.append((input_text, target_token))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTERING AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def filter_by_event_type(dataset, event_type='trend'):\n",
    "    \"\"\"Filter sequences by event type\"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        # Check if this annotation contains the event type\n",
    "        has_event = any(e.event_type == event_type for e in ann.all_events)\n",
    "        if has_event:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    print(f\"Found {len(filtered)} sequences with {event_type} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def analyze_event_statistics(dataset):\n",
    "    \"\"\"Compute detailed event statistics\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'total_sequences': len(dataset),\n",
    "        'events_by_type': {},\n",
    "        'events_by_scale': {},\n",
    "        'events_by_label': {},\n",
    "    }\n",
    "    \n",
    "    # Count events\n",
    "    for ann in dataset.annotations:\n",
    "        for event in ann.all_events:\n",
    "            # By type\n",
    "            stats['events_by_type'][event.event_type] = \\\n",
    "                stats['events_by_type'].get(event.event_type, 0) + 1\n",
    "            \n",
    "            # By scale\n",
    "            stats['events_by_scale'][event.scale.name] = \\\n",
    "                stats['events_by_scale'].get(event.scale.name, 0) + 1\n",
    "            \n",
    "            # By label\n",
    "            stats['events_by_label'][event.label_name] = \\\n",
    "                stats['events_by_label'].get(event.label_name, 0) + 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EVENT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nBy Event Type:\")\n",
    "    for event_type, count in sorted(stats['events_by_type'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {event_type:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nBy Scale:\")\n",
    "    for scale, count in sorted(stats['events_by_scale'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {scale:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Labels:\")\n",
    "    top_labels = sorted(stats['events_by_label'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for label, count in top_labels:\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {label:.<30} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TEXT FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "def create_custom_format(ann):\n",
    "    \"\"\"Create your own text format\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    parts.append(f\"SEQ_LEN:{len(ann.sequence)}\")\n",
    "    \n",
    "    # Add global regime\n",
    "    global_events = ann.get_events_at_scale(EventScale.GLOBAL)\n",
    "    if global_events:\n",
    "        parts.append(f\"REGIME:{global_events[0].label_name}\")\n",
    "    \n",
    "    # Add macro trends\n",
    "    macro_events = ann.get_events_at_scale(EventScale.MACRO)\n",
    "    parts.append(f\"TRENDS:{len(macro_events)}\")\n",
    "    for event in macro_events:\n",
    "        parts.append(f\"T[{event.start}-{event.end}]:{event.label_name}\")\n",
    "    \n",
    "    # Add peaks\n",
    "    peaks = [e for e in ann.all_events if e.event_type == 'peak']\n",
    "    parts.append(f\"PEAKS:{len(peaks)}\")\n",
    "    for peak in peaks:\n",
    "        parts.append(f\"P[{peak.start}]:{peak.label_name}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_large_dataset_in_batches(data_generator, batch_size=1000):\n",
    "    \"\"\"Process very large datasets in batches\"\"\"\n",
    "    \n",
    "    all_annotations = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(data_generator):\n",
    "        print(f\"\\nProcessing batch {batch_idx}...\")\n",
    "        \n",
    "        # Create dataset for this batch\n",
    "        dataset = HierarchicalEventDataset(batch_data, verbose=False)\n",
    "        \n",
    "        # Collect annotations\n",
    "        all_annotations.extend(dataset.annotations)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        torch.save(dataset.annotations, f'annotations_batch_{batch_idx}.pt')\n",
    "    \n",
    "    print(f\"\\nTotal annotations: {len(all_annotations)}\")\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH TRAINING PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "def create_pytorch_dataloader(dataset, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoader for training\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function for hierarchical annotations\"\"\"\n",
    "        sequences = torch.stack([ann.sequence for ann in batch])\n",
    "        step_labels = torch.stack([ann.step_labels for ann in batch])\n",
    "        \n",
    "        # Generate text representations\n",
    "        texts = [ann.to_text(format='depth_marked') for ann in batch]\n",
    "        \n",
    "        return {\n",
    "            'sequences': sequences,\n",
    "            'step_labels': step_labels,\n",
    "            'texts': texts,\n",
    "            'annotations': batch  # Keep full annotations if needed\n",
    "        }\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def prepare_for_huggingface(dataset, tokenizer):\n",
    "    \"\"\"Prepare data for HuggingFace transformers\"\"\"\n",
    "    \n",
    "    # Generate text corpus\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    texts = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE WORKFLOWS\n",
    "# ============================================================================\n",
    "\n",
    "def complete_workflow_example():\n",
    "    \"\"\"Complete end-to-end workflow\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Generate/Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    B, L = 100, 336\n",
    "    x = torch.randn(B, L)  # Replace with real data\n",
    "    \n",
    "    # 2. Create dataset\n",
    "    print(\"\\n[2/6] Creating hierarchical dataset...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    print(f\"  ✓ Processed {len(dataset)} sequences\")\n",
    "    \n",
    "    # 3. Explore one example\n",
    "    print(\"\\n[3/6] Exploring example annotation...\")\n",
    "    explore_annotation(dataset)\n",
    "    \n",
    "    # 4. Analyze statistics\n",
    "    print(\"\\n[4/6] Computing statistics...\")\n",
    "    stats = analyze_event_statistics(dataset)\n",
    "    \n",
    "    # 5. Generate training corpus\n",
    "    print(\"\\n[5/6] Generating training corpus...\")\n",
    "    generate_training_corpus(dataset, 'output_corpus.txt')\n",
    "    \n",
    "    # 6. Create DataLoader\n",
    "    print(\"\\n[6/6] Creating DataLoader...\")\n",
    "    dataloader = create_pytorch_dataloader(dataset, batch_size=16)\n",
    "    print(f\"  ✓ DataLoader ready with {len(dataloader)} batches\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete workflow\n",
    "    complete_workflow_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 100\n",
      "Length: 512\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 51200 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/100...\n",
      "      Processing sequence 50/100...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 35500\n",
      "      Avg per sequence: 355.0\n",
      "      By scale:\n",
      "        MICRO.......  319.4 per sequence\n",
      "        MINI........   25.7 per sequence\n",
      "        MESO........    8.8 per sequence\n",
      "        MACRO.......    0.1 per sequence\n",
      "        GLOBAL......    1.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Hierarchical Events (Total: 342)\n",
      "================================================================================\n",
      "[000-511] SIDEWAYS_REGIME (scale=GLOBAL) (129 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO) (2 children)\n",
      "    [000-014] LOW_VOLATILITY (scale=MINI) (7 children)\n",
      "      [001-001] SHARP_TROUGH (scale=MICRO)\n",
      "      [003-003] SHARP_PEAK (scale=MICRO)\n",
      "      [005-005] SHARP_TROUGH (scale=MICRO)\n",
      "      [007-007] SHARP_PEAK (scale=MICRO)\n",
      "      [009-009] SHARP_TROUGH (scale=MICRO)\n",
      "      [011-011] SHARP_PEAK (scale=MICRO)\n",
      "      [014-014] SHARP_TROUGH (scale=MICRO)\n",
      "    [015-015] SHARP_PEAK (scale=MICRO)\n",
      "  [015-035] NORMAL_VOLATILITY (scale=MESO) (9 children)\n",
      "    [020-020] SHARP_TROUGH (scale=MICRO)\n",
      "    [021-021] SHARP_PEAK (scale=MICRO)\n",
      "    [022-022] SHARP_TROUGH (scale=MICRO)\n",
      "    [025-025] SHARP_PEAK (scale=MICRO)\n",
      "    [026-026] SHARP_TROUGH (scale=MICRO)\n",
      "    [027-027] LOCAL_PEAK (scale=MICRO)\n",
      "    [028-028] LOCAL_TROUGH (scale=MICRO)\n",
      "    [029-029] SHARP_PEAK (scale=MICRO)\n",
      "    [032-032] SHARP_TROUGH (scale=MICRO)\n",
      "  [035-040] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [035-035] SHARP_PEAK (scale=MICRO)\n",
      "    [038-038] SHARP_TROUGH (scale=MICRO)\n",
      "    [039-039] SHARP_PEAK (scale=MICRO)\n",
      "  [040-048] NORMAL_VOLATILITY (scale=MINI) (4 children)\n",
      "    [041-041] SHARP_TROUGH (scale=MICRO)\n",
      "    [045-045] SHARP_PEAK (scale=MICRO)\n",
      "    [046-046] SHARP_TROUGH (scale=MICRO)\n",
      "    [047-047] SHARP_PEAK (scale=MICRO)\n",
      "  [049-049] SHARP_TROUGH (scale=MICRO)\n",
      "  [051-051] SHARP_PEAK (scale=MICRO)\n",
      "  [053-053] SHARP_TROUGH (scale=MICRO)\n",
      "  [054-054] SHARP_PEAK (scale=MICRO)\n",
      "  [055-055] SHARP_TROUGH (scale=MICRO)\n",
      "  [056-064] HIGH_VOLATILITY (scale=MINI) (6 children)\n",
      "    [059-059] SHARP_PEAK (scale=MICRO)\n",
      "    [060-060] LOCAL_TROUGH (scale=MICRO)\n",
      "    [061-061] LOCAL_PEAK (scale=MICRO)\n",
      "    [062-062] SHARP_TROUGH (scale=MICRO)\n",
      "    [063-063] SHARP_PEAK (scale=MICRO)\n",
      "    [064-064] SHARP_TROUGH (scale=MICRO)\n",
      "  [066-066] SHARP_PEAK (scale=MICRO)\n",
      "  [067-067] SHARP_TROUGH (scale=MICRO)\n",
      "  [068-068] SHARP_PEAK (scale=MICRO)\n",
      "  [072-072] SHARP_TROUGH (scale=MICRO)\n",
      "  [073-073] SHARP_PEAK (scale=MICRO)\n",
      "  [074-074] SHARP_TROUGH (scale=MICRO)\n",
      "  [076-076] SHARP_PEAK (scale=MICRO)\n",
      "  [078-078] SHARP_TROUGH (scale=MICRO)\n",
      "  [080-087] NORMAL_VOLATILITY (scale=MINI) (5 children)\n",
      "    [081-081] SHARP_PEAK (scale=MICRO)\n",
      "    [082-082] SHARP_TROUGH (scale=MICRO)\n",
      "    [084-084] SHARP_PEAK (scale=MICRO)\n",
      "    [086-086] SHARP_TROUGH (scale=MICRO)\n",
      "    [087-087] SHARP_PEAK (scale=MICRO)\n",
      "  [088-088] SHARP_TROUGH (scale=MICRO)\n",
      "  [091-091] SHARP_PEAK (scale=MICRO)\n",
      "  [093-099] HIGH_VOLATILITY (scale=MINI) (2 children)\n",
      "    [093-093] SHARP_TROUGH (scale=MICRO)\n",
      "    [094-094] SHARP_PEAK (scale=MICRO)\n",
      "  [100-108] VOLATILITY_SPIKE (scale=MINI) (6 children)\n",
      "    [100-100] SHARP_TROUGH (scale=MICRO)\n",
      "    [101-101] LOCAL_PEAK (scale=MICRO)\n",
      "    [102-102] LOCAL_TROUGH (scale=MICRO)\n",
      "    [105-105] SHARP_PEAK (scale=MICRO)\n",
      "    [106-106] SHARP_TROUGH (scale=MICRO)\n",
      "    [108-108] SHARP_PEAK (scale=MICRO)\n",
      "  [110-110] SHARP_TROUGH (scale=MICRO)\n",
      "  [111-111] SHARP_PEAK (scale=MICRO)\n",
      "  [112-112] SHARP_TROUGH (scale=MICRO)\n",
      "  [113-119] VOLATILITY_SPIKE (scale=MINI) (2 children)\n",
      "    [113-113] SHARP_PEAK (scale=MICRO)\n",
      "    [115-115] SHARP_TROUGH (scale=MICRO)\n",
      "  [116-121] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [116-116] SHARP_PEAK (scale=MICRO)\n",
      "    [119-119] SHARP_TROUGH (scale=MICRO)\n",
      "    [120-120] SHARP_PEAK (scale=MICRO)\n",
      "  [124-124] SHARP_TROUGH (scale=MICRO)\n",
      "  [125-125] SHARP_PEAK (scale=MICRO)\n",
      "  [126-126] SHARP_TROUGH (scale=MICRO)\n",
      "  [127-127] SHARP_PEAK (scale=MICRO)\n",
      "  [129-129] SHARP_TROUGH (scale=MICRO)\n",
      "  [130-130] SHARP_PEAK (scale=MICRO)\n",
      "  [131-131] LOCAL_TROUGH (scale=MICRO)\n",
      "  [132-141] LOW_VOLATILITY (scale=MINI) (6 children)\n",
      "    [132-132] LOCAL_PEAK (scale=MICRO)\n",
      "    [136-136] SHARP_TROUGH (scale=MICRO)\n",
      "    [137-137] SHARP_PEAK (scale=MICRO)\n",
      "    [138-138] SHARP_TROUGH (scale=MICRO)\n",
      "    [139-139] SHARP_PEAK (scale=MICRO)\n",
      "    [140-140] SHARP_TROUGH (scale=MICRO)\n",
      "  [142-154] NORMAL_VOLATILITY (scale=MINI) (10 children)\n",
      "    [142-142] SHARP_PEAK (scale=MICRO)\n",
      "    [143-143] LOCAL_TROUGH (scale=MICRO)\n",
      "    [144-144] LOCAL_PEAK (scale=MICRO)\n",
      "    [145-145] SHARP_TROUGH (scale=MICRO)\n",
      "    [146-146] SHARP_PEAK (scale=MICRO)\n",
      "    [149-149] SHARP_TROUGH (scale=MICRO)\n",
      "    [150-150] SHARP_PEAK (scale=MICRO)\n",
      "    [152-152] SHARP_TROUGH (scale=MICRO)\n",
      "    [153-153] SHARP_PEAK (scale=MICRO)\n",
      "    [154-154] SHARP_TROUGH (scale=MICRO)\n",
      "  [155-155] SHARP_PEAK (scale=MICRO)\n",
      "  [156-156] SHARP_TROUGH (scale=MICRO)\n",
      "  [157-157] SHARP_PEAK (scale=MICRO)\n",
      "  [158-158] SHARP_TROUGH (scale=MICRO)\n",
      "  [159-159] SHARP_PEAK (scale=MICRO)\n",
      "  [161-161] SHARP_TROUGH (scale=MICRO)\n",
      "  [163-163] LOCAL_PEAK (scale=MICRO)\n",
      "  [164-164] LOCAL_TROUGH (scale=MICRO)\n",
      "  [165-165] SHARP_PEAK (scale=MICRO)\n",
      "  [166-176] NORMAL_VOLATILITY (scale=MINI) (5 children)\n",
      "    [166-166] SHARP_TROUGH (scale=MICRO)\n",
      "    [169-169] SHARP_PEAK (scale=MICRO)\n",
      "    [170-170] LOCAL_TROUGH (scale=MICRO)\n",
      "    [171-171] LOCAL_PEAK (scale=MICRO)\n",
      "    [175-175] SHARP_TROUGH (scale=MICRO)\n",
      "  [177-182] LOW_VOLATILITY (scale=MINI) (5 children)\n",
      "    [177-177] SHARP_PEAK (scale=MICRO)\n",
      "    [178-178] SHARP_TROUGH (scale=MICRO)\n",
      "    [179-179] SHARP_PEAK (scale=MICRO)\n",
      "    [180-180] SHARP_TROUGH (scale=MICRO)\n",
      "    [181-181] SHARP_PEAK (scale=MICRO)\n",
      "  [183-198] NORMAL_VOLATILITY (scale=MESO) (9 children)\n",
      "    [183-183] SHARP_TROUGH (scale=MICRO)\n",
      "    [185-185] SHARP_PEAK (scale=MICRO)\n",
      "    [186-186] SHARP_TROUGH (scale=MICRO)\n",
      "    [187-187] SHARP_PEAK (scale=MICRO)\n",
      "    [188-188] SHARP_TROUGH (scale=MICRO)\n",
      "    [190-190] SHARP_PEAK (scale=MICRO)\n",
      "    [192-192] SHARP_TROUGH (scale=MICRO)\n",
      "    [196-196] SHARP_PEAK (scale=MICRO)\n",
      "    [197-197] SHARP_TROUGH (scale=MICRO)\n",
      "  [199-199] SHARP_PEAK (scale=MICRO)\n",
      "  [200-200] SHARP_TROUGH (scale=MICRO)\n",
      "  [201-201] SHARP_PEAK (scale=MICRO)\n",
      "  [202-202] SHARP_TROUGH (scale=MICRO)\n",
      "  [203-211] NORMAL_VOLATILITY (scale=MINI) (4 children)\n",
      "    [206-206] SHARP_PEAK (scale=MICRO)\n",
      "    [208-208] SHARP_TROUGH (scale=MICRO)\n",
      "    [209-209] SHARP_PEAK (scale=MICRO)\n",
      "    [211-211] SHARP_TROUGH (scale=MICRO)\n",
      "  [212-212] SHARP_PEAK (scale=MICRO)\n",
      "  [214-214] SHARP_TROUGH (scale=MICRO)\n",
      "  [216-225] NORMAL_VOLATILITY (scale=MINI) (8 children)\n",
      "    [216-216] SHARP_PEAK (scale=MICRO)\n",
      "    [218-218] SHARP_TROUGH (scale=MICRO)\n",
      "    [220-220] SHARP_PEAK (scale=MICRO)\n",
      "    [221-221] SHARP_TROUGH (scale=MICRO)\n",
      "    [222-222] SHARP_PEAK (scale=MICRO)\n",
      "    [223-223] SHARP_TROUGH (scale=MICRO)\n",
      "    [224-224] SHARP_PEAK (scale=MICRO)\n",
      "    [225-225] SHARP_TROUGH (scale=MICRO)\n",
      "  [226-226] SHARP_PEAK (scale=MICRO)\n",
      "  [227-227] SHARP_TROUGH (scale=MICRO)\n",
      "  [228-233] NORMAL_VOLATILITY (scale=MINI) (3 children)\n",
      "    [228-228] SHARP_PEAK (scale=MICRO)\n",
      "    [231-231] SHARP_TROUGH (scale=MICRO)\n",
      "    [233-233] SHARP_PEAK (scale=MICRO)\n",
      "  [234-234] SHARP_TROUGH (scale=MICRO)\n",
      "  [236-236] SHARP_PEAK (scale=MICRO)\n",
      "  [237-237] SHARP_TROUGH (scale=MICRO)\n",
      "  [238-238] SHARP_PEAK (scale=MICRO)\n",
      "  [239-239] SHARP_TROUGH (scale=MICRO)\n",
      "  [242-242] SHARP_PEAK (scale=MICRO)\n",
      "  [243-243] SHARP_TROUGH (scale=MICRO)\n",
      "  [245-252] NORMAL_VOLATILITY (scale=MINI) (3 children)\n",
      "    [248-248] SHARP_PEAK (scale=MICRO)\n",
      "    [251-251] SHARP_TROUGH (scale=MICRO)\n",
      "    [252-252] SHARP_PEAK (scale=MICRO)\n",
      "  [253-253] SHARP_TROUGH (scale=MICRO)\n",
      "  [256-256] SHARP_PEAK (scale=MICRO)\n",
      "  [257-257] SHARP_TROUGH (scale=MICRO)\n",
      "  [258-258] SHARP_PEAK (scale=MICRO)\n",
      "  [259-259] SHARP_TROUGH (scale=MICRO)\n",
      "  [260-260] SHARP_PEAK (scale=MICRO)\n",
      "  [261-261] SHARP_TROUGH (scale=MICRO)\n",
      "  [264-264] SHARP_PEAK (scale=MICRO)\n",
      "  [265-265] LOCAL_TROUGH (scale=MICRO)\n",
      "  [266-266] LOCAL_PEAK (scale=MICRO)\n",
      "  [267-267] LOCAL_TROUGH (scale=MICRO)\n",
      "  [268-275] NORMAL_VOLATILITY (scale=MINI) (6 children)\n",
      "    [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "    [270-270] SHARP_TROUGH (scale=MICRO)\n",
      "    [271-271] LOCAL_PEAK (scale=MICRO)\n",
      "    [272-272] LOCAL_TROUGH (scale=MICRO)\n",
      "    [273-273] SHARP_PEAK (scale=MICRO)\n",
      "    [274-274] SHARP_TROUGH (scale=MICRO)\n",
      "  [276-276] SHARP_PEAK (scale=MICRO)\n",
      "  [277-277] SHARP_TROUGH (scale=MICRO)\n",
      "  [278-283] UPTREND_SHORT (scale=MINI) (3 children)\n",
      "    [278-278] SHARP_PEAK (scale=MICRO)\n",
      "    [281-281] SHARP_TROUGH (scale=MICRO)\n",
      "    [283-283] SHARP_PEAK (scale=MICRO)\n",
      "  [281-294] NORMAL_VOLATILITY (scale=MINI) (7 children)\n",
      "    [285-285] SHARP_TROUGH (scale=MICRO)\n",
      "    [286-286] SHARP_PEAK (scale=MICRO)\n",
      "    [288-288] SHARP_TROUGH (scale=MICRO)\n",
      "    [289-289] SHARP_PEAK (scale=MICRO)\n",
      "    [290-290] SHARP_TROUGH (scale=MICRO)\n",
      "    [293-293] SHARP_PEAK (scale=MICRO)\n",
      "    [294-294] SHARP_TROUGH (scale=MICRO)\n",
      "  [295-295] SHARP_PEAK (scale=MICRO)\n",
      "  [296-296] SHARP_TROUGH (scale=MICRO)\n",
      "  [297-304] HIGH_VOLATILITY (scale=MINI) (6 children)\n",
      "    [297-297] SHARP_PEAK (scale=MICRO)\n",
      "    [298-298] SHARP_TROUGH (scale=MICRO)\n",
      "    [300-300] SHARP_PEAK (scale=MICRO)\n",
      "    [302-302] SHARP_TROUGH (scale=MICRO)\n",
      "    [303-303] SHARP_PEAK (scale=MICRO)\n",
      "    [304-304] SHARP_TROUGH (scale=MICRO)\n",
      "  [307-307] SHARP_PEAK (scale=MICRO)\n",
      "  [308-308] SHARP_TROUGH (scale=MICRO)\n",
      "  [310-310] SHARP_PEAK (scale=MICRO)\n",
      "  [311-311] SHARP_TROUGH (scale=MICRO)\n",
      "  [313-313] SHARP_PEAK (scale=MICRO)\n",
      "  [314-314] SHARP_TROUGH (scale=MICRO)\n",
      "  [315-315] SHARP_PEAK (scale=MICRO)\n",
      "  [316-316] SHARP_TROUGH (scale=MICRO)\n",
      "  [318-338] LOW_VOLATILITY (scale=MESO) (9 children)\n",
      "    [318-318] SHARP_PEAK (scale=MICRO)\n",
      "    [320-320] SHARP_TROUGH (scale=MICRO)\n",
      "    [322-322] SHARP_PEAK (scale=MICRO)\n",
      "    [323-323] SHARP_TROUGH (scale=MICRO)\n",
      "    [324-324] SHARP_PEAK (scale=MICRO)\n",
      "    [326-326] SHARP_TROUGH (scale=MICRO)\n",
      "    [329-329] SHARP_PEAK (scale=MICRO)\n",
      "    [330-330] SHARP_TROUGH (scale=MICRO)\n",
      "    [337-337] SHARP_PEAK (scale=MICRO)\n",
      "  [339-356] NORMAL_VOLATILITY (scale=MESO) (9 children)\n",
      "    [339-339] SHARP_TROUGH (scale=MICRO)\n",
      "    [341-341] SHARP_PEAK (scale=MICRO)\n",
      "    [342-342] SHARP_TROUGH (scale=MICRO)\n",
      "    [344-344] SHARP_PEAK (scale=MICRO)\n",
      "    [346-346] SHARP_TROUGH (scale=MICRO)\n",
      "    [347-347] SHARP_PEAK (scale=MICRO)\n",
      "    [348-348] SHARP_TROUGH (scale=MICRO)\n",
      "    [351-351] SHARP_PEAK (scale=MICRO)\n",
      "    [355-355] SHARP_TROUGH (scale=MICRO)\n",
      "  [357-357] SHARP_PEAK (scale=MICRO)\n",
      "  [359-359] SHARP_TROUGH (scale=MICRO)\n",
      "  [361-366] VOLATILITY_SPIKE (scale=MINI) (5 children)\n",
      "    [361-361] SHARP_PEAK (scale=MICRO)\n",
      "    [362-362] SHARP_TROUGH (scale=MICRO)\n",
      "    [363-363] SHARP_PEAK (scale=MICRO)\n",
      "    [364-364] SHARP_TROUGH (scale=MICRO)\n",
      "    [365-365] SHARP_PEAK (scale=MICRO)\n",
      "  [367-367] SHARP_TROUGH (scale=MICRO)\n",
      "  [369-369] SHARP_PEAK (scale=MICRO)\n",
      "  [372-372] SHARP_TROUGH (scale=MICRO)\n",
      "  [374-374] SHARP_PEAK (scale=MICRO)\n",
      "  [376-376] SHARP_TROUGH (scale=MICRO)\n",
      "  [378-378] SHARP_PEAK (scale=MICRO)\n",
      "  [380-380] SHARP_TROUGH (scale=MICRO)\n",
      "  [381-381] SHARP_PEAK (scale=MICRO)\n",
      "  [382-390] DOWNTREND_SHORT (scale=MINI) (5 children)\n",
      "    [382-382] SHARP_TROUGH (scale=MICRO)\n",
      "    [384-384] SHARP_PEAK (scale=MICRO)\n",
      "    [387-387] SHARP_TROUGH (scale=MICRO)\n",
      "    [388-388] SHARP_PEAK (scale=MICRO)\n",
      "    [389-389] SHARP_TROUGH (scale=MICRO)\n",
      "  [391-391] LOCAL_PEAK (scale=MICRO)\n",
      "  [392-392] LOCAL_TROUGH (scale=MICRO)\n",
      "  [393-393] SHARP_PEAK (scale=MICRO)\n",
      "  [395-395] SHARP_TROUGH (scale=MICRO)\n",
      "  [396-396] SHARP_PEAK (scale=MICRO)\n",
      "  [397-397] SHARP_TROUGH (scale=MICRO)\n",
      "  [398-398] SHARP_PEAK (scale=MICRO)\n",
      "  [399-399] SHARP_TROUGH (scale=MICRO)\n",
      "  [401-401] SHARP_PEAK (scale=MICRO)\n",
      "  [404-404] SHARP_TROUGH (scale=MICRO)\n",
      "  [405-405] SHARP_PEAK (scale=MICRO)\n",
      "  [406-406] SHARP_TROUGH (scale=MICRO)\n",
      "  [407-420] NORMAL_VOLATILITY (scale=MINI) (7 children)\n",
      "    [407-407] SHARP_PEAK (scale=MICRO)\n",
      "    [408-408] SHARP_TROUGH (scale=MICRO)\n",
      "    [409-409] SHARP_PEAK (scale=MICRO)\n",
      "    [412-412] SHARP_TROUGH (scale=MICRO)\n",
      "    [416-416] SHARP_PEAK (scale=MICRO)\n",
      "    [417-417] SHARP_TROUGH (scale=MICRO)\n",
      "    [418-418] SHARP_PEAK (scale=MICRO)\n",
      "  [422-422] SHARP_TROUGH (scale=MICRO)\n",
      "  [424-452] LOW_VOLATILITY (scale=MESO) (17 children)\n",
      "    [424-424] SHARP_PEAK (scale=MICRO)\n",
      "    [425-425] SHARP_TROUGH (scale=MICRO)\n",
      "    [426-426] SHARP_PEAK (scale=MICRO)\n",
      "    [427-427] SHARP_TROUGH (scale=MICRO)\n",
      "    [429-429] SHARP_PEAK (scale=MICRO)\n",
      "    [431-431] LOCAL_TROUGH (scale=MICRO)\n",
      "    [432-432] LOCAL_PEAK (scale=MICRO)\n",
      "    [434-434] SHARP_TROUGH (scale=MICRO)\n",
      "    [438-438] SHARP_PEAK (scale=MICRO)\n",
      "    [439-439] SHARP_TROUGH (scale=MICRO)\n",
      "    [441-441] SHARP_PEAK (scale=MICRO)\n",
      "    [443-443] SHARP_TROUGH (scale=MICRO)\n",
      "    [444-444] LOCAL_PEAK (scale=MICRO)\n",
      "    [445-445] LOCAL_TROUGH (scale=MICRO)\n",
      "    [448-448] SHARP_PEAK (scale=MICRO)\n",
      "    [449-449] SHARP_TROUGH (scale=MICRO)\n",
      "    [452-452] SHARP_PEAK (scale=MICRO)\n",
      "  [453-453] SHARP_TROUGH (scale=MICRO)\n",
      "  [455-468] NORMAL_VOLATILITY (scale=MINI) (10 children)\n",
      "    [455-455] SHARP_PEAK (scale=MICRO)\n",
      "    [456-456] SHARP_TROUGH (scale=MICRO)\n",
      "    [457-457] SHARP_PEAK (scale=MICRO)\n",
      "    [458-458] SHARP_TROUGH (scale=MICRO)\n",
      "    [460-460] SHARP_PEAK (scale=MICRO)\n",
      "    [461-461] SHARP_TROUGH (scale=MICRO)\n",
      "    [462-462] LOCAL_PEAK (scale=MICRO)\n",
      "    [464-464] LOCAL_TROUGH (scale=MICRO)\n",
      "    [465-465] SHARP_PEAK (scale=MICRO)\n",
      "    [466-466] SHARP_TROUGH (scale=MICRO)\n",
      "  [469-476] LOW_VOLATILITY (scale=MINI) (4 children)\n",
      "    [470-470] SHARP_PEAK (scale=MICRO)\n",
      "    [471-471] SHARP_TROUGH (scale=MICRO)\n",
      "    [474-474] SHARP_PEAK (scale=MICRO)\n",
      "    [476-476] SHARP_TROUGH (scale=MICRO)\n",
      "  [477-495] NORMAL_VOLATILITY (scale=MESO) (13 children)\n",
      "    [477-477] SHARP_PEAK (scale=MICRO)\n",
      "    [479-479] SHARP_TROUGH (scale=MICRO)\n",
      "    [480-480] SHARP_PEAK (scale=MICRO)\n",
      "    [482-482] SHARP_TROUGH (scale=MICRO)\n",
      "    [483-483] SHARP_PEAK (scale=MICRO)\n",
      "    [486-486] SHARP_TROUGH (scale=MICRO)\n",
      "    [487-487] SHARP_PEAK (scale=MICRO)\n",
      "    [488-488] SHARP_TROUGH (scale=MICRO)\n",
      "    [491-491] SHARP_PEAK (scale=MICRO)\n",
      "    [492-492] SHARP_TROUGH (scale=MICRO)\n",
      "    [493-493] LOCAL_PEAK (scale=MICRO)\n",
      "    [494-494] LOCAL_TROUGH (scale=MICRO)\n",
      "    [495-495] LOCAL_PEAK (scale=MICRO)\n",
      "  [496-511] LOW_VOLATILITY (scale=MESO) (9 children)\n",
      "    [496-496] LOCAL_TROUGH (scale=MICRO)\n",
      "    [498-498] SHARP_PEAK (scale=MICRO)\n",
      "    [499-499] SHARP_TROUGH (scale=MICRO)\n",
      "    [501-501] SHARP_PEAK (scale=MICRO)\n",
      "    [504-504] SHARP_TROUGH (scale=MICRO)\n",
      "    [505-505] SHARP_PEAK (scale=MICRO)\n",
      "    [506-506] LOCAL_TROUGH (scale=MICRO)\n",
      "    [507-507] LOCAL_PEAK (scale=MICRO)\n",
      "    [510-510] SHARP_TROUGH (scale=MICRO)\n",
      "[0-511]SIDEWAYS_REGIME >[0-19]FLAT_SEGMENT >>[0-14]LOW_VOLATILITY >>>[1-1]SHARP_TROUGH >>>[3-3]SHARP_PEAK >>>[5-5]SHARP_TROUGH >>>[7-7]SHARP_PEAK >>>[9-9]SHARP_TROUGH >>>[11-11]SHARP_PEAK >>>[14-14]SHARP_TROUGH >>[15-15]SHARP_PEAK >[15-35]NORMAL_VOLATILITY >>[20-20]SHARP_TROUGH >>[21-21]SHARP_PEAK >>[22-22]SHARP_TROUGH >>[25-25]SHARP_PEAK >>[26-26]SHARP_TROUGH >>[27-27]LOCAL_PEAK >>[28-28]LOCAL_TROUGH >>[29-29]SHARP_PEAK >>[32-32]SHARP_TROUGH >[35-40]UPTREND_SHORT >>[35-35]SHARP_PEAK >>[38-38]SHARP_TROUGH >>[39-39]SHARP_PEAK >[40-48]NORMAL_VOLATILITY >>[41-41]SHARP_TROUGH >>[45-45]SHARP_PEAK >>[46-46]SHARP_TROUGH >>[47-47]SHARP_PEAK >[49-49]SHARP_TROUGH >[51-51]SHARP_PEAK >[53-53]SHARP_TROUGH >[54-54]SHARP_PEAK >[55-55]SHARP_TROUGH >[56-64]HIGH_VOLATILITY >>[59-59]SHARP_PEAK >>[60-60]LOCAL_TROUGH >>[61-61]LOCAL_PEAK >>[62-62]SHARP_TROUGH >>[63-63]SHARP_PEAK >>[64-64]SHARP_TROUGH >[66-66]SHARP_PEAK >[67-67]SHARP_TROUGH >[68-68]SHARP_PEAK >[72-72]SHARP_TROUGH >[73-73]SHARP_PEAK >[74-74]SHARP_TROUGH >[76-76]SHARP_PEAK >[78-78]SHARP_TROUGH >[80-87]NORMAL_VOLATILITY >>[81-81]SHARP_PEAK >>[82-82]SHARP_TROUGH >>[84-84]SHARP_PEAK >>[86-86]SHARP_TROUGH >>[87-87]SHARP_PEAK >[88-88]SHARP_TROUGH >[91-91]SHARP_PEAK >[93-99]HIGH_VOLATILITY >>[93-93]SHARP_TROUGH >>[94-94]SHARP_PEAK >[100-108]VOLATILITY_SPIKE >>[100-100]SHARP_TROUGH >>[101-101]LOCAL_PEAK >>[102-102]LOCAL_TROUGH >>[105-105]SHARP_PEAK >>[106-106]SHARP_TROUGH >>[108-108]SHARP_PEAK >[110-110]SHARP_TROUGH >[111-111]SHARP_PEAK >[112-112]SHARP_TROUGH >[113-119]VOLATILITY_SPIKE >>[113-113]SHARP_PEAK >>[115-115]SHARP_TROUGH >[116-121]UPTREND_SHORT >>[116-116]SHARP_PEAK >>[119-119]SHARP_TROUGH >>[120-120]SHARP_PEAK >[124-124]SHARP_TROUGH >[125-125]SHARP_PEAK >[126-126]SHARP_TROUGH >[127-127]SHARP_PEAK >[129-129]SHARP_TROUGH >[130-130]SHARP_PEAK >[131-131]LOCAL_TROUGH >[132-141]LOW_VOLATILITY >>[132-132]LOCAL_PEAK >>[136-136]SHARP_TROUGH >>[137-137]SHARP_PEAK >>[138-138]SHARP_TROUGH >>[139-139]SHARP_PEAK >>[140-140]SHARP_TROUGH >[142-154]NORMAL_VOLATILITY >>[142-142]SHARP_PEAK >>[143-143]LOCAL_TROUGH >>[144-144]LOCAL_PEAK >>[145-145]SHARP_TROUGH >>[146-146]SHARP_PEAK >>[149-149]SHARP_TROUGH >>[150-150]SHARP_PEAK >>[152-152]SHARP_TROUGH >>[153-153]SHARP_PEAK >>[154-154]SHARP_TROUGH >[155-155]SHARP_PEAK >[156-156]SHARP_TROUGH >[157-157]SHARP_PEAK >[158-158]SHARP_TROUGH >[159-159]SHARP_PEAK >[161-161]SHARP_TROUGH >[163-163]LOCAL_PEAK >[164-164]LOCAL_TROUGH >[165-165]SHARP_PEAK >[166-176]NORMAL_VOLATILITY >>[166-166]SHARP_TROUGH >>[169-169]SHARP_PEAK >>[170-170]LOCAL_TROUGH >>[171-171]LOCAL_PEAK >>[175-175]SHARP_TROUGH >[177-182]LOW_VOLATILITY >>[177-177]SHARP_PEAK >>[178-178]SHARP_TROUGH >>[179-179]SHARP_PEAK >>[180-180]SHARP_TROUGH >>[181-181]SHARP_PEAK >[183-198]NORMAL_VOLATILITY >>[183-183]SHARP_TROUGH >>[185-185]SHARP_PEAK >>[186-186]SHARP_TROUGH >>[187-187]SHARP_PEAK >>[188-188]SHARP_TROUGH >>[190-190]SHARP_PEAK >>[192-192]SHARP_TROUGH >>[196-196]SHARP_PEAK >>[197-197]SHARP_TROUGH >[199-199]SHARP_PEAK >[200-200]SHARP_TROUGH >[201-201]SHARP_PEAK >[202-202]SHARP_TROUGH >[203-211]NORMAL_VOLATILITY >>[206-206]SHARP_PEAK >>[208-208]SHARP_TROUGH >>[209-209]SHARP_PEAK >>[211-211]SHARP_TROUGH >[212-212]SHARP_PEAK >[214-214]SHARP_TROUGH >[216-225]NORMAL_VOLATILITY >>[216-216]SHARP_PEAK >>[218-218]SHARP_TROUGH >>[220-220]SHARP_PEAK >>[221-221]SHARP_TROUGH >>[222-222]SHARP_PEAK >>[223-223]SHARP_TROUGH >>[224-224]SHARP_PEAK >>[225-225]SHARP_TROUGH >[226-226]SHARP_PEAK >[227-227]SHARP_TROUGH >[228-233]NORMAL_VOLATILITY >>[228-228]SHARP_PEAK >>[231-231]SHARP_TROUGH >>[233-233]SHARP_PEAK >[234-234]SHARP_TROUGH >[236-236]SHARP_PEAK >[237-237]SHARP_TROUGH >[238-238]SHARP_PEAK >[239-239]SHARP_TROUGH >[242-242]SHARP_PEAK >[243-243]SHARP_TROUGH >[245-252]NORMAL_VOLATILITY >>[248-248]SHARP_PEAK >>[251-251]SHARP_TROUGH >>[252-252]SHARP_PEAK >[253-253]SHARP_TROUGH >[256-256]SHARP_PEAK >[257-257]SHARP_TROUGH >[258-258]SHARP_PEAK >[259-259]SHARP_TROUGH >[260-260]SHARP_PEAK >[261-261]SHARP_TROUGH >[264-264]SHARP_PEAK >[265-265]LOCAL_TROUGH >[266-266]LOCAL_PEAK >[267-267]LOCAL_TROUGH >[268-275]NORMAL_VOLATILITY >>[268-268]LOCAL_PEAK >>[270-270]SHARP_TROUGH >>[271-271]LOCAL_PEAK >>[272-272]LOCAL_TROUGH >>[273-273]SHARP_PEAK >>[274-274]SHARP_TROUGH >[276-276]SHARP_PEAK >[277-277]SHARP_TROUGH >[278-283]UPTREND_SHORT >>[278-278]SHARP_PEAK >>[281-281]SHARP_TROUGH >>[283-283]SHARP_PEAK >[281-294]NORMAL_VOLATILITY >>[285-285]SHARP_TROUGH >>[286-286]SHARP_PEAK >>[288-288]SHARP_TROUGH >>[289-289]SHARP_PEAK >>[290-290]SHARP_TROUGH >>[293-293]SHARP_PEAK >>[294-294]SHARP_TROUGH >[295-295]SHARP_PEAK >[296-296]SHARP_TROUGH >[297-304]HIGH_VOLATILITY >>[297-297]SHARP_PEAK >>[298-298]SHARP_TROUGH >>[300-300]SHARP_PEAK >>[302-302]SHARP_TROUGH >>[303-303]SHARP_PEAK >>[304-304]SHARP_TROUGH >[307-307]SHARP_PEAK >[308-308]SHARP_TROUGH >[310-310]SHARP_PEAK >[311-311]SHARP_TROUGH >[313-313]SHARP_PEAK >[314-314]SHARP_TROUGH >[315-315]SHARP_PEAK >[316-316]SHARP_TROUGH >[318-338]LOW_VOLATILITY >>[318-318]SHARP_PEAK >>[320-320]SHARP_TROUGH >>[322-322]SHARP_PEAK >>[323-323]SHARP_TROUGH >>[324-324]SHARP_PEAK >>[326-326]SHARP_TROUGH >>[329-329]SHARP_PEAK >>[330-330]SHARP_TROUGH >>[337-337]SHARP_PEAK >[339-356]NORMAL_VOLATILITY >>[339-339]SHARP_TROUGH >>[341-341]SHARP_PEAK >>[342-342]SHARP_TROUGH >>[344-344]SHARP_PEAK >>[346-346]SHARP_TROUGH >>[347-347]SHARP_PEAK >>[348-348]SHARP_TROUGH >>[351-351]SHARP_PEAK >>[355-355]SHARP_TROUGH >[357-357]SHARP_PEAK >[359-359]SHARP_TROUGH >[361-366]VOLATILITY_SPIKE >>[361-361]SHARP_PEAK >>[362-362]SHARP_TROUGH >>[363-363]SHARP_PEAK >>[364-364]SHARP_TROUGH >>[365-365]SHARP_PEAK >[367-367]SHARP_TROUGH >[369-369]SHARP_PEAK >[372-372]SHARP_TROUGH >[374-374]SHARP_PEAK >[376-376]SHARP_TROUGH >[378-378]SHARP_PEAK >[380-380]SHARP_TROUGH >[381-381]SHARP_PEAK >[382-390]DOWNTREND_SHORT >>[382-382]SHARP_TROUGH >>[384-384]SHARP_PEAK >>[387-387]SHARP_TROUGH >>[388-388]SHARP_PEAK >>[389-389]SHARP_TROUGH >[391-391]LOCAL_PEAK >[392-392]LOCAL_TROUGH >[393-393]SHARP_PEAK >[395-395]SHARP_TROUGH >[396-396]SHARP_PEAK >[397-397]SHARP_TROUGH >[398-398]SHARP_PEAK >[399-399]SHARP_TROUGH >[401-401]SHARP_PEAK >[404-404]SHARP_TROUGH >[405-405]SHARP_PEAK >[406-406]SHARP_TROUGH >[407-420]NORMAL_VOLATILITY >>[407-407]SHARP_PEAK >>[408-408]SHARP_TROUGH >>[409-409]SHARP_PEAK >>[412-412]SHARP_TROUGH >>[416-416]SHARP_PEAK >>[417-417]SHARP_TROUGH >>[418-418]SHARP_PEAK >[422-422]SHARP_TROUGH >[424-452]LOW_VOLATILITY >>[424-424]SHARP_PEAK >>[425-425]SHARP_TROUGH >>[426-426]SHARP_PEAK >>[427-427]SHARP_TROUGH >>[429-429]SHARP_PEAK >>[431-431]LOCAL_TROUGH >>[432-432]LOCAL_PEAK >>[434-434]SHARP_TROUGH >>[438-438]SHARP_PEAK >>[439-439]SHARP_TROUGH >>[441-441]SHARP_PEAK >>[443-443]SHARP_TROUGH >>[444-444]LOCAL_PEAK >>[445-445]LOCAL_TROUGH >>[448-448]SHARP_PEAK >>[449-449]SHARP_TROUGH >>[452-452]SHARP_PEAK >[453-453]SHARP_TROUGH >[455-468]NORMAL_VOLATILITY >>[455-455]SHARP_PEAK >>[456-456]SHARP_TROUGH >>[457-457]SHARP_PEAK >>[458-458]SHARP_TROUGH >>[460-460]SHARP_PEAK >>[461-461]SHARP_TROUGH >>[462-462]LOCAL_PEAK >>[464-464]LOCAL_TROUGH >>[465-465]SHARP_PEAK >>[466-466]SHARP_TROUGH >[469-476]LOW_VOLATILITY >>[470-470]SHARP_PEAK >>[471-471]SHARP_TROUGH >>[474-474]SHARP_PEAK >>[476-476]SHARP_TROUGH >[477-495]NORMAL_VOLATILITY >>[477-477]SHARP_PEAK >>[479-479]SHARP_TROUGH >>[480-480]SHARP_PEAK >>[482-482]SHARP_TROUGH >>[483-483]SHARP_PEAK >>[486-486]SHARP_TROUGH >>[487-487]SHARP_PEAK >>[488-488]SHARP_TROUGH >>[491-491]SHARP_PEAK >>[492-492]SHARP_TROUGH >>[493-493]LOCAL_PEAK >>[494-494]LOCAL_TROUGH >>[495-495]LOCAL_PEAK >[496-511]LOW_VOLATILITY >>[496-496]LOCAL_TROUGH >>[498-498]SHARP_PEAK >>[499-499]SHARP_TROUGH >>[501-501]SHARP_PEAK >>[504-504]SHARP_TROUGH >>[505-505]SHARP_PEAK >>[506-506]LOCAL_TROUGH >>[507-507]LOCAL_PEAK >>[510-510]SHARP_TROUGH\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "\n",
    "# 1. Prepare your data [batch_size, sequence_length]\n",
    "x = torch.randn(100, 512)  # 100 sequences, 336 timesteps each\n",
    "\n",
    "# 2. Create dataset (this does all processing)\n",
    "dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "\n",
    "# 3. Get annotation for first sequence\n",
    "ann = dataset[0]\n",
    "\n",
    "# 4. View hierarchical structure\n",
    "ann.print_hierarchy()\n",
    "\n",
    "# 5. Generate training text\n",
    "text = ann.to_text(format='depth_marked')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3119d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[000-511] SIDEWAYS_REGIME (scale=GLOBAL) (129 children),\n",
       "   [000-019] FLAT_SEGMENT (scale=MESO) (2 children),\n",
       "     [000-014] LOW_VOLATILITY (scale=MINI) (7 children),\n",
       "       [001-001] SHARP_TROUGH (scale=MICRO),\n",
       "       [003-003] SHARP_PEAK (scale=MICRO),\n",
       "       [005-005] SHARP_TROUGH (scale=MICRO),\n",
       "       [007-007] SHARP_PEAK (scale=MICRO),\n",
       "       [009-009] SHARP_TROUGH (scale=MICRO),\n",
       "       [011-011] SHARP_PEAK (scale=MICRO),\n",
       "       [014-014] SHARP_TROUGH (scale=MICRO),\n",
       "     [015-015] SHARP_PEAK (scale=MICRO),\n",
       "   [015-035] NORMAL_VOLATILITY (scale=MESO) (9 children),\n",
       "     [020-020] SHARP_TROUGH (scale=MICRO),\n",
       "     [021-021] SHARP_PEAK (scale=MICRO),\n",
       "     [022-022] SHARP_TROUGH (scale=MICRO),\n",
       "     [025-025] SHARP_PEAK (scale=MICRO),\n",
       "     [026-026] SHARP_TROUGH (scale=MICRO),\n",
       "     [027-027] LOCAL_PEAK (scale=MICRO),\n",
       "     [028-028] LOCAL_TROUGH (scale=MICRO),\n",
       "     [029-029] SHARP_PEAK (scale=MICRO),\n",
       "     [032-032] SHARP_TROUGH (scale=MICRO),\n",
       "   [035-040] UPTREND_SHORT (scale=MINI) (3 children),\n",
       "     [035-035] SHARP_PEAK (scale=MICRO),\n",
       "     [038-038] SHARP_TROUGH (scale=MICRO),\n",
       "     [039-039] SHARP_PEAK (scale=MICRO),\n",
       "   [040-048] NORMAL_VOLATILITY (scale=MINI) (4 children),\n",
       "     [041-041] SHARP_TROUGH (scale=MICRO),\n",
       "     [045-045] SHARP_PEAK (scale=MICRO),\n",
       "     [046-046] SHARP_TROUGH (scale=MICRO),\n",
       "     [047-047] SHARP_PEAK (scale=MICRO),\n",
       "   [049-049] SHARP_TROUGH (scale=MICRO),\n",
       "   [051-051] SHARP_PEAK (scale=MICRO),\n",
       "   [053-053] SHARP_TROUGH (scale=MICRO),\n",
       "   [054-054] SHARP_PEAK (scale=MICRO),\n",
       "   [055-055] SHARP_TROUGH (scale=MICRO),\n",
       "   [056-064] HIGH_VOLATILITY (scale=MINI) (6 children),\n",
       "     [059-059] SHARP_PEAK (scale=MICRO),\n",
       "     [060-060] LOCAL_TROUGH (scale=MICRO),\n",
       "     [061-061] LOCAL_PEAK (scale=MICRO),\n",
       "     [062-062] SHARP_TROUGH (scale=MICRO),\n",
       "     [063-063] SHARP_PEAK (scale=MICRO),\n",
       "     [064-064] SHARP_TROUGH (scale=MICRO),\n",
       "   [066-066] SHARP_PEAK (scale=MICRO),\n",
       "   [067-067] SHARP_TROUGH (scale=MICRO),\n",
       "   [068-068] SHARP_PEAK (scale=MICRO),\n",
       "   [072-072] SHARP_TROUGH (scale=MICRO),\n",
       "   [073-073] SHARP_PEAK (scale=MICRO),\n",
       "   [074-074] SHARP_TROUGH (scale=MICRO),\n",
       "   [076-076] SHARP_PEAK (scale=MICRO),\n",
       "   [078-078] SHARP_TROUGH (scale=MICRO),\n",
       "   [080-087] NORMAL_VOLATILITY (scale=MINI) (5 children),\n",
       "     [081-081] SHARP_PEAK (scale=MICRO),\n",
       "     [082-082] SHARP_TROUGH (scale=MICRO),\n",
       "     [084-084] SHARP_PEAK (scale=MICRO),\n",
       "     [086-086] SHARP_TROUGH (scale=MICRO),\n",
       "     [087-087] SHARP_PEAK (scale=MICRO),\n",
       "   [088-088] SHARP_TROUGH (scale=MICRO),\n",
       "   [091-091] SHARP_PEAK (scale=MICRO),\n",
       "   [093-099] HIGH_VOLATILITY (scale=MINI) (2 children),\n",
       "     [093-093] SHARP_TROUGH (scale=MICRO),\n",
       "     [094-094] SHARP_PEAK (scale=MICRO),\n",
       "   [100-108] VOLATILITY_SPIKE (scale=MINI) (6 children),\n",
       "     [100-100] SHARP_TROUGH (scale=MICRO),\n",
       "     [101-101] LOCAL_PEAK (scale=MICRO),\n",
       "     [102-102] LOCAL_TROUGH (scale=MICRO),\n",
       "     [105-105] SHARP_PEAK (scale=MICRO),\n",
       "     [106-106] SHARP_TROUGH (scale=MICRO),\n",
       "     [108-108] SHARP_PEAK (scale=MICRO),\n",
       "   [110-110] SHARP_TROUGH (scale=MICRO),\n",
       "   [111-111] SHARP_PEAK (scale=MICRO),\n",
       "   [112-112] SHARP_TROUGH (scale=MICRO),\n",
       "   [113-119] VOLATILITY_SPIKE (scale=MINI) (2 children),\n",
       "     [113-113] SHARP_PEAK (scale=MICRO),\n",
       "     [115-115] SHARP_TROUGH (scale=MICRO),\n",
       "   [116-121] UPTREND_SHORT (scale=MINI) (3 children),\n",
       "     [116-116] SHARP_PEAK (scale=MICRO),\n",
       "     [119-119] SHARP_TROUGH (scale=MICRO),\n",
       "     [120-120] SHARP_PEAK (scale=MICRO),\n",
       "   [124-124] SHARP_TROUGH (scale=MICRO),\n",
       "   [125-125] SHARP_PEAK (scale=MICRO),\n",
       "   [126-126] SHARP_TROUGH (scale=MICRO),\n",
       "   [127-127] SHARP_PEAK (scale=MICRO),\n",
       "   [129-129] SHARP_TROUGH (scale=MICRO),\n",
       "   [130-130] SHARP_PEAK (scale=MICRO),\n",
       "   [131-131] LOCAL_TROUGH (scale=MICRO),\n",
       "   [132-141] LOW_VOLATILITY (scale=MINI) (6 children),\n",
       "     [132-132] LOCAL_PEAK (scale=MICRO),\n",
       "     [136-136] SHARP_TROUGH (scale=MICRO),\n",
       "     [137-137] SHARP_PEAK (scale=MICRO),\n",
       "     [138-138] SHARP_TROUGH (scale=MICRO),\n",
       "     [139-139] SHARP_PEAK (scale=MICRO),\n",
       "     [140-140] SHARP_TROUGH (scale=MICRO),\n",
       "   [142-154] NORMAL_VOLATILITY (scale=MINI) (10 children),\n",
       "     [142-142] SHARP_PEAK (scale=MICRO),\n",
       "     [143-143] LOCAL_TROUGH (scale=MICRO),\n",
       "     [144-144] LOCAL_PEAK (scale=MICRO),\n",
       "     [145-145] SHARP_TROUGH (scale=MICRO),\n",
       "     [146-146] SHARP_PEAK (scale=MICRO),\n",
       "     [149-149] SHARP_TROUGH (scale=MICRO),\n",
       "     [150-150] SHARP_PEAK (scale=MICRO),\n",
       "     [152-152] SHARP_TROUGH (scale=MICRO),\n",
       "     [153-153] SHARP_PEAK (scale=MICRO),\n",
       "     [154-154] SHARP_TROUGH (scale=MICRO),\n",
       "   [155-155] SHARP_PEAK (scale=MICRO),\n",
       "   [156-156] SHARP_TROUGH (scale=MICRO),\n",
       "   [157-157] SHARP_PEAK (scale=MICRO),\n",
       "   [158-158] SHARP_TROUGH (scale=MICRO),\n",
       "   [159-159] SHARP_PEAK (scale=MICRO),\n",
       "   [161-161] SHARP_TROUGH (scale=MICRO),\n",
       "   [163-163] LOCAL_PEAK (scale=MICRO),\n",
       "   [164-164] LOCAL_TROUGH (scale=MICRO),\n",
       "   [165-165] SHARP_PEAK (scale=MICRO),\n",
       "   [166-176] NORMAL_VOLATILITY (scale=MINI) (5 children),\n",
       "     [166-166] SHARP_TROUGH (scale=MICRO),\n",
       "     [169-169] SHARP_PEAK (scale=MICRO),\n",
       "     [170-170] LOCAL_TROUGH (scale=MICRO),\n",
       "     [171-171] LOCAL_PEAK (scale=MICRO),\n",
       "     [175-175] SHARP_TROUGH (scale=MICRO),\n",
       "   [177-182] LOW_VOLATILITY (scale=MINI) (5 children),\n",
       "     [177-177] SHARP_PEAK (scale=MICRO),\n",
       "     [178-178] SHARP_TROUGH (scale=MICRO),\n",
       "     [179-179] SHARP_PEAK (scale=MICRO),\n",
       "     [180-180] SHARP_TROUGH (scale=MICRO),\n",
       "     [181-181] SHARP_PEAK (scale=MICRO),\n",
       "   [183-198] NORMAL_VOLATILITY (scale=MESO) (9 children),\n",
       "     [183-183] SHARP_TROUGH (scale=MICRO),\n",
       "     [185-185] SHARP_PEAK (scale=MICRO),\n",
       "     [186-186] SHARP_TROUGH (scale=MICRO),\n",
       "     [187-187] SHARP_PEAK (scale=MICRO),\n",
       "     [188-188] SHARP_TROUGH (scale=MICRO),\n",
       "     [190-190] SHARP_PEAK (scale=MICRO),\n",
       "     [192-192] SHARP_TROUGH (scale=MICRO),\n",
       "     [196-196] SHARP_PEAK (scale=MICRO),\n",
       "     [197-197] SHARP_TROUGH (scale=MICRO),\n",
       "   [199-199] SHARP_PEAK (scale=MICRO),\n",
       "   [200-200] SHARP_TROUGH (scale=MICRO),\n",
       "   [201-201] SHARP_PEAK (scale=MICRO),\n",
       "   [202-202] SHARP_TROUGH (scale=MICRO),\n",
       "   [203-211] NORMAL_VOLATILITY (scale=MINI) (4 children),\n",
       "     [206-206] SHARP_PEAK (scale=MICRO),\n",
       "     [208-208] SHARP_TROUGH (scale=MICRO),\n",
       "     [209-209] SHARP_PEAK (scale=MICRO),\n",
       "     [211-211] SHARP_TROUGH (scale=MICRO),\n",
       "   [212-212] SHARP_PEAK (scale=MICRO),\n",
       "   [214-214] SHARP_TROUGH (scale=MICRO),\n",
       "   [216-225] NORMAL_VOLATILITY (scale=MINI) (8 children),\n",
       "     [216-216] SHARP_PEAK (scale=MICRO),\n",
       "     [218-218] SHARP_TROUGH (scale=MICRO),\n",
       "     [220-220] SHARP_PEAK (scale=MICRO),\n",
       "     [221-221] SHARP_TROUGH (scale=MICRO),\n",
       "     [222-222] SHARP_PEAK (scale=MICRO),\n",
       "     [223-223] SHARP_TROUGH (scale=MICRO),\n",
       "     [224-224] SHARP_PEAK (scale=MICRO),\n",
       "     [225-225] SHARP_TROUGH (scale=MICRO),\n",
       "   [226-226] SHARP_PEAK (scale=MICRO),\n",
       "   [227-227] SHARP_TROUGH (scale=MICRO),\n",
       "   [228-233] NORMAL_VOLATILITY (scale=MINI) (3 children),\n",
       "     [228-228] SHARP_PEAK (scale=MICRO),\n",
       "     [231-231] SHARP_TROUGH (scale=MICRO),\n",
       "     [233-233] SHARP_PEAK (scale=MICRO),\n",
       "   [234-234] SHARP_TROUGH (scale=MICRO),\n",
       "   [236-236] SHARP_PEAK (scale=MICRO),\n",
       "   [237-237] SHARP_TROUGH (scale=MICRO),\n",
       "   [238-238] SHARP_PEAK (scale=MICRO),\n",
       "   [239-239] SHARP_TROUGH (scale=MICRO),\n",
       "   [242-242] SHARP_PEAK (scale=MICRO),\n",
       "   [243-243] SHARP_TROUGH (scale=MICRO),\n",
       "   [245-252] NORMAL_VOLATILITY (scale=MINI) (3 children),\n",
       "     [248-248] SHARP_PEAK (scale=MICRO),\n",
       "     [251-251] SHARP_TROUGH (scale=MICRO),\n",
       "     [252-252] SHARP_PEAK (scale=MICRO),\n",
       "   [253-253] SHARP_TROUGH (scale=MICRO),\n",
       "   [256-256] SHARP_PEAK (scale=MICRO),\n",
       "   [257-257] SHARP_TROUGH (scale=MICRO),\n",
       "   [258-258] SHARP_PEAK (scale=MICRO),\n",
       "   [259-259] SHARP_TROUGH (scale=MICRO),\n",
       "   [260-260] SHARP_PEAK (scale=MICRO),\n",
       "   [261-261] SHARP_TROUGH (scale=MICRO),\n",
       "   [264-264] SHARP_PEAK (scale=MICRO),\n",
       "   [265-265] LOCAL_TROUGH (scale=MICRO),\n",
       "   [266-266] LOCAL_PEAK (scale=MICRO),\n",
       "   [267-267] LOCAL_TROUGH (scale=MICRO),\n",
       "   [268-275] NORMAL_VOLATILITY (scale=MINI) (6 children),\n",
       "     [268-268] LOCAL_PEAK (scale=MICRO),\n",
       "     [270-270] SHARP_TROUGH (scale=MICRO),\n",
       "     [271-271] LOCAL_PEAK (scale=MICRO),\n",
       "     [272-272] LOCAL_TROUGH (scale=MICRO),\n",
       "     [273-273] SHARP_PEAK (scale=MICRO),\n",
       "     [274-274] SHARP_TROUGH (scale=MICRO),\n",
       "   [276-276] SHARP_PEAK (scale=MICRO),\n",
       "   [277-277] SHARP_TROUGH (scale=MICRO),\n",
       "   [278-283] UPTREND_SHORT (scale=MINI) (3 children),\n",
       "     [278-278] SHARP_PEAK (scale=MICRO),\n",
       "     [281-281] SHARP_TROUGH (scale=MICRO),\n",
       "     [283-283] SHARP_PEAK (scale=MICRO),\n",
       "   [281-294] NORMAL_VOLATILITY (scale=MINI) (7 children),\n",
       "     [285-285] SHARP_TROUGH (scale=MICRO),\n",
       "     [286-286] SHARP_PEAK (scale=MICRO),\n",
       "     [288-288] SHARP_TROUGH (scale=MICRO),\n",
       "     [289-289] SHARP_PEAK (scale=MICRO),\n",
       "     [290-290] SHARP_TROUGH (scale=MICRO),\n",
       "     [293-293] SHARP_PEAK (scale=MICRO),\n",
       "     [294-294] SHARP_TROUGH (scale=MICRO),\n",
       "   [295-295] SHARP_PEAK (scale=MICRO),\n",
       "   [296-296] SHARP_TROUGH (scale=MICRO),\n",
       "   [297-304] HIGH_VOLATILITY (scale=MINI) (6 children),\n",
       "     [297-297] SHARP_PEAK (scale=MICRO),\n",
       "     [298-298] SHARP_TROUGH (scale=MICRO),\n",
       "     [300-300] SHARP_PEAK (scale=MICRO),\n",
       "     [302-302] SHARP_TROUGH (scale=MICRO),\n",
       "     [303-303] SHARP_PEAK (scale=MICRO),\n",
       "     [304-304] SHARP_TROUGH (scale=MICRO),\n",
       "   [307-307] SHARP_PEAK (scale=MICRO),\n",
       "   [308-308] SHARP_TROUGH (scale=MICRO),\n",
       "   [310-310] SHARP_PEAK (scale=MICRO),\n",
       "   [311-311] SHARP_TROUGH (scale=MICRO),\n",
       "   [313-313] SHARP_PEAK (scale=MICRO),\n",
       "   [314-314] SHARP_TROUGH (scale=MICRO),\n",
       "   [315-315] SHARP_PEAK (scale=MICRO),\n",
       "   [316-316] SHARP_TROUGH (scale=MICRO),\n",
       "   [318-338] LOW_VOLATILITY (scale=MESO) (9 children),\n",
       "     [318-318] SHARP_PEAK (scale=MICRO),\n",
       "     [320-320] SHARP_TROUGH (scale=MICRO),\n",
       "     [322-322] SHARP_PEAK (scale=MICRO),\n",
       "     [323-323] SHARP_TROUGH (scale=MICRO),\n",
       "     [324-324] SHARP_PEAK (scale=MICRO),\n",
       "     [326-326] SHARP_TROUGH (scale=MICRO),\n",
       "     [329-329] SHARP_PEAK (scale=MICRO),\n",
       "     [330-330] SHARP_TROUGH (scale=MICRO),\n",
       "     [337-337] SHARP_PEAK (scale=MICRO),\n",
       "   [339-356] NORMAL_VOLATILITY (scale=MESO) (9 children),\n",
       "     [339-339] SHARP_TROUGH (scale=MICRO),\n",
       "     [341-341] SHARP_PEAK (scale=MICRO),\n",
       "     [342-342] SHARP_TROUGH (scale=MICRO),\n",
       "     [344-344] SHARP_PEAK (scale=MICRO),\n",
       "     [346-346] SHARP_TROUGH (scale=MICRO),\n",
       "     [347-347] SHARP_PEAK (scale=MICRO),\n",
       "     [348-348] SHARP_TROUGH (scale=MICRO),\n",
       "     [351-351] SHARP_PEAK (scale=MICRO),\n",
       "     [355-355] SHARP_TROUGH (scale=MICRO),\n",
       "   [357-357] SHARP_PEAK (scale=MICRO),\n",
       "   [359-359] SHARP_TROUGH (scale=MICRO),\n",
       "   [361-366] VOLATILITY_SPIKE (scale=MINI) (5 children),\n",
       "     [361-361] SHARP_PEAK (scale=MICRO),\n",
       "     [362-362] SHARP_TROUGH (scale=MICRO),\n",
       "     [363-363] SHARP_PEAK (scale=MICRO),\n",
       "     [364-364] SHARP_TROUGH (scale=MICRO),\n",
       "     [365-365] SHARP_PEAK (scale=MICRO),\n",
       "   [367-367] SHARP_TROUGH (scale=MICRO),\n",
       "   [369-369] SHARP_PEAK (scale=MICRO),\n",
       "   [372-372] SHARP_TROUGH (scale=MICRO),\n",
       "   [374-374] SHARP_PEAK (scale=MICRO),\n",
       "   [376-376] SHARP_TROUGH (scale=MICRO),\n",
       "   [378-378] SHARP_PEAK (scale=MICRO),\n",
       "   [380-380] SHARP_TROUGH (scale=MICRO),\n",
       "   [381-381] SHARP_PEAK (scale=MICRO),\n",
       "   [382-390] DOWNTREND_SHORT (scale=MINI) (5 children),\n",
       "     [382-382] SHARP_TROUGH (scale=MICRO),\n",
       "     [384-384] SHARP_PEAK (scale=MICRO),\n",
       "     [387-387] SHARP_TROUGH (scale=MICRO),\n",
       "     [388-388] SHARP_PEAK (scale=MICRO),\n",
       "     [389-389] SHARP_TROUGH (scale=MICRO),\n",
       "   [391-391] LOCAL_PEAK (scale=MICRO),\n",
       "   [392-392] LOCAL_TROUGH (scale=MICRO),\n",
       "   [393-393] SHARP_PEAK (scale=MICRO),\n",
       "   [395-395] SHARP_TROUGH (scale=MICRO),\n",
       "   [396-396] SHARP_PEAK (scale=MICRO),\n",
       "   [397-397] SHARP_TROUGH (scale=MICRO),\n",
       "   [398-398] SHARP_PEAK (scale=MICRO),\n",
       "   [399-399] SHARP_TROUGH (scale=MICRO),\n",
       "   [401-401] SHARP_PEAK (scale=MICRO),\n",
       "   [404-404] SHARP_TROUGH (scale=MICRO),\n",
       "   [405-405] SHARP_PEAK (scale=MICRO),\n",
       "   [406-406] SHARP_TROUGH (scale=MICRO),\n",
       "   [407-420] NORMAL_VOLATILITY (scale=MINI) (7 children),\n",
       "     [407-407] SHARP_PEAK (scale=MICRO),\n",
       "     [408-408] SHARP_TROUGH (scale=MICRO),\n",
       "     [409-409] SHARP_PEAK (scale=MICRO),\n",
       "     [412-412] SHARP_TROUGH (scale=MICRO),\n",
       "     [416-416] SHARP_PEAK (scale=MICRO),\n",
       "     [417-417] SHARP_TROUGH (scale=MICRO),\n",
       "     [418-418] SHARP_PEAK (scale=MICRO),\n",
       "   [422-422] SHARP_TROUGH (scale=MICRO),\n",
       "   [424-452] LOW_VOLATILITY (scale=MESO) (17 children),\n",
       "     [424-424] SHARP_PEAK (scale=MICRO),\n",
       "     [425-425] SHARP_TROUGH (scale=MICRO),\n",
       "     [426-426] SHARP_PEAK (scale=MICRO),\n",
       "     [427-427] SHARP_TROUGH (scale=MICRO),\n",
       "     [429-429] SHARP_PEAK (scale=MICRO),\n",
       "     [431-431] LOCAL_TROUGH (scale=MICRO),\n",
       "     [432-432] LOCAL_PEAK (scale=MICRO),\n",
       "     [434-434] SHARP_TROUGH (scale=MICRO),\n",
       "     [438-438] SHARP_PEAK (scale=MICRO),\n",
       "     [439-439] SHARP_TROUGH (scale=MICRO),\n",
       "     [441-441] SHARP_PEAK (scale=MICRO),\n",
       "     [443-443] SHARP_TROUGH (scale=MICRO),\n",
       "     [444-444] LOCAL_PEAK (scale=MICRO),\n",
       "     [445-445] LOCAL_TROUGH (scale=MICRO),\n",
       "     [448-448] SHARP_PEAK (scale=MICRO),\n",
       "     [449-449] SHARP_TROUGH (scale=MICRO),\n",
       "     [452-452] SHARP_PEAK (scale=MICRO),\n",
       "   [453-453] SHARP_TROUGH (scale=MICRO),\n",
       "   [455-468] NORMAL_VOLATILITY (scale=MINI) (10 children),\n",
       "     [455-455] SHARP_PEAK (scale=MICRO),\n",
       "     [456-456] SHARP_TROUGH (scale=MICRO),\n",
       "     [457-457] SHARP_PEAK (scale=MICRO),\n",
       "     [458-458] SHARP_TROUGH (scale=MICRO),\n",
       "     [460-460] SHARP_PEAK (scale=MICRO),\n",
       "     [461-461] SHARP_TROUGH (scale=MICRO),\n",
       "     [462-462] LOCAL_PEAK (scale=MICRO),\n",
       "     [464-464] LOCAL_TROUGH (scale=MICRO),\n",
       "     [465-465] SHARP_PEAK (scale=MICRO),\n",
       "     [466-466] SHARP_TROUGH (scale=MICRO),\n",
       "   [469-476] LOW_VOLATILITY (scale=MINI) (4 children),\n",
       "     [470-470] SHARP_PEAK (scale=MICRO),\n",
       "     [471-471] SHARP_TROUGH (scale=MICRO),\n",
       "     [474-474] SHARP_PEAK (scale=MICRO),\n",
       "     [476-476] SHARP_TROUGH (scale=MICRO),\n",
       "   [477-495] NORMAL_VOLATILITY (scale=MESO) (13 children),\n",
       "     [477-477] SHARP_PEAK (scale=MICRO),\n",
       "     [479-479] SHARP_TROUGH (scale=MICRO),\n",
       "     [480-480] SHARP_PEAK (scale=MICRO),\n",
       "     [482-482] SHARP_TROUGH (scale=MICRO),\n",
       "     [483-483] SHARP_PEAK (scale=MICRO),\n",
       "     [486-486] SHARP_TROUGH (scale=MICRO),\n",
       "     [487-487] SHARP_PEAK (scale=MICRO),\n",
       "     [488-488] SHARP_TROUGH (scale=MICRO),\n",
       "     [491-491] SHARP_PEAK (scale=MICRO),\n",
       "     [492-492] SHARP_TROUGH (scale=MICRO),\n",
       "     [493-493] LOCAL_PEAK (scale=MICRO),\n",
       "     [494-494] LOCAL_TROUGH (scale=MICRO),\n",
       "     [495-495] LOCAL_PEAK (scale=MICRO),\n",
       "   [496-511] LOW_VOLATILITY (scale=MESO) (9 children),\n",
       "     [496-496] LOCAL_TROUGH (scale=MICRO),\n",
       "     [498-498] SHARP_PEAK (scale=MICRO),\n",
       "     [499-499] SHARP_TROUGH (scale=MICRO),\n",
       "     [501-501] SHARP_PEAK (scale=MICRO),\n",
       "     [504-504] SHARP_TROUGH (scale=MICRO),\n",
       "     [505-505] SHARP_PEAK (scale=MICRO),\n",
       "     [506-506] LOCAL_TROUGH (scale=MICRO),\n",
       "     [507-507] LOCAL_PEAK (scale=MICRO),\n",
       "     [510-510] SHARP_TROUGH (scale=MICRO)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae54a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0412b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cb084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce52cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa23a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785adb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd03730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a325e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5157001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4117019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
      "Demonstration\n",
      "================================================================================\n",
      "\n",
      "Generating 20 synthetic sequences of length 336...\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 20\n",
      "Length: 336\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 6720 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/20...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 998\n",
      "      Avg per sequence: 49.9\n",
      "      By scale:\n",
      "        MICRO.......   23.2 per sequence\n",
      "        MINI........   17.6 per sequence\n",
      "        MESO........    8.1 per sequence\n",
      "        MACRO.......    0.0 per sequence\n",
      "        GLOBAL......    1.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: HIERARCHICAL STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "Hierarchical Events (Total: 54)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (26 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO)\n",
      "  [003-018] NORMAL_VOLATILITY (scale=MESO) (2 children)\n",
      "    [003-003] SHARP_TROUGH (scale=MICRO)\n",
      "    [009-009] SHARP_PEAK (scale=MICRO)\n",
      "  [019-030] HIGH_VOLATILITY (scale=MINI)\n",
      "  [021-027] UPTREND_SHORT (scale=MINI)\n",
      "  [032-032] LOCAL_TROUGH (scale=MICRO)\n",
      "  [034-044] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [045-060] LOW_VOLATILITY (scale=MESO) (2 children)\n",
      "    [045-045] LOCAL_PEAK (scale=MICRO)\n",
      "    [056-056] SHARP_TROUGH (scale=MICRO)\n",
      "  [061-077] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [063-068] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "      [066-066] SHARP_PEAK (scale=MICRO)\n",
      "  [078-078] SHARP_TROUGH (scale=MICRO)\n",
      "  [081-097] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [094-094] LOCAL_PEAK (scale=MICRO)\n",
      "  [098-127] LOW_VOLATILITY (scale=MESO) (3 children)\n",
      "    [103-108] FLAT_SEGMENT (scale=MINI)\n",
      "    [112-112] SHARP_TROUGH (scale=MICRO)\n",
      "    [116-122] FLAT_SEGMENT (scale=MINI)\n",
      "  [128-137] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [128-128] SHARP_PEAK (scale=MICRO)\n",
      "  [138-157] HIGH_VOLATILITY (scale=MESO) (1 children)\n",
      "    [138-138] SHARP_TROUGH (scale=MICRO)\n",
      "  [158-168] LOW_VOLATILITY (scale=MINI)\n",
      "  [169-185] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [175-175] SHARP_PEAK (scale=MICRO)\n",
      "  [186-186] SHARP_TROUGH (scale=MICRO)\n",
      "  [188-211] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [192-192] SHARP_PEAK (scale=MICRO)\n",
      "  [213-218] HIGH_VOLATILITY (scale=MINI) (1 children)\n",
      "    [213-213] SHARP_TROUGH (scale=MICRO)\n",
      "  [219-232] VOLATILITY_SPIKE (scale=MINI) (1 children)\n",
      "    [219-219] LOCAL_PEAK (scale=MICRO)\n",
      "  [240-259] VOLATILITY_SPIKE (scale=MESO) (3 children)\n",
      "    [240-240] SHARP_TROUGH (scale=MICRO)\n",
      "    [247-247] SHARP_PEAK (scale=MICRO)\n",
      "    [259-259] LOCAL_TROUGH (scale=MICRO)\n",
      "  [260-270] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [268-268] LOCAL_PEAK (scale=MICRO)\n",
      "  [271-276] FLAT_SEGMENT (scale=MINI) (1 children)\n",
      "    [274-274] LOCAL_TROUGH (scale=MICRO)\n",
      "  [271-279] LOW_VOLATILITY (scale=MINI)\n",
      "  [277-286] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "    [281-281] SHARP_PEAK (scale=MICRO)\n",
      "  [280-321] NORMAL_VOLATILITY (scale=MESO) (3 children)\n",
      "    [289-289] SHARP_TROUGH (scale=MICRO)\n",
      "    [296-296] SHARP_PEAK (scale=MICRO)\n",
      "    [307-307] SHARP_TROUGH (scale=MICRO)\n",
      "  [322-335] LOW_VOLATILITY (scale=MINI) (2 children)\n",
      "    [322-322] LOCAL_PEAK (scale=MICRO)\n",
      "    [330-330] LOCAL_TROUGH (scale=MICRO)\n",
      "\n",
      "================================================================================\n",
      "EVENTS BY HIERARCHICAL SCALE\n",
      "================================================================================\n",
      "\n",
      "MICRO (27 events):\n",
      "  [003-003] SHARP_TROUGH\n",
      "  [009-009] SHARP_PEAK\n",
      "  [032-032] LOCAL_TROUGH\n",
      "  [045-045] LOCAL_PEAK\n",
      "  [056-056] SHARP_TROUGH\n",
      "\n",
      "MINI (15 events):\n",
      "  [019-030] HIGH_VOLATILITY\n",
      "  [021-027] UPTREND_SHORT\n",
      "  [034-044] NORMAL_VOLATILITY\n",
      "  [063-068] UPTREND_SHORT\n",
      "  [103-108] FLAT_SEGMENT\n",
      "\n",
      "MESO (11 events):\n",
      "  [000-019] FLAT_SEGMENT\n",
      "  [003-018] NORMAL_VOLATILITY\n",
      "  [045-060] LOW_VOLATILITY\n",
      "  [061-077] NORMAL_VOLATILITY\n",
      "  [081-097] NORMAL_VOLATILITY\n",
      "\n",
      "MACRO (0 events):\n",
      "\n",
      "GLOBAL (1 events):\n",
      "  [000-335] SIDEWAYS_REGIME\n",
      "\n",
      "================================================================================\n",
      "TEXT GENERATION FOR LM TRAINING\n",
      "================================================================================\n",
      "\n",
      "DEPTH_MARKED:\n",
      "  Tokens: 54, Chars: 1277\n",
      "  Preview: [0-335]SIDEWAYS_REGIME >[0-19]FLAT_SEGMENT >[3-18]NORMAL_VOLATILITY >>[3-3]SHARP_TROUGH >>[9-9]SHARP_PEAK >[19-30]HIGH_VOLATILITY >[21-27]UPTREND_SHORT >[32-32]LOCAL_TROUGH >[34-44]NORMAL_VOLATILITY >...\n",
      "\n",
      "FLAT:\n",
      "  Tokens: 54, Chars: 1196\n",
      "  Preview: [0-335]SIDEWAYS_REGIME [0-19]FLAT_SEGMENT [3-18]NORMAL_VOLATILITY [3-3]SHARP_TROUGH [9-9]SHARP_PEAK [19-30]HIGH_VOLATILITY [21-27]UPTREND_SHORT [32-32]LOCAL_TROUGH [34-44]NORMAL_VOLATILITY [45-60]LOW_...\n",
      "\n",
      "NARRATIVE:\n",
      "  Tokens: 3, Chars: 25\n",
      "  Preview: Overall: sideways regime....\n",
      "\n",
      "================================================================================\n",
      "FULL CORPUS STATISTICS\n",
      "================================================================================\n",
      "  num_documents: 20\n",
      "  total_tokens: 998\n",
      "  total_chars: 23,894\n",
      "  avg_tokens_per_doc: 49.9\n",
      "  avg_chars_per_doc: 1,194.7\n",
      "\n",
      "================================================================================\n",
      "✓ DEMONSTRATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "The system is ready for processing real time series data.\n",
      "Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\n",
    "===============================================\n",
    "\n",
    "A comprehensive system for detecting and labeling events in time series data\n",
    "with hierarchical structure preservation.\n",
    "\n",
    "Workflow:\n",
    "    1. Define vocabulary and hierarchical structures\n",
    "    2. Extract multi-scale features from raw time series\n",
    "    3. Encode step-wise labels for each timestep\n",
    "    4. Detect higher-level events (trends, peaks, volatility, change points)\n",
    "    5. Build hierarchical event tree\n",
    "    6. Generate training text in various formats\n",
    "\n",
    "Author: Sachith\n",
    "Date: December 2024\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from scipy import signal as scipy_signal\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: CORE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "class EventScale(IntEnum):\n",
    "    \"\"\"Hierarchical scale levels for events\"\"\"\n",
    "    MICRO = 1      # 1-5 timesteps (spikes, single points)\n",
    "    MINI = 2       # 5-15 timesteps (very short segments)\n",
    "    MESO = 3       # 15-50 timesteps (medium segments, local patterns)\n",
    "    MACRO = 4      # 50-150 timesteps (major trends)\n",
    "    GLOBAL = 5     # 150+ timesteps (full sequence characteristics)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EventVocabulary:\n",
    "    \"\"\"\n",
    "    Complete event vocabulary with 64 distinct labels.\n",
    "    \n",
    "    Categories:\n",
    "        - Special tokens (0-2)\n",
    "        - Step movements (3-10)\n",
    "        - Trend segments (20-26)\n",
    "        - Peaks/troughs (30-33)\n",
    "        - Volatility regimes (40-43)\n",
    "        - Change points (50-51)\n",
    "        - Global regimes (60-63)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    PAD = 0\n",
    "    MASK = 1\n",
    "    FLAT = 2\n",
    "    \n",
    "    # Step-level movements\n",
    "    UP_SMALL = 3\n",
    "    UP_MEDIUM = 4\n",
    "    UP_LARGE = 5\n",
    "    DOWN_SMALL = 6\n",
    "    DOWN_MEDIUM = 7\n",
    "    DOWN_LARGE = 8\n",
    "    SPIKE_UP = 9\n",
    "    SPIKE_DOWN = 10\n",
    "    \n",
    "    # Trend segments\n",
    "    UPTREND_SHORT = 20\n",
    "    UPTREND_MEDIUM = 21\n",
    "    UPTREND_LONG = 22\n",
    "    DOWNTREND_SHORT = 23\n",
    "    DOWNTREND_MEDIUM = 24\n",
    "    DOWNTREND_LONG = 25\n",
    "    FLAT_SEGMENT = 26\n",
    "    \n",
    "    # Peaks and troughs\n",
    "    LOCAL_PEAK = 30\n",
    "    SHARP_PEAK = 31\n",
    "    LOCAL_TROUGH = 32\n",
    "    SHARP_TROUGH = 33\n",
    "    \n",
    "    # Volatility regimes\n",
    "    LOW_VOLATILITY = 40\n",
    "    NORMAL_VOLATILITY = 41\n",
    "    HIGH_VOLATILITY = 42\n",
    "    VOLATILITY_SPIKE = 43\n",
    "    \n",
    "    # Change points\n",
    "    MEAN_SHIFT_UP = 50\n",
    "    MEAN_SHIFT_DOWN = 51\n",
    "    \n",
    "    # Global regimes\n",
    "    BULLISH_REGIME = 60\n",
    "    BEARISH_REGIME = 61\n",
    "    SIDEWAYS_REGIME = 62\n",
    "    VOLATILE_REGIME = 63\n",
    "    \n",
    "    @classmethod\n",
    "    def get_vocab_size(cls) -> int:\n",
    "        \"\"\"Return total vocabulary size\"\"\"\n",
    "        return 64\n",
    "    \n",
    "    @classmethod\n",
    "    def id_to_label(cls, idx: int) -> str:\n",
    "        \"\"\"Convert label ID to string name\"\"\"\n",
    "        for name, value in vars(cls).items():\n",
    "            if isinstance(value, int) and value == idx:\n",
    "                return name\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalEvent:\n",
    "    \"\"\"\n",
    "    Event node in hierarchical tree structure.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting timestep index\n",
    "        end: Ending timestep index\n",
    "        label: Vocabulary ID\n",
    "        label_name: Human-readable label\n",
    "        scale: Hierarchical scale level\n",
    "        event_type: Category (trend/peak/volatility/changepoint/regime)\n",
    "        confidence: Detection confidence score\n",
    "        metadata: Additional event-specific information\n",
    "        parent: Parent event in hierarchy (None for root)\n",
    "        children: List of child events\n",
    "    \"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    label_name: str\n",
    "    scale: EventScale\n",
    "    event_type: str\n",
    "    confidence: float\n",
    "    metadata: Dict\n",
    "    parent: Optional['HierarchicalEvent'] = None\n",
    "    children: List['HierarchicalEvent'] = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def duration(self) -> int:\n",
    "        \"\"\"Duration in timesteps\"\"\"\n",
    "        return self.end - self.start + 1\n",
    "    \n",
    "    @property\n",
    "    def depth(self) -> int:\n",
    "        \"\"\"Depth in hierarchy tree (0 = root)\"\"\"\n",
    "        depth = 0\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            depth += 1\n",
    "            node = node.parent\n",
    "        return depth\n",
    "    \n",
    "    def contains(self, other: 'HierarchicalEvent') -> bool:\n",
    "        \"\"\"Check if this event fully contains another event\"\"\"\n",
    "        return (self.start <= other.start and \n",
    "                self.end >= other.end and\n",
    "                self != other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        indent = \"  \" * self.depth\n",
    "        children_info = f\" ({len(self.children)} children)\" if self.children else \"\"\n",
    "        return (f\"{indent}[{self.start:03d}-{self.end:03d}] {self.label_name} \"\n",
    "                f\"(scale={self.scale.name}){children_info}\")\n",
    "\n",
    "\n",
    "# Global vocabulary instance\n",
    "VOCAB = EventVocabulary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "class MultiScaleFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features at multiple temporal scales using efficient convolutions.\n",
    "    \n",
    "    Features computed:\n",
    "        - First derivative (dx)\n",
    "        - Rolling mean at multiple window sizes\n",
    "        - Rolling standard deviation (volatility)\n",
    "        - Rolling slope (trend strength)\n",
    "        - Z-scores for anomaly detection\n",
    "    \n",
    "    Args:\n",
    "        scales: List of window sizes for rolling features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scales: List[int] = [5, 10, 20, 50]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def extract_features(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from time series batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor of shape [B, L]\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of feature tensors, each shape [B, L]:\n",
    "                - 'dx': First derivative\n",
    "                - 'mean_{w}': Rolling mean with window w\n",
    "                - 'std_{w}': Rolling std with window w\n",
    "                - 'slope_{w}': Rolling slope with window w\n",
    "                - 'zscore': Normalized z-scores\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        features = {}\n",
    "        \n",
    "        # First derivative (rate of change)\n",
    "        dx = torch.diff(x, dim=1)  # [B, L-1]\n",
    "        features['dx'] = F.pad(dx, (1, 0), value=0)  # [B, L]\n",
    "        \n",
    "        # Multi-scale rolling features\n",
    "        for w in self.scales:\n",
    "            if w >= L:\n",
    "                continue\n",
    "            \n",
    "            x_3d = x.unsqueeze(1)  # [B, 1, L]\n",
    "            kernel = torch.ones(1, 1, w, device=device) / w\n",
    "            padding = w - 1\n",
    "            \n",
    "            # Rolling mean using efficient convolution\n",
    "            x_padded = F.pad(x_3d, (padding, 0), mode='replicate')\n",
    "            rolling_mean = F.conv1d(x_padded, kernel).squeeze(1)\n",
    "            features[f'mean_{w}'] = rolling_mean\n",
    "            \n",
    "            # Rolling standard deviation (volatility measure)\n",
    "            x_diff = x.unsqueeze(1) - rolling_mean.unsqueeze(1)\n",
    "            x_diff_padded = F.pad(x_diff, (padding, 0), mode='replicate')\n",
    "            rolling_var = F.conv1d(x_diff_padded ** 2, kernel).squeeze(1)\n",
    "            features[f'std_{w}'] = torch.sqrt(rolling_var.clamp(min=1e-8))\n",
    "            \n",
    "            # Rolling slope (trend direction and strength)\n",
    "            slopes = self._compute_slopes(x, w)\n",
    "            features[f'slope_{w}'] = slopes\n",
    "        \n",
    "        # Z-scores for outlier detection\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        std = x.std(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "        features['zscore'] = (x - mean) / std\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_slopes(self, x: torch.Tensor, window: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute rolling linear slope over window.\n",
    "        \n",
    "        Simple implementation: slope = (end - start) / window\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        slopes = torch.zeros(B, L, device=x.device)\n",
    "        \n",
    "        for i in range(window, L):\n",
    "            slopes[:, i] = (x[:, i] - x[:, i-window+1]) / window\n",
    "        \n",
    "        return slopes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: STEP-WISE LABEL ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "class StepWiseEncoder:\n",
    "    \"\"\"\n",
    "    Encode each timestep with symbolic movement labels.\n",
    "    \n",
    "    Labels based on magnitude of first derivative:\n",
    "        - FLAT: negligible change\n",
    "        - UP/DOWN_SMALL/MEDIUM/LARGE: quantile-based magnitude bins\n",
    "        - SPIKE_UP/DOWN: extreme changes (>90th percentile)\n",
    "    \"\"\"\n",
    "    \n",
    "    def encode(self, x: torch.Tensor, features: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode step-wise movement labels for entire batch.\n",
    "        \n",
    "        Args:\n",
    "            x: Time series tensor [B, L]\n",
    "            features: Feature dictionary from MultiScaleFeatureExtractor\n",
    "        \n",
    "        Returns:\n",
    "            Label tensor [B, L] with vocabulary IDs\n",
    "        \"\"\"\n",
    "        B, L = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        dx = features['dx']\n",
    "        abs_dx = torch.abs(dx[:, 1:])  # Skip first padded value\n",
    "        \n",
    "        if abs_dx.numel() == 0:\n",
    "            return torch.full((B, L), VOCAB.FLAT, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Compute quantiles for adaptive thresholding\n",
    "        q33, q66, q90 = torch.quantile(\n",
    "            abs_dx.reshape(-1),\n",
    "            torch.tensor([0.33, 0.66, 0.90], device=device)\n",
    "        )\n",
    "        \n",
    "        epsilon = 0.1 * q33  # Flat threshold\n",
    "        labels = torch.full((B, L), VOCAB.PAD, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Classify each timestep\n",
    "        for t in range(1, L):\n",
    "            diff = dx[:, t]\n",
    "            abs_diff = torch.abs(diff)\n",
    "            \n",
    "            # Flat (negligible change)\n",
    "            flat_mask = abs_diff < epsilon\n",
    "            labels[flat_mask, t] = VOCAB.FLAT\n",
    "            \n",
    "            # Upward movements\n",
    "            up_mask = (diff > 0) & (~flat_mask)\n",
    "            labels[up_mask & (abs_diff <= q33), t] = VOCAB.UP_SMALL\n",
    "            labels[up_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.UP_MEDIUM\n",
    "            labels[up_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.UP_LARGE\n",
    "            labels[up_mask & (abs_diff > q90), t] = VOCAB.SPIKE_UP\n",
    "            \n",
    "            # Downward movements\n",
    "            down_mask = (diff < 0) & (~flat_mask)\n",
    "            labels[down_mask & (abs_diff <= q33), t] = VOCAB.DOWN_SMALL\n",
    "            labels[down_mask & (abs_diff > q33) & (abs_diff <= q66), t] = VOCAB.DOWN_MEDIUM\n",
    "            labels[down_mask & (abs_diff > q66) & (abs_diff <= q90), t] = VOCAB.DOWN_LARGE\n",
    "            labels[down_mask & (abs_diff > q90), t] = VOCAB.SPIKE_DOWN\n",
    "        \n",
    "        return labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: EVENT DETECTORS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SimpleSegment:\n",
    "    \"\"\"Simple segment representation for detector outputs\"\"\"\n",
    "    start: int\n",
    "    end: int\n",
    "    label: int\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TrendSegmentDetector:\n",
    "    \"\"\"\n",
    "    Detect trend segments using slope sign changes.\n",
    "    \n",
    "    Algorithm:\n",
    "        1. Compute rolling slopes\n",
    "        2. Find sign changes (trend reversals)\n",
    "        3. Classify segments by direction and duration\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect trend segments in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of trend segments\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        L = len(x_np)\n",
    "        \n",
    "        # Get slopes\n",
    "        if 'slope_20' in features:\n",
    "            slopes = features['slope_20'][idx].cpu().numpy()\n",
    "        else:\n",
    "            slopes = np.gradient(x_np)\n",
    "        \n",
    "        # Find trend direction changes\n",
    "        slope_sign = np.sign(slopes)\n",
    "        slope_sign[np.abs(slopes) < 0.01] = 0  # Flat threshold\n",
    "        \n",
    "        changes = np.where(np.diff(slope_sign) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        segments = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short segments\n",
    "                continue\n",
    "            \n",
    "            avg_slope = slopes[start:end+1].mean()\n",
    "            duration = end - start + 1\n",
    "            \n",
    "            # Classify segment\n",
    "            if abs(avg_slope) < 0.01:\n",
    "                label = VOCAB.FLAT_SEGMENT\n",
    "            elif avg_slope > 0:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.UPTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.UPTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.UPTREND_LONG\n",
    "            else:\n",
    "                if duration < 30:\n",
    "                    label = VOCAB.DOWNTREND_SHORT\n",
    "                elif duration < 80:\n",
    "                    label = VOCAB.DOWNTREND_MEDIUM\n",
    "                else:\n",
    "                    label = VOCAB.DOWNTREND_LONG\n",
    "            \n",
    "            segments.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label,\n",
    "                metadata={'slope': float(avg_slope)}\n",
    "            ))\n",
    "        \n",
    "        return segments\n",
    "\n",
    "\n",
    "class PeakTroughDetector:\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs using scipy's find_peaks with proper filtering.\n",
    "    \n",
    "    Key improvements:\n",
    "        - Minimum distance enforcement (prevent adjacent peaks)\n",
    "        - Adaptive prominence thresholds\n",
    "        - Peak-trough alternation validation\n",
    "    \n",
    "    Classifies peaks/troughs by:\n",
    "        - Prominence: How much the peak stands out\n",
    "        - Type: Sharp vs broad based on width\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_distance: int = 10, min_prominence_percentile: float = 75):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_distance: Minimum timesteps between peaks/troughs\n",
    "            min_prominence_percentile: Percentile for adaptive prominence threshold\n",
    "        \"\"\"\n",
    "        self.min_distance = min_distance\n",
    "        self.min_prominence_percentile = min_prominence_percentile\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect peaks and troughs in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            idx: Sequence index (unused but kept for consistency)\n",
    "        \n",
    "        Returns:\n",
    "            List of peak/trough events (alternating peaks and troughs)\n",
    "        \"\"\"\n",
    "        x_np = x.cpu().numpy()\n",
    "        \n",
    "        # Compute adaptive prominence threshold\n",
    "        std = np.std(x_np)\n",
    "        min_prominence = max(0.2 * std, 0.1)  # At least 0.2 std or 0.1 absolute\n",
    "        \n",
    "        events = []\n",
    "        \n",
    "        # Find peaks\n",
    "        try:\n",
    "            peaks, props = scipy_signal.find_peaks(\n",
    "                x_np, \n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for pk, prom in zip(peaks, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_PEAK\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_PEAK\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(pk),\n",
    "                    end=int(pk),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'peak'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Find troughs (peaks of inverted signal)\n",
    "        try:\n",
    "            troughs, props = scipy_signal.find_peaks(\n",
    "                -x_np,\n",
    "                prominence=min_prominence,\n",
    "                distance=self.min_distance,  # Enforce minimum separation\n",
    "                width=1\n",
    "            )\n",
    "            \n",
    "            for tr, prom in zip(troughs, props['prominences']):\n",
    "                # Classify by prominence\n",
    "                if prom > std:\n",
    "                    label = VOCAB.SHARP_TROUGH\n",
    "                else:\n",
    "                    label = VOCAB.LOCAL_TROUGH\n",
    "                \n",
    "                events.append(SimpleSegment(\n",
    "                    start=int(tr),\n",
    "                    end=int(tr),\n",
    "                    label=label,\n",
    "                    metadata={\n",
    "                        'prominence': float(prom),\n",
    "                        'type': 'trough'\n",
    "                    }\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Sort by position and validate alternation\n",
    "        events = self._validate_alternation(events)\n",
    "        \n",
    "        return events\n",
    "    \n",
    "    def _validate_alternation(self, events: List[SimpleSegment]) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Ensure peaks and troughs alternate (remove consecutive same-type events).\n",
    "        Keep the more prominent one when there are consecutive same types.\n",
    "        \"\"\"\n",
    "        if len(events) <= 1:\n",
    "            return events\n",
    "        \n",
    "        # Sort by position\n",
    "        events.sort(key=lambda e: e.start)\n",
    "        \n",
    "        filtered = [events[0]]\n",
    "        \n",
    "        for event in events[1:]:\n",
    "            last_event = filtered[-1]\n",
    "            \n",
    "            # Check if types alternate\n",
    "            last_type = last_event.metadata.get('type')\n",
    "            curr_type = event.metadata.get('type')\n",
    "            \n",
    "            if last_type == curr_type:\n",
    "                # Same type consecutive - keep more prominent\n",
    "                if event.metadata['prominence'] > last_event.metadata['prominence']:\n",
    "                    filtered[-1] = event  # Replace with more prominent\n",
    "                # else: keep the existing one\n",
    "            else:\n",
    "                # Different types - check minimum distance\n",
    "                if event.start - last_event.start >= self.min_distance // 2:\n",
    "                    filtered.append(event)\n",
    "                # else: skip (too close even if different types)\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "\n",
    "class VolatilityRegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect volatility regimes using rolling standard deviation.\n",
    "    \n",
    "    Classifies regimes by quantile thresholds:\n",
    "        - LOW: Below 25th percentile\n",
    "        - NORMAL: 25th-75th percentile\n",
    "        - HIGH: Above 75th percentile\n",
    "        - SPIKE: Above 90th percentile\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect(self, x: torch.Tensor, features: Dict, idx: int) -> List[SimpleSegment]:\n",
    "        \"\"\"\n",
    "        Detect volatility regimes in single sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Single sequence tensor [L]\n",
    "            features: Feature dictionary\n",
    "            idx: Sequence index in batch\n",
    "        \n",
    "        Returns:\n",
    "            List of volatility regime segments\n",
    "        \"\"\"\n",
    "        if 'std_20' not in features:\n",
    "            return []\n",
    "        \n",
    "        vol = features['std_20'][idx].cpu().numpy()\n",
    "        L = len(vol)\n",
    "        \n",
    "        # Compute quantile thresholds\n",
    "        q25, q75, q90 = np.percentile(vol, [25, 75, 90])\n",
    "        \n",
    "        # Classify each point\n",
    "        vol_levels = np.zeros(L, dtype=int)\n",
    "        vol_levels[vol <= q25] = 0  # low\n",
    "        vol_levels[(vol > q25) & (vol <= q75)] = 1  # normal\n",
    "        vol_levels[(vol > q75) & (vol <= q90)] = 2  # high\n",
    "        vol_levels[vol > q90] = 3  # spike\n",
    "        \n",
    "        # Find regime changes\n",
    "        changes = np.where(np.diff(vol_levels) != 0)[0] + 1\n",
    "        breakpoints = np.concatenate([[0], changes, [L]])\n",
    "        \n",
    "        regimes = []\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start = breakpoints[i]\n",
    "            end = breakpoints[i + 1] - 1\n",
    "            \n",
    "            if end - start < 5:  # Skip very short regimes\n",
    "                continue\n",
    "            \n",
    "            level_code = vol_levels[start]\n",
    "            avg_vol = vol[start:end+1].mean()\n",
    "            \n",
    "            # Map to vocabulary\n",
    "            label_map = {\n",
    "                0: VOCAB.LOW_VOLATILITY,\n",
    "                1: VOCAB.NORMAL_VOLATILITY,\n",
    "                2: VOCAB.HIGH_VOLATILITY,\n",
    "                3: VOCAB.VOLATILITY_SPIKE\n",
    "            }\n",
    "            \n",
    "            regimes.append(SimpleSegment(\n",
    "                start=start,\n",
    "                end=end,\n",
    "                label=label_map[level_code],\n",
    "                metadata={'avg_volatility': float(avg_vol)}\n",
    "            ))\n",
    "        \n",
    "        return regimes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: HIERARCHICAL STRUCTURE BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventBuilder:\n",
    "    \"\"\"\n",
    "    Build hierarchical event tree from flat event list.\n",
    "    \n",
    "    Process:\n",
    "        1. Classify each event's scale based on duration\n",
    "        2. Sort events by scale (largest first)\n",
    "        3. Build parent-child relationships via containment\n",
    "        4. Sort children by temporal order\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events: List[HierarchicalEvent] = []\n",
    "    \n",
    "    def add_event(self, start: int, end: int, label: int, event_type: str,\n",
    "                  confidence: float = 1.0, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add event to collection with automatic scale classification.\n",
    "        \n",
    "        Args:\n",
    "            start: Starting timestep\n",
    "            end: Ending timestep\n",
    "            label: Vocabulary ID\n",
    "            event_type: Category string\n",
    "            confidence: Detection confidence score\n",
    "            metadata: Additional information dictionary\n",
    "        \"\"\"\n",
    "        duration = end - start + 1\n",
    "        \n",
    "        # Determine hierarchical scale\n",
    "        if duration <= 5:\n",
    "            scale = EventScale.MICRO\n",
    "        elif duration <= 15:\n",
    "            scale = EventScale.MINI\n",
    "        elif duration <= 50:\n",
    "            scale = EventScale.MESO\n",
    "        elif duration <= 150:\n",
    "            scale = EventScale.MACRO\n",
    "        else:\n",
    "            scale = EventScale.GLOBAL\n",
    "        \n",
    "        event = HierarchicalEvent(\n",
    "            start=start, end=end, label=label,\n",
    "            label_name=VOCAB.id_to_label(label),\n",
    "            scale=scale, event_type=event_type,\n",
    "            confidence=confidence, metadata=metadata or {}\n",
    "        )\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def build_hierarchy(self) -> List[HierarchicalEvent]:\n",
    "        \"\"\"\n",
    "        Build hierarchical tree structure.\n",
    "        \n",
    "        Returns:\n",
    "            List of root events (events with no parent)\n",
    "        \"\"\"\n",
    "        # Sort by scale (largest first), then by start position\n",
    "        sorted_events = sorted(self.events, key=lambda e: (-e.scale, e.start))\n",
    "        roots = []\n",
    "        \n",
    "        # Build parent-child relationships\n",
    "        for event in sorted_events:\n",
    "            parent = self._find_parent(event, sorted_events)\n",
    "            if parent is None:\n",
    "                roots.append(event)\n",
    "            else:\n",
    "                event.parent = parent\n",
    "                parent.children.append(event)\n",
    "        \n",
    "        # Sort children within each parent\n",
    "        self._sort_children(roots)\n",
    "        return roots\n",
    "    \n",
    "    def _find_parent(self, event: HierarchicalEvent,\n",
    "                     all_events: List[HierarchicalEvent]) -> Optional[HierarchicalEvent]:\n",
    "        \"\"\"Find smallest event that contains this event (most specific parent)\"\"\"\n",
    "        candidates = [e for e in all_events if e != event and \n",
    "                     e.scale > event.scale and e.contains(event)]\n",
    "        return min(candidates, key=lambda e: e.duration) if candidates else None\n",
    "    \n",
    "    def _sort_children(self, nodes: List[HierarchicalEvent]):\n",
    "        \"\"\"Recursively sort children by start position\"\"\"\n",
    "        for node in nodes:\n",
    "            if node.children:\n",
    "                node.children.sort(key=lambda c: c.start)\n",
    "                self._sort_children(node.children)\n",
    "    \n",
    "    def get_flat_list(self, roots: List[HierarchicalEvent]) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get flattened list via depth-first traversal\"\"\"\n",
    "        result = []\n",
    "        def traverse(node):\n",
    "            result.append(node)\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in roots:\n",
    "            traverse(root)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: HIERARCHICAL ANNOTATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalAnnotation:\n",
    "    \"\"\"\n",
    "    Complete hierarchical annotation for one sequence.\n",
    "    \n",
    "    Attributes:\n",
    "        sequence: Original time series [L]\n",
    "        step_labels: Step-wise labels [L]\n",
    "        event_roots: Root nodes of hierarchy tree\n",
    "        all_events: Flattened list of all events\n",
    "    \"\"\"\n",
    "    sequence: torch.Tensor\n",
    "    step_labels: torch.Tensor\n",
    "    event_roots: List[HierarchicalEvent]\n",
    "    all_events: List[HierarchicalEvent]\n",
    "    \n",
    "    def print_hierarchy(self, max_depth: int = 10):\n",
    "        \"\"\"Pretty print hierarchical structure\"\"\"\n",
    "        def print_tree(node: HierarchicalEvent, depth: int = 0):\n",
    "            if depth > max_depth:\n",
    "                return\n",
    "            print(node)\n",
    "            for child in node.children:\n",
    "                print_tree(child, depth + 1)\n",
    "        \n",
    "        print(f\"\\nHierarchical Events (Total: {len(self.all_events)})\")\n",
    "        print(\"=\" * 80)\n",
    "        for root in self.event_roots:\n",
    "            print_tree(root)\n",
    "    \n",
    "    def get_events_at_scale(self, scale: EventScale) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events at specific hierarchical scale\"\"\"\n",
    "        return [e for e in self.all_events if e.scale == scale]\n",
    "    \n",
    "    def get_events_in_range(self, start: int, end: int) -> List[HierarchicalEvent]:\n",
    "        \"\"\"Get all events overlapping with time range\"\"\"\n",
    "        return [e for e in self.all_events \n",
    "                if not (e.end < start or e.start > end)]\n",
    "    \n",
    "    def to_text(self, format: str = 'depth_marked') -> str:\n",
    "        \"\"\"\n",
    "        Generate text representation.\n",
    "        \n",
    "        Args:\n",
    "            format: Output format\n",
    "                - 'depth_marked': Depth indicators with events\n",
    "                - 'flat': Simple sequential list\n",
    "                - 'narrative': Natural language description\n",
    "        \n",
    "        Returns:\n",
    "            Text string for language model training\n",
    "        \"\"\"\n",
    "        if format == 'depth_marked':\n",
    "            return self._depth_marked_text()\n",
    "        elif format == 'flat':\n",
    "            return self._flat_text()\n",
    "        elif format == 'narrative':\n",
    "            return self._narrative_text()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "    \n",
    "    def _depth_marked_text(self) -> str:\n",
    "        \"\"\"Depth markers indicate nesting: > >> >>> etc.\"\"\"\n",
    "        parts = []\n",
    "        def traverse(node):\n",
    "            depth_marker = \">\" * node.depth\n",
    "            parts.append(f\"{depth_marker}[{node.start}-{node.end}]{node.label_name}\")\n",
    "            for child in node.children:\n",
    "                traverse(child)\n",
    "        for root in self.event_roots:\n",
    "            traverse(root)\n",
    "        return \" \".join(parts)\n",
    "    \n",
    "    def _flat_text(self) -> str:\n",
    "        \"\"\"Simple sequential list (loses hierarchy)\"\"\"\n",
    "        events = sorted(self.all_events, key=lambda e: e.start)\n",
    "        return \" \".join(f\"[{e.start}-{e.end}]{e.label_name}\" for e in events)\n",
    "    \n",
    "    def _narrative_text(self) -> str:\n",
    "        \"\"\"Natural language with hierarchical context\"\"\"\n",
    "        sentences = []\n",
    "        \n",
    "        # Start with global view\n",
    "        global_events = self.get_events_at_scale(EventScale.GLOBAL)\n",
    "        if global_events:\n",
    "            sentences.append(\n",
    "                f\"Overall: {global_events[0].label_name.lower().replace('_', ' ')}.\"\n",
    "            )\n",
    "        \n",
    "        # Describe macro events\n",
    "        macro_events = self.get_events_at_scale(EventScale.MACRO)\n",
    "        if macro_events:\n",
    "            sentences.append(f\"{len(macro_events)} major segments detected.\")\n",
    "            for event in macro_events[:3]:\n",
    "                desc = event.label_name.lower().replace('_', ' ')\n",
    "                sentences.append(f\"[{event.start}-{event.end}]: {desc}\")\n",
    "                if event.children:\n",
    "                    nested = \", \".join(set(c.event_type for c in event.children))\n",
    "                    sentences.append(f\"  (contains: {nested})\")\n",
    "        \n",
    "        return \" \".join(sentences)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: MAIN DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class HierarchicalEventDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Main dataset class for hierarchical event labeling.\n",
    "    \n",
    "    Processing pipeline:\n",
    "        1. Extract multi-scale features\n",
    "        2. Encode step-wise labels\n",
    "        3. Detect events (trends, peaks, volatility)\n",
    "        4. Add global regime classification\n",
    "        5. Build hierarchical structure\n",
    "        6. Create annotations\n",
    "    \n",
    "    Args:\n",
    "        x: Time series tensor [B, L]\n",
    "        verbose: Print progress messages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x: torch.Tensor, verbose: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(\"Expected x with shape [B, L]\")\n",
    "        \n",
    "        self.x = x\n",
    "        B, L = x.shape\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INITIALIZING HIERARCHICAL EVENT DATASET\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Sequences: {B}\")\n",
    "            print(f\"Length: {L}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_extractor = MultiScaleFeatureExtractor()\n",
    "        self.step_encoder = StepWiseEncoder()\n",
    "        self.trend_detector = TrendSegmentDetector()\n",
    "        self.peak_detector = PeakTroughDetector()\n",
    "        self.vol_detector = VolatilityRegimeDetector()\n",
    "        \n",
    "        # STEP 1: Extract features\n",
    "        if verbose:\n",
    "            print(f\"\\n[1/4] Extracting multi-scale features...\")\n",
    "        self.features = self.feature_extractor.extract_features(x)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Computed {len(self.features)} feature types\")\n",
    "        \n",
    "        # STEP 2: Encode step labels\n",
    "        if verbose:\n",
    "            print(f\"[2/4] Encoding step-wise labels...\")\n",
    "        self.step_labels = self.step_encoder.encode(x, self.features)\n",
    "        if verbose:\n",
    "            print(f\"      ✓ Encoded {B * L} timesteps\")\n",
    "        \n",
    "        # STEP 3: Detect events and build hierarchy\n",
    "        if verbose:\n",
    "            print(f\"[3/4] Detecting events and building hierarchy...\")\n",
    "        \n",
    "        self.annotations = []\n",
    "        for i in range(B):\n",
    "            if verbose and i % 50 == 0:\n",
    "                print(f\"      Processing sequence {i}/{B}...\")\n",
    "            \n",
    "            annotation = self._build_annotation(i, L)\n",
    "            self.annotations.append(annotation)\n",
    "        \n",
    "        # STEP 4: Compute statistics\n",
    "        if verbose:\n",
    "            print(f\"[4/4] Computing statistics...\")\n",
    "            self._print_statistics()\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"✓ DATASET READY\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    def _build_annotation(self, idx: int, L: int) -> HierarchicalAnnotation:\n",
    "        \"\"\"Build complete hierarchical annotation for one sequence\"\"\"\n",
    "        \n",
    "        builder = HierarchicalEventBuilder()\n",
    "        \n",
    "        # Detect all event types\n",
    "        trends = self.trend_detector.detect(self.x[idx], self.features, idx)\n",
    "        peaks = self.peak_detector.detect(self.x[idx], idx)\n",
    "        vol_regimes = self.vol_detector.detect(self.x[idx], self.features, idx)\n",
    "        \n",
    "        # Add trend segments\n",
    "        for seg in trends:\n",
    "            builder.add_event(seg.start, seg.end, seg.label, 'trend',\n",
    "                            confidence=0.9, metadata=seg.metadata)\n",
    "        \n",
    "        # Add peaks/troughs\n",
    "        for pk in peaks:\n",
    "            builder.add_event(pk.start, pk.end, pk.label, 'peak',\n",
    "                            confidence=0.85, metadata=pk.metadata)\n",
    "        \n",
    "        # Add volatility regimes\n",
    "        for vr in vol_regimes:\n",
    "            builder.add_event(vr.start, vr.end, vr.label, 'volatility',\n",
    "                            confidence=0.8, metadata=vr.metadata)\n",
    "        \n",
    "        # Add global regime\n",
    "        global_label = self._classify_global_regime(idx)\n",
    "        builder.add_event(0, L-1, global_label, 'regime', confidence=0.7)\n",
    "        \n",
    "        # Build hierarchy\n",
    "        roots = builder.build_hierarchy()\n",
    "        all_events = builder.get_flat_list(roots)\n",
    "        \n",
    "        return HierarchicalAnnotation(\n",
    "            sequence=self.x[idx],\n",
    "            step_labels=self.step_labels[idx],\n",
    "            event_roots=roots,\n",
    "            all_events=all_events\n",
    "        )\n",
    "    \n",
    "    def _classify_global_regime(self, idx: int) -> int:\n",
    "        \"\"\"Classify overall sequence regime\"\"\"\n",
    "        if 'slope_20' in self.features:\n",
    "            avg_slope = self.features['slope_20'][idx].mean().item()\n",
    "        else:\n",
    "            avg_slope = 0\n",
    "        \n",
    "        if avg_slope > 0.05:\n",
    "            return VOCAB.BULLISH_REGIME\n",
    "        elif avg_slope < -0.05:\n",
    "            return VOCAB.BEARISH_REGIME\n",
    "        else:\n",
    "            return VOCAB.SIDEWAYS_REGIME\n",
    "    \n",
    "    def _print_statistics(self):\n",
    "        \"\"\"Print dataset statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        avg_events = total_events / len(self.annotations)\n",
    "        \n",
    "        # Count by scale\n",
    "        scale_counts = {scale: 0 for scale in EventScale}\n",
    "        for ann in self.annotations:\n",
    "            for event in ann.all_events:\n",
    "                scale_counts[event.scale] += 1\n",
    "        \n",
    "        print(f\"      Total events: {total_events}\")\n",
    "        print(f\"      Avg per sequence: {avg_events:.1f}\")\n",
    "        print(f\"      By scale:\")\n",
    "        for scale in EventScale:\n",
    "            count = scale_counts[scale]\n",
    "            avg = count / len(self.annotations)\n",
    "            print(f\"        {scale.name:.<12} {avg:>6.1f} per sequence\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.annotations[idx]\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get comprehensive statistics\"\"\"\n",
    "        total_events = sum(len(a.all_events) for a in self.annotations)\n",
    "        \n",
    "        return {\n",
    "            'num_sequences': len(self.annotations),\n",
    "            'sequence_length': len(self.annotations[0].sequence),\n",
    "            'vocab_size': VOCAB.get_vocab_size(),\n",
    "            'total_events': total_events,\n",
    "            'avg_events_per_sequence': total_events / len(self.annotations),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: TEXT GENERATION FOR LM TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TextCorpusGenerator:\n",
    "    \"\"\"\n",
    "    Generate training text in various formats.\n",
    "    \n",
    "    Formats:\n",
    "        - depth_marked: Hierarchical with depth indicators (>)\n",
    "        - flat: Simple sequential list\n",
    "        - narrative: Natural language description\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_corpus(dataset: HierarchicalEventDataset, \n",
    "                       format: str = 'depth_marked') -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text corpus for all sequences.\n",
    "        \n",
    "        Args:\n",
    "            dataset: HierarchicalEventDataset instance\n",
    "            format: Text format\n",
    "        \n",
    "        Returns:\n",
    "            List of text strings, one per sequence\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for annotation in dataset.annotations:\n",
    "            text = annotation.to_text(format=format)\n",
    "            corpus.append(text)\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(corpus: List[str]) -> Dict:\n",
    "        \"\"\"Estimate token counts for corpus\"\"\"\n",
    "        total_tokens = sum(len(text.split()) for text in corpus)\n",
    "        total_chars = sum(len(text) for text in corpus)\n",
    "        \n",
    "        return {\n",
    "            'num_documents': len(corpus),\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_chars': total_chars,\n",
    "            'avg_tokens_per_doc': total_tokens / len(corpus),\n",
    "            'avg_chars_per_doc': total_chars / len(corpus),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DEMONSTRATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_data(B: int = 50, L: int = 336, seed: int = 42) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic time series.\n",
    "    \n",
    "    Components:\n",
    "        - Multi-scale sinusoidal trends\n",
    "        - Volatility clusters\n",
    "        - Random spikes\n",
    "        - Local corrections (creates nested events)\n",
    "    \n",
    "    Args:\n",
    "        B: Batch size (number of sequences)\n",
    "        L: Sequence length\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [B, L]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    x = torch.zeros(B, L)\n",
    "    \n",
    "    for i in range(B):\n",
    "        # Base trend (multiple scales)\n",
    "        trend = 0.5 * torch.sin(t / 2) + 0.1 * t\n",
    "        \n",
    "        # Add volatility clusters\n",
    "        vol_modulator = 0.1 + 0.2 * (torch.sin(3 * t) > 0).float()\n",
    "        noise = torch.randn(L) * vol_modulator\n",
    "        \n",
    "        # Add random spikes\n",
    "        num_spikes = np.random.randint(2, 5)\n",
    "        spike_indices = torch.randint(50, L-50, (num_spikes,))\n",
    "        spikes = torch.zeros(L)\n",
    "        spikes[spike_indices] = torch.randn(num_spikes) * 2\n",
    "        \n",
    "        # Add local correction (creates nested structure)\n",
    "        if i % 3 == 0:\n",
    "            # Dip in middle of uptrend\n",
    "            start = L // 2\n",
    "            end = start + 30\n",
    "            x[i, start:end] = x[i, start:end] - 0.5\n",
    "        \n",
    "        x[i] = trend + noise + spikes\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def demonstrate_system():\n",
    "    \"\"\"Run complete demonstration of the system\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL TIME SERIES EVENT LABELING SYSTEM\")\n",
    "    print(\"Demonstration\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    B, L = 20, 336\n",
    "    print(f\"\\nGenerating {B} synthetic sequences of length {L}...\")\n",
    "    x = generate_synthetic_data(B, L)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # Show example annotation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE: HIERARCHICAL STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # Show events by scale\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTS BY HIERARCHICAL SCALE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"\\n{scale.name} ({len(events)} events):\")\n",
    "        for e in events[:5]:\n",
    "            print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name}\")\n",
    "    \n",
    "    # Generate text in different formats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT GENERATION FOR LM TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    for fmt in formats:\n",
    "        text = ann.to_text(format=fmt)\n",
    "        tokens = len(text.split())\n",
    "        chars = len(text)\n",
    "        print(f\"\\n{fmt.upper()}:\")\n",
    "        print(f\"  Tokens: {tokens}, Chars: {chars}\")\n",
    "        print(f\"  Preview: {text[:200]}...\")\n",
    "    \n",
    "    # Generate full corpus\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FULL CORPUS STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    text_gen = TextCorpusGenerator()\n",
    "    corpus = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    stats = text_gen.estimate_tokens(corpus)\n",
    "    \n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value:,.1f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ DEMONSTRATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nThe system is ready for processing real time series data.\")\n",
    "    print(\"Simply create a dataset with: dataset = HierarchicalEventDataset(your_data)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33b855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE WORKFLOW EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "\n",
      "[2/6] Creating hierarchical dataset...\n",
      "  ✓ Processed 100 sequences\n",
      "\n",
      "[3/6] Exploring example annotation...\n",
      "================================================================================\n",
      "ANNOTATION EXPLORATION\n",
      "================================================================================\n",
      "\n",
      "1. HIERARCHICAL TREE:\n",
      "\n",
      "Hierarchical Events (Total: 46)\n",
      "================================================================================\n",
      "[000-335] SIDEWAYS_REGIME (scale=GLOBAL) (30 children)\n",
      "  [000-019] FLAT_SEGMENT (scale=MESO) (1 children)\n",
      "    [003-012] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [021-032] HIGH_VOLATILITY (scale=MINI)\n",
      "  [033-038] UPTREND_SHORT (scale=MINI) (1 children)\n",
      "    [033-033] SHARP_PEAK (scale=MICRO)\n",
      "  [033-038] VOLATILITY_SPIKE (scale=MINI)\n",
      "  [042-082] NORMAL_VOLATILITY (scale=MESO) (2 children)\n",
      "    [046-052] DOWNTREND_SHORT (scale=MINI)\n",
      "    [081-081] SHARP_TROUGH (scale=MICRO)\n",
      "  [092-102] VOLATILITY_SPIKE (scale=MINI) (1 children)\n",
      "    [095-095] SHARP_PEAK (scale=MICRO)\n",
      "  [103-103] SHARP_TROUGH (scale=MICRO)\n",
      "  [112-124] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [125-133] LOW_VOLATILITY (scale=MINI)\n",
      "  [134-140] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [142-155] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [143-148] DOWNTREND_SHORT (scale=MINI)\n",
      "  [157-171] NORMAL_VOLATILITY (scale=MINI) (2 children)\n",
      "    [162-162] SHARP_PEAK (scale=MICRO)\n",
      "    [167-167] SHARP_TROUGH (scale=MICRO)\n",
      "  [174-174] LOCAL_PEAK (scale=MICRO)\n",
      "  [177-189] NORMAL_VOLATILITY (scale=MINI)\n",
      "  [193-209] NORMAL_VOLATILITY (scale=MESO) (1 children)\n",
      "    [193-193] SHARP_TROUGH (scale=MICRO)\n",
      "  [212-212] SHARP_PEAK (scale=MICRO)\n",
      "  [213-220] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [217-217] SHARP_TROUGH (scale=MICRO)\n",
      "  [224-224] SHARP_PEAK (scale=MICRO)\n",
      "  [226-235] LOW_VOLATILITY (scale=MINI)\n",
      "  [236-244] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [237-237] SHARP_TROUGH (scale=MICRO)\n",
      "  [245-245] SHARP_PEAK (scale=MICRO)\n",
      "  [248-253] DOWNTREND_SHORT (scale=MINI)\n",
      "  [248-255] VOLATILITY_SPIKE (scale=MINI)\n",
      "  [256-262] HIGH_VOLATILITY (scale=MINI)\n",
      "  [267-267] SHARP_TROUGH (scale=MICRO)\n",
      "  [268-298] LOW_VOLATILITY (scale=MESO) (3 children)\n",
      "    [277-277] SHARP_PEAK (scale=MICRO)\n",
      "    [285-285] SHARP_TROUGH (scale=MICRO)\n",
      "    [291-291] SHARP_PEAK (scale=MICRO)\n",
      "  [299-305] NORMAL_VOLATILITY (scale=MINI) (1 children)\n",
      "    [305-305] SHARP_TROUGH (scale=MICRO)\n",
      "  [306-328] LOW_VOLATILITY (scale=MESO) (1 children)\n",
      "    [328-328] SHARP_PEAK (scale=MICRO)\n",
      "  [329-335] NORMAL_VOLATILITY (scale=MINI)\n",
      "\n",
      "2. EVENTS BY SCALE:\n",
      "MICRO: 19 events\n",
      "MINI: 21 events\n",
      "MESO: 5 events\n",
      "MACRO: 0 events\n",
      "GLOBAL: 1 events\n",
      "\n",
      "3. EVENTS IN TIME RANGE [100-200]:\n",
      "  [000-335] SIDEWAYS_REGIME (GLOBAL)\n",
      "  [092-102] VOLATILITY_SPIKE (MINI)\n",
      "  [103-103] SHARP_TROUGH (MICRO)\n",
      "  [112-124] NORMAL_VOLATILITY (MINI)\n",
      "  [125-133] LOW_VOLATILITY (MINI)\n",
      "  [134-140] NORMAL_VOLATILITY (MINI)\n",
      "  [142-155] NORMAL_VOLATILITY (MINI)\n",
      "  [143-148] DOWNTREND_SHORT (MINI)\n",
      "  [157-171] NORMAL_VOLATILITY (MINI)\n",
      "  [162-162] SHARP_PEAK (MICRO)\n",
      "  [167-167] SHARP_TROUGH (MICRO)\n",
      "  [174-174] LOCAL_PEAK (MICRO)\n",
      "  [177-189] NORMAL_VOLATILITY (MINI)\n",
      "  [193-209] NORMAL_VOLATILITY (MESO)\n",
      "  [193-193] SHARP_TROUGH (MICRO)\n",
      "\n",
      "4. EVENT METADATA:\n",
      "SIDEWAYS_REGIME: {}\n",
      "FLAT_SEGMENT: {'slope': 0.0}\n",
      "NORMAL_VOLATILITY: {'avg_volatility': 0.9497693777084351}\n",
      "HIGH_VOLATILITY: {'avg_volatility': 1.0931562185287476}\n",
      "UPTREND_SHORT: {'slope': 0.07785416394472122}\n",
      "\n",
      "5. STEP-WISE LABELS (first 20):\n",
      "IDs: [0, 9, 6, 10, 9, 7, 4, 5, 8, 6, 6, 4, 8, 5, 6, 7, 7, 5, 8, 7]\n",
      "Names: ['PAD', 'SPIKE_UP', 'DOWN_SMALL', 'SPIKE_DOWN', 'SPIKE_UP', 'DOWN_MEDIUM', 'UP_MEDIUM', 'UP_LARGE', 'DOWN_LARGE', 'DOWN_SMALL', 'DOWN_SMALL', 'UP_MEDIUM', 'DOWN_LARGE', 'UP_LARGE', 'DOWN_SMALL', 'DOWN_MEDIUM', 'DOWN_MEDIUM', 'UP_LARGE', 'DOWN_LARGE', 'DOWN_MEDIUM']\n",
      "\n",
      "[4/6] Computing statistics...\n",
      "\n",
      "================================================================================\n",
      "DETAILED EVENT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "By Event Type:\n",
      "  peak................   2570 (25.70 per sequence)\n",
      "  regime..............    100 (1.00 per sequence)\n",
      "  trend...............    376 (3.76 per sequence)\n",
      "  volatility..........   1980 (19.80 per sequence)\n",
      "\n",
      "By Scale:\n",
      "  GLOBAL..............    100 (1.00 per sequence)\n",
      "  MESO................    596 (5.96 per sequence)\n",
      "  MICRO...............   2570 (25.70 per sequence)\n",
      "  MINI................   1760 (17.60 per sequence)\n",
      "\n",
      "Top 10 Labels:\n",
      "  SHARP_PEAK....................   1266 (12.66 per sequence)\n",
      "  SHARP_TROUGH..................   1265 (12.65 per sequence)\n",
      "  NORMAL_VOLATILITY.............   1083 (10.83 per sequence)\n",
      "  LOW_VOLATILITY................    470 (4.70 per sequence)\n",
      "  HIGH_VOLATILITY...............    229 (2.29 per sequence)\n",
      "  VOLATILITY_SPIKE..............    198 (1.98 per sequence)\n",
      "  UPTREND_SHORT.................    143 (1.43 per sequence)\n",
      "  DOWNTREND_SHORT...............    133 (1.33 per sequence)\n",
      "  SIDEWAYS_REGIME...............    100 (1.00 per sequence)\n",
      "  FLAT_SEGMENT..................    100 (1.00 per sequence)\n",
      "\n",
      "[5/6] Generating training corpus...\n",
      "\n",
      "Generating depth_marked format...\n",
      "Saved to depth_marked_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 5,026\n",
      "  Avg tokens/doc: 50.3\n",
      "\n",
      "Generating flat format...\n",
      "Saved to flat_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 5,026\n",
      "  Avg tokens/doc: 50.3\n",
      "\n",
      "Generating narrative format...\n",
      "Saved to narrative_output_corpus.txt\n",
      "  Documents: 100\n",
      "  Total tokens: 300\n",
      "  Avg tokens/doc: 3.0\n",
      "\n",
      "[6/6] Creating DataLoader...\n",
      "  ✓ DataLoader ready with 7 batches\n",
      "\n",
      "================================================================================\n",
      "✓ WORKFLOW COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "USAGE GUIDE: Hierarchical Time Series Event Labeling System\n",
    "============================================================\n",
    "\n",
    "Quick Start Guide and Common Use Cases\n",
    "\"\"\"\n",
    "\n",
    "# import torch\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     TextCorpusGenerator,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK START\n",
    "# ============================================================================\n",
    "\n",
    "def quick_start_example():\n",
    "    \"\"\"Minimal example to get started\"\"\"\n",
    "    \n",
    "    # 1. Prepare your data as [B, L] tensor\n",
    "    B, L = 100, 336  # 100 sequences, each 336 timesteps\n",
    "    x = torch.randn(B, L)  # Replace with your real data\n",
    "    \n",
    "    # 2. Create dataset (this does all the processing)\n",
    "    dataset = HierarchicalEventDataset(x, verbose=True)\n",
    "    \n",
    "    # 3. Access annotations\n",
    "    annotation = dataset[0]  # Get first sequence annotation\n",
    "    \n",
    "    # 4. Generate training text\n",
    "    text = annotation.to_text(format='depth_marked')\n",
    "    print(text)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOADING REAL DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_from_numpy():\n",
    "    \"\"\"Load from numpy arrays\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load your numpy data\n",
    "    data = np.load('your_data.npy')  # Shape: [num_samples, sequence_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_from_csv():\n",
    "    \"\"\"Load from CSV files\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "    \n",
    "    # Assuming each row is a sequence\n",
    "    data = df.values  # Shape: [num_sequences, sequence_length]\n",
    "    x = torch.from_numpy(data).float()\n",
    "    \n",
    "    dataset = HierarchicalEventDataset(x)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_eeg_example():\n",
    "    \"\"\"Example for EEG data\"\"\"\n",
    "    \n",
    "    # Assuming you have EEG data: [num_trials, num_channels, time_points]\n",
    "    eeg_data = torch.randn(100, 64, 1000)  # Replace with real data\n",
    "    \n",
    "    # Process each channel separately\n",
    "    datasets = []\n",
    "    for channel in range(64):\n",
    "        channel_data = eeg_data[:, channel, :]  # [num_trials, time_points]\n",
    "        dataset = HierarchicalEventDataset(channel_data, verbose=False)\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORING ANNOTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def explore_annotation(dataset):\n",
    "    \"\"\"Explore annotation structure\"\"\"\n",
    "    \n",
    "    ann = dataset[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION EXPLORATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. View hierarchical structure\n",
    "    print(\"\\n1. HIERARCHICAL TREE:\")\n",
    "    ann.print_hierarchy(max_depth=3)\n",
    "    \n",
    "    # 2. Get events at specific scale\n",
    "    print(\"\\n2. EVENTS BY SCALE:\")\n",
    "    for scale in EventScale:\n",
    "        events = ann.get_events_at_scale(scale)\n",
    "        print(f\"{scale.name}: {len(events)} events\")\n",
    "    \n",
    "    # 3. Get events in time range\n",
    "    print(\"\\n3. EVENTS IN TIME RANGE [100-200]:\")\n",
    "    events = ann.get_events_in_range(100, 200)\n",
    "    for e in events:\n",
    "        print(f\"  [{e.start:03d}-{e.end:03d}] {e.label_name} ({e.scale.name})\")\n",
    "    \n",
    "    # 4. Access event metadata\n",
    "    print(\"\\n4. EVENT METADATA:\")\n",
    "    for event in ann.all_events[:5]:\n",
    "        print(f\"{event.label_name}: {event.metadata}\")\n",
    "    \n",
    "    # 5. Step-wise labels\n",
    "    print(f\"\\n5. STEP-WISE LABELS (first 20):\")\n",
    "    print(f\"IDs: {ann.step_labels[:20].tolist()}\")\n",
    "    print(f\"Names: {[VOCAB.id_to_label(int(l)) for l in ann.step_labels[:20]]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT GENERATION FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_training_corpus(dataset, output_file='training_corpus.txt'):\n",
    "    \"\"\"Generate complete training corpus\"\"\"\n",
    "    \n",
    "    # Generate text for all sequences\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['depth_marked', 'flat', 'narrative']\n",
    "    \n",
    "    for fmt in formats:\n",
    "        print(f\"\\nGenerating {fmt} format...\")\n",
    "        corpus = text_gen.generate_corpus(dataset, format=fmt)\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f'{fmt}_{output_file}'\n",
    "        with open(filename, 'w') as f:\n",
    "            for i, text in enumerate(corpus):\n",
    "                f.write(f\"<sequence_{i}>\\n{text}\\n</sequence_{i}>\\n\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = text_gen.estimate_tokens(corpus)\n",
    "        print(f\"Saved to {filename}\")\n",
    "        print(f\"  Documents: {stats['num_documents']}\")\n",
    "        print(f\"  Total tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"  Avg tokens/doc: {stats['avg_tokens_per_doc']:.1f}\")\n",
    "\n",
    "\n",
    "def create_autoregressive_pairs(dataset):\n",
    "    \"\"\"Create input-output pairs for autoregressive LM training\"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for ann in dataset.annotations:\n",
    "        # Get hierarchical text\n",
    "        full_text = ann.to_text(format='depth_marked')\n",
    "        tokens = full_text.split()\n",
    "        \n",
    "        # Create prefix-completion pairs\n",
    "        for i in range(1, len(tokens)):\n",
    "            input_text = \" \".join(tokens[:i])\n",
    "            target_token = tokens[i]\n",
    "            pairs.append((input_text, target_token))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FILTERING AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def filter_by_event_type(dataset, event_type='trend'):\n",
    "    \"\"\"Filter sequences by event type\"\"\"\n",
    "    \n",
    "    filtered = []\n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        # Check if this annotation contains the event type\n",
    "        has_event = any(e.event_type == event_type for e in ann.all_events)\n",
    "        if has_event:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    print(f\"Found {len(filtered)} sequences with {event_type} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def analyze_event_statistics(dataset):\n",
    "    \"\"\"Compute detailed event statistics\"\"\"\n",
    "    \n",
    "    stats = {\n",
    "        'total_sequences': len(dataset),\n",
    "        'events_by_type': {},\n",
    "        'events_by_scale': {},\n",
    "        'events_by_label': {},\n",
    "    }\n",
    "    \n",
    "    # Count events\n",
    "    for ann in dataset.annotations:\n",
    "        for event in ann.all_events:\n",
    "            # By type\n",
    "            stats['events_by_type'][event.event_type] = \\\n",
    "                stats['events_by_type'].get(event.event_type, 0) + 1\n",
    "            \n",
    "            # By scale\n",
    "            stats['events_by_scale'][event.scale.name] = \\\n",
    "                stats['events_by_scale'].get(event.scale.name, 0) + 1\n",
    "            \n",
    "            # By label\n",
    "            stats['events_by_label'][event.label_name] = \\\n",
    "                stats['events_by_label'].get(event.label_name, 0) + 1\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED EVENT STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nBy Event Type:\")\n",
    "    for event_type, count in sorted(stats['events_by_type'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {event_type:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nBy Scale:\")\n",
    "    for scale, count in sorted(stats['events_by_scale'].items()):\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {scale:.<20} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    print(\"\\nTop 10 Labels:\")\n",
    "    top_labels = sorted(stats['events_by_label'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for label, count in top_labels:\n",
    "        avg = count / stats['total_sequences']\n",
    "        print(f\"  {label:.<30} {count:>6} ({avg:.2f} per sequence)\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM TEXT FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "def create_custom_format(ann):\n",
    "    \"\"\"Create your own text format\"\"\"\n",
    "    \n",
    "    parts = []\n",
    "    \n",
    "    # Add sequence metadata\n",
    "    parts.append(f\"SEQ_LEN:{len(ann.sequence)}\")\n",
    "    \n",
    "    # Add global regime\n",
    "    global_events = ann.get_events_at_scale(EventScale.GLOBAL)\n",
    "    if global_events:\n",
    "        parts.append(f\"REGIME:{global_events[0].label_name}\")\n",
    "    \n",
    "    # Add macro trends\n",
    "    macro_events = ann.get_events_at_scale(EventScale.MACRO)\n",
    "    parts.append(f\"TRENDS:{len(macro_events)}\")\n",
    "    for event in macro_events:\n",
    "        parts.append(f\"T[{event.start}-{event.end}]:{event.label_name}\")\n",
    "    \n",
    "    # Add peaks\n",
    "    peaks = [e for e in ann.all_events if e.event_type == 'peak']\n",
    "    parts.append(f\"PEAKS:{len(peaks)}\")\n",
    "    for peak in peaks:\n",
    "        parts.append(f\"P[{peak.start}]:{peak.label_name}\")\n",
    "    \n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_large_dataset_in_batches(data_generator, batch_size=1000):\n",
    "    \"\"\"Process very large datasets in batches\"\"\"\n",
    "    \n",
    "    all_annotations = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(data_generator):\n",
    "        print(f\"\\nProcessing batch {batch_idx}...\")\n",
    "        \n",
    "        # Create dataset for this batch\n",
    "        dataset = HierarchicalEventDataset(batch_data, verbose=False)\n",
    "        \n",
    "        # Collect annotations\n",
    "        all_annotations.extend(dataset.annotations)\n",
    "        \n",
    "        # Optionally save intermediate results\n",
    "        torch.save(dataset.annotations, f'annotations_batch_{batch_idx}.pt')\n",
    "    \n",
    "    print(f\"\\nTotal annotations: {len(all_annotations)}\")\n",
    "    return all_annotations\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH TRAINING PIPELINES\n",
    "# ============================================================================\n",
    "\n",
    "def create_pytorch_dataloader(dataset, batch_size=32):\n",
    "    \"\"\"Create PyTorch DataLoader for training\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function for hierarchical annotations\"\"\"\n",
    "        sequences = torch.stack([ann.sequence for ann in batch])\n",
    "        step_labels = torch.stack([ann.step_labels for ann in batch])\n",
    "        \n",
    "        # Generate text representations\n",
    "        texts = [ann.to_text(format='depth_marked') for ann in batch]\n",
    "        \n",
    "        return {\n",
    "            'sequences': sequences,\n",
    "            'step_labels': step_labels,\n",
    "            'texts': texts,\n",
    "            'annotations': batch  # Keep full annotations if needed\n",
    "        }\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def prepare_for_huggingface(dataset, tokenizer):\n",
    "    \"\"\"Prepare data for HuggingFace transformers\"\"\"\n",
    "    \n",
    "    # Generate text corpus\n",
    "    text_gen = TextCorpusGenerator()\n",
    "    texts = text_gen.generate_corpus(dataset, format='depth_marked')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE WORKFLOWS\n",
    "# ============================================================================\n",
    "\n",
    "def complete_workflow_example():\n",
    "    \"\"\"Complete end-to-end workflow\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE WORKFLOW EXAMPLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Generate/Load data\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    B, L = 100, 336\n",
    "    x = torch.randn(B, L)  # Replace with real data\n",
    "    \n",
    "    # 2. Create dataset\n",
    "    print(\"\\n[2/6] Creating hierarchical dataset...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    print(f\"  ✓ Processed {len(dataset)} sequences\")\n",
    "    \n",
    "    # 3. Explore one example\n",
    "    print(\"\\n[3/6] Exploring example annotation...\")\n",
    "    explore_annotation(dataset)\n",
    "    \n",
    "    # 4. Analyze statistics\n",
    "    print(\"\\n[4/6] Computing statistics...\")\n",
    "    stats = analyze_event_statistics(dataset)\n",
    "    \n",
    "    # 5. Generate training corpus\n",
    "    print(\"\\n[5/6] Generating training corpus...\")\n",
    "    generate_training_corpus(dataset, 'output_corpus.txt')\n",
    "    \n",
    "    # 6. Create DataLoader\n",
    "    print(\"\\n[6/6] Creating DataLoader...\")\n",
    "    dataloader = create_pytorch_dataloader(dataset, batch_size=16)\n",
    "    print(f\"  ✓ DataLoader ready with {len(dataloader)} batches\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ WORKFLOW COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete workflow\n",
    "    complete_workflow_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df39066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PEAK DETECTION FIX VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Test signals:\n",
      "  1. Clean sinusoid (should have ~4 peaks, ~4 troughs)\n",
      "  2. Noisy sinusoid (should have similar, filtered)\n",
      "\n",
      "Processing...\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "\n",
      "Clean Signal:\n",
      "----------------------------------------\n",
      "  Peaks detected: 2\n",
      "  Troughs detected: 2\n",
      "  Peak positions: [25, 124]\n",
      "  Min distance between peaks: 99\n",
      "  Trough positions: [75, 174]\n",
      "  Min distance between troughs: 99\n",
      "  Peaks/troughs alternate: True ✓\n",
      "  Sequence: peak -> trough -> peak -> trough\n",
      "\n",
      "  Detailed event list:\n",
      "    [025] SHARP_PEAK (peak, prom=1.000)\n",
      "    [075] SHARP_TROUGH (trough, prom=1.999)\n",
      "    [124] SHARP_PEAK (peak, prom=1.999)\n",
      "    [174] SHARP_TROUGH (trough, prom=1.000)\n",
      "\n",
      "Noisy Signal:\n",
      "----------------------------------------\n",
      "  Peaks detected: 5\n",
      "  Troughs detected: 5\n",
      "  Peak positions: [18, 83, 116, 126, 198]\n",
      "  Min distance between peaks: 10\n",
      "  Trough positions: [1, 60, 95, 121, 176]\n",
      "  Min distance between troughs: 26\n",
      "  Peaks/troughs alternate: True ✓\n",
      "  Sequence: trough -> peak -> trough -> peak -> trough -> peak -> trough -> peak -> trough -> peak\n",
      "\n",
      "  Detailed event list:\n",
      "    [001] LOCAL_TROUGH (trough, prom=0.257)\n",
      "    [018] SHARP_PEAK (peak, prom=1.217)\n",
      "    [060] LOCAL_TROUGH (trough, prom=0.218)\n",
      "    [083] LOCAL_PEAK (peak, prom=0.213)\n",
      "    [095] LOCAL_TROUGH (trough, prom=0.149)\n",
      "    [116] LOCAL_PEAK (peak, prom=0.236)\n",
      "    [121] LOCAL_TROUGH (trough, prom=0.183)\n",
      "    [126] SHARP_PEAK (peak, prom=2.249)\n",
      "    [176] SHARP_TROUGH (trough, prom=1.123)\n",
      "    [198] LOCAL_PEAK (peak, prom=0.214)\n",
      "\n",
      "================================================================================\n",
      "VALIDATION\n",
      "================================================================================\n",
      "Clean - Alternation: ✓ PASS\n",
      "Clean - Min distance (49): ✓ PASS\n",
      "Clean - Reasonable count (4): ✓ PASS\n",
      "Noisy - Alternation: ✓ PASS\n",
      "Noisy - Min distance (5): ✓ PASS\n",
      "Noisy - Reasonable count (10): ✓ PASS\n",
      "\n",
      "================================================================================\n",
      "✓ ALL TESTS PASSED - Peak detection is working correctly!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEFORE vs AFTER COMPARISON\n",
      "================================================================================\n",
      "\n",
      "BEFORE (Bug):\n",
      "-------------\n",
      "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
      "[070] SHARP_TROUGH    <-- Trough at timestep 70 (only 4 steps away!)\n",
      "[071] SHARP_PEAK      <-- Peak at timestep 71 (1 step away from trough!)\n",
      "[072] LOCAL_TROUGH    <-- Trough at timestep 72 (1 step away from peak!)\n",
      "\n",
      "Problems:\n",
      "  ✗ Consecutive peaks/troughs (no alternation)\n",
      "  ✗ Too close together (1-4 timesteps)\n",
      "  ✗ Likely just high-frequency noise\n",
      "\n",
      "AFTER (Fixed):\n",
      "--------------\n",
      "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
      "[089] SHARP_TROUGH    <-- Trough at timestep 89 (23 steps away) ✓\n",
      "[112] LOCAL_PEAK      <-- Peak at timestep 112 (23 steps away) ✓\n",
      "[135] LOCAL_TROUGH    <-- Trough at timestep 135 (23 steps away) ✓\n",
      "\n",
      "Improvements:\n",
      "  ✓ Proper alternation (peak -> trough -> peak -> trough)\n",
      "  ✓ Minimum distance enforced (default: 10 timesteps)\n",
      "  ✓ Filters out high-frequency noise\n",
      "  ✓ Adaptive prominence thresholds\n",
      "\n",
      "Key Changes in Code:\n",
      "--------------------\n",
      "1. Added min_distance parameter to find_peaks:\n",
      "   peaks, props = scipy_signal.find_peaks(\n",
      "       x_np, \n",
      "       prominence=min_prominence,\n",
      "       distance=self.min_distance,  # NEW: Enforce separation\n",
      "   )\n",
      "\n",
      "2. Adaptive prominence threshold:\n",
      "   std = np.std(x_np)\n",
      "   min_prominence = max(0.2 * std, 0.1)  # At least 20% of signal std\n",
      "\n",
      "3. Alternation validation:\n",
      "   - After detecting all peaks/troughs\n",
      "   - Remove consecutive same-type events\n",
      "   - Keep more prominent one if conflict\n",
      "\n",
      "4. Type tracking in metadata:\n",
      "   metadata={'prominence': float(prom), 'type': 'peak'}\n",
      "   # Used to enforce peak -> trough -> peak pattern\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Tests: PASSED ✓\n",
      "The peak detection now properly:\n",
      "  1. Enforces minimum distance between events\n",
      "  2. Ensures peaks and troughs alternate\n",
      "  3. Uses adaptive prominence thresholds\n",
      "  4. Filters out high-frequency noise\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Peak Detection Fix\n",
    "========================\n",
    "\n",
    "This script verifies that peaks and troughs:\n",
    "1. Are properly separated (minimum distance)\n",
    "2. Alternate correctly (peak -> trough -> peak)\n",
    "3. Have reasonable prominence\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Import from the fixed version\n",
    "# import sys\n",
    "# sys.path.insert(0, '/home/claude')\n",
    "# from hierarchical_event_labeling import (\n",
    "#     HierarchicalEventDataset,\n",
    "#     EventScale,\n",
    "#     VOCAB\n",
    "# )\n",
    "\n",
    "\n",
    "def test_peak_detection():\n",
    "    \"\"\"Test that peak detection is working correctly\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PEAK DETECTION FIX VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a simple test signal with known peaks/troughs\n",
    "    L = 200\n",
    "    t = torch.linspace(0, 4*np.pi, L)\n",
    "    \n",
    "    # Clean sinusoidal signal (known peaks/troughs)\n",
    "    clean_signal = torch.sin(t)\n",
    "    \n",
    "    # Add some noise\n",
    "    noisy_signal = clean_signal + 0.1 * torch.randn(L)\n",
    "    \n",
    "    # Stack into batch\n",
    "    x = torch.stack([clean_signal, noisy_signal]).unsqueeze(0)  # [1, 2, L]\n",
    "    x = x.reshape(-1, L)  # [2, L]\n",
    "    \n",
    "    print(f\"\\nTest signals:\")\n",
    "    print(f\"  1. Clean sinusoid (should have ~4 peaks, ~4 troughs)\")\n",
    "    print(f\"  2. Noisy sinusoid (should have similar, filtered)\")\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"\\nProcessing...\")\n",
    "    dataset = HierarchicalEventDataset(x, verbose=False)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        signal_type = \"Clean\" if i == 0 else \"Noisy\"\n",
    "        print(f\"\\n{signal_type} Signal:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get peaks and troughs\n",
    "        peaks = [e for e in ann.all_events if 'peak' in e.metadata.get('type', '')]\n",
    "        troughs = [e for e in ann.all_events if 'trough' in e.metadata.get('type', '')]\n",
    "        \n",
    "        print(f\"  Peaks detected: {len(peaks)}\")\n",
    "        print(f\"  Troughs detected: {len(troughs)}\")\n",
    "        \n",
    "        # Show peak positions\n",
    "        if peaks:\n",
    "            peak_positions = [p.start for p in peaks]\n",
    "            print(f\"  Peak positions: {peak_positions}\")\n",
    "            \n",
    "            # Check minimum distance\n",
    "            if len(peak_positions) > 1:\n",
    "                distances = [peak_positions[i+1] - peak_positions[i] \n",
    "                           for i in range(len(peak_positions)-1)]\n",
    "                min_dist = min(distances) if distances else 0\n",
    "                print(f\"  Min distance between peaks: {min_dist}\")\n",
    "        \n",
    "        # Show trough positions\n",
    "        if troughs:\n",
    "            trough_positions = [t.start for t in troughs]\n",
    "            print(f\"  Trough positions: {trough_positions}\")\n",
    "            \n",
    "            if len(trough_positions) > 1:\n",
    "                distances = [trough_positions[i+1] - trough_positions[i] \n",
    "                           for i in range(len(trough_positions)-1)]\n",
    "                min_dist = min(distances) if distances else 0\n",
    "                print(f\"  Min distance between troughs: {min_dist}\")\n",
    "        \n",
    "        # Check alternation\n",
    "        all_extrema = sorted(peaks + troughs, key=lambda e: e.start)\n",
    "        if len(all_extrema) > 1:\n",
    "            types = [e.metadata.get('type') for e in all_extrema]\n",
    "            alternates = all([types[i] != types[i+1] for i in range(len(types)-1)])\n",
    "            print(f\"  Peaks/troughs alternate: {alternates} ✓\" if alternates \n",
    "                  else f\"  Peaks/troughs alternate: {alternates} ✗\")\n",
    "            \n",
    "            # Show sequence\n",
    "            print(f\"  Sequence: {' -> '.join(types[:10])}\")\n",
    "        \n",
    "        # Check consecutive issues\n",
    "        print(\"\\n  Detailed event list:\")\n",
    "        for j, e in enumerate(all_extrema[:15]):  # Show first 15\n",
    "            etype = e.metadata.get('type')\n",
    "            prom = e.metadata.get('prominence', 0)\n",
    "            print(f\"    [{e.start:03d}] {e.label_name} ({etype}, prom={prom:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation checks\n",
    "    all_passed = True\n",
    "    \n",
    "    for i, ann in enumerate(dataset.annotations):\n",
    "        signal_type = \"Clean\" if i == 0 else \"Noisy\"\n",
    "        \n",
    "        # Get all extrema\n",
    "        peaks = [e for e in ann.all_events if 'peak' in e.metadata.get('type', '')]\n",
    "        troughs = [e for e in ann.all_events if 'trough' in e.metadata.get('type', '')]\n",
    "        all_extrema = sorted(peaks + troughs, key=lambda e: e.start)\n",
    "        \n",
    "        # Check 1: Alternation\n",
    "        if len(all_extrema) > 1:\n",
    "            types = [e.metadata.get('type') for e in all_extrema]\n",
    "            alternates = all([types[i] != types[i+1] for i in range(len(types)-1)])\n",
    "            \n",
    "            status = \"✓ PASS\" if alternates else \"✗ FAIL\"\n",
    "            print(f\"{signal_type} - Alternation: {status}\")\n",
    "            all_passed = all_passed and alternates\n",
    "        \n",
    "        # Check 2: Minimum distance\n",
    "        if len(all_extrema) > 1:\n",
    "            positions = [e.start for e in all_extrema]\n",
    "            min_dist = min([positions[i+1] - positions[i] for i in range(len(positions)-1)])\n",
    "            \n",
    "            passed = min_dist >= 5  # At least 5 timesteps apart\n",
    "            status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "            print(f\"{signal_type} - Min distance ({min_dist}): {status}\")\n",
    "            all_passed = all_passed and passed\n",
    "        \n",
    "        # Check 3: Reasonable count\n",
    "        total = len(all_extrema)\n",
    "        reasonable = 4 <= total <= 20  # Should have 4-20 extrema for our test signal\n",
    "        \n",
    "        status = \"✓ PASS\" if reasonable else \"✗ FAIL\"\n",
    "        print(f\"{signal_type} - Reasonable count ({total}): {status}\")\n",
    "        all_passed = all_passed and reasonable\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if all_passed:\n",
    "        print(\"✓ ALL TESTS PASSED - Peak detection is working correctly!\")\n",
    "    else:\n",
    "        print(\"✗ SOME TESTS FAILED - See details above\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def show_before_after_comparison():\n",
    "    \"\"\"Show what the problem was and how it's fixed\"\"\"\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BEFORE vs AFTER COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "BEFORE (Bug):\n",
    "-------------\n",
    "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
    "[070] SHARP_TROUGH    <-- Trough at timestep 70 (only 4 steps away!)\n",
    "[071] SHARP_PEAK      <-- Peak at timestep 71 (1 step away from trough!)\n",
    "[072] LOCAL_TROUGH    <-- Trough at timestep 72 (1 step away from peak!)\n",
    "\n",
    "Problems:\n",
    "  ✗ Consecutive peaks/troughs (no alternation)\n",
    "  ✗ Too close together (1-4 timesteps)\n",
    "  ✗ Likely just high-frequency noise\n",
    "\n",
    "AFTER (Fixed):\n",
    "--------------\n",
    "[066] SHARP_PEAK      <-- Peak at timestep 66\n",
    "[089] SHARP_TROUGH    <-- Trough at timestep 89 (23 steps away) ✓\n",
    "[112] LOCAL_PEAK      <-- Peak at timestep 112 (23 steps away) ✓\n",
    "[135] LOCAL_TROUGH    <-- Trough at timestep 135 (23 steps away) ✓\n",
    "\n",
    "Improvements:\n",
    "  ✓ Proper alternation (peak -> trough -> peak -> trough)\n",
    "  ✓ Minimum distance enforced (default: 10 timesteps)\n",
    "  ✓ Filters out high-frequency noise\n",
    "  ✓ Adaptive prominence thresholds\n",
    "\n",
    "Key Changes in Code:\n",
    "--------------------\n",
    "1. Added min_distance parameter to find_peaks:\n",
    "   peaks, props = scipy_signal.find_peaks(\n",
    "       x_np, \n",
    "       prominence=min_prominence,\n",
    "       distance=self.min_distance,  # NEW: Enforce separation\n",
    "   )\n",
    "\n",
    "2. Adaptive prominence threshold:\n",
    "   std = np.std(x_np)\n",
    "   min_prominence = max(0.2 * std, 0.1)  # At least 20% of signal std\n",
    "\n",
    "3. Alternation validation:\n",
    "   - After detecting all peaks/troughs\n",
    "   - Remove consecutive same-type events\n",
    "   - Keep more prominent one if conflict\n",
    "\n",
    "4. Type tracking in metadata:\n",
    "   metadata={'prominence': float(prom), 'type': 'peak'}\n",
    "   # Used to enforce peak -> trough -> peak pattern\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run tests\n",
    "    passed = test_peak_detection()\n",
    "    \n",
    "    # Show explanation\n",
    "    show_before_after_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Tests: {'PASSED ✓' if passed else 'FAILED ✗'}\")\n",
    "    print(\"The peak detection now properly:\")\n",
    "    print(\"  1. Enforces minimum distance between events\")\n",
    "    print(\"  2. Ensures peaks and troughs alternate\")\n",
    "    print(\"  3. Uses adaptive prominence thresholds\")\n",
    "    print(\"  4. Filters out high-frequency noise\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1ea89fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 1\n",
      "Length: 561\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 561 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/1...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 87\n",
      "      Avg per sequence: 87.0\n",
      "      By scale:\n",
      "        MICRO.......   28.0 per sequence\n",
      "        MINI........   43.0 per sequence\n",
      "        MESO........   13.0 per sequence\n",
      "        MACRO.......    2.0 per sequence\n",
      "        GLOBAL......    1.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "Total tokens: 440\n"
     ]
    }
   ],
   "source": [
    "from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "\n",
    "# Your data\n",
    "dataset = HierarchicalEventDataset(har_tensor)\n",
    "\n",
    "# Generate Qwen-optimized corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset)\n",
    "\n",
    "# Save for training\n",
    "generator.save_corpus(corpus, 'qwen_training.txt')\n",
    "\n",
    "# Get statistics\n",
    "stats = generator.estimate_tokens(corpus)\n",
    "print(f\"Total tokens: {stats['estimated_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78f2393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sequence length=1000 events=134> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-17] low volatility period, [4] sharp deep trough, [18-23] normal volatility, [25-31] normal volatility, [28] sharp prominent peak, [32-55] sudden volatility spike, [49] sharp deep trough, [63-69] normal volatility, [65] sharp prominent peak, [71] sharp deep trough, [72-83] normal volatility, [77] sharp prominent peak, [88] sharp deep trough, [92-107] normal volatility, [108-118] low volatility period, [112] sharp prominent peak, [118-124] short upward trend, [119-124] normal volatility, [125] sharp deep trough, [134-146] normal volatility, [140] sharp prominent peak, [147] sharp deep trough, [154-159] sudden volatility spike, [163] sharp prominent peak, [168-182] normal volatility, [170-176] short downward trend, [183-193] low volatility period, [194-201] normal volatility, [205-212] normal volatility, [214] sharp deep trough, [228] sharp prominent peak, [234-257] low volatility period, [252] sharp deep trough, [258-270] normal volatility, [271] sharp prominent peak, [272-284] normal volatility, [284] sharp deep trough, [293-299] high volatility period, [297] sharp prominent peak, [304-309] low volatility period, [308] sharp deep trough, [316-324] normal volatility, [316] sharp prominent peak, [325] sharp deep trough, [330-339] low volatility period, [340-346] normal volatility, [343] sharp prominent peak, [347-352] high volatility period, [353-362] normal volatility, [363-382] low volatility period, [371] sharp deep trough, [386] sharp prominent peak, [393-402] normal volatility, [403-411] low volatility period, [408] sharp deep trough, [412-417] normal volatility, [429-438] normal volatility, [431] sharp prominent peak, [439-449] low volatility period, [450] sharp deep trough, [451-466] low volatility period, [467-487] normal volatility, [468] sharp prominent peak, [481] sharp deep trough, [488-510] low volatility period, [511-516] normal volatility, [514] sharp prominent peak, [523-536] normal volatility, [527] sharp deep trough, [532] sharp prominent peak, [541] sharp deep trough, [549] sharp prominent peak, [566-573] normal volatility, [566] sharp deep trough, [576-594] normal volatility, [592] sharp prominent peak, [602-607] normal volatility, [608] sharp deep trough, [615-621] sudden volatility spike, [627] sharp prominent peak, [632-646] normal volatility, [648-664] normal volatility, [648] sharp deep trough, [665] sharp prominent peak, [666-671] sudden volatility spike, [674-679] high volatility period, [689-695] normal volatility, [689] sharp deep trough, [694] sharp prominent peak, [700] sharp deep trough, [701-707] high volatility period, [708] sharp prominent peak, [709-727] normal volatility, [717] sharp deep trough, [737-744] low volatility period, [741] sharp prominent peak, [745-750] normal volatility, [752-764] normal volatility, [760] sharp deep trough, [764-769] short upward trend, [765-779] low volatility period, [780-785] normal volatility, [786-799] sudden volatility spike, [786] sharp prominent peak, [808] sharp deep trough, [811-817] normal volatility, [815] sharp prominent peak, [818-846] low volatility period, [828] sharp deep trough, [840] sharp prominent peak, [847-864] normal volatility, [848] sharp deep trough, [853-860] short downward trend, [864] sharp prominent peak, [870] sharp deep trough, [875-881] high volatility period, [882] sharp prominent peak, [894-900] high volatility period, [894] sharp deep trough, [896-903] short upward trend, [901] sharp prominent peak, [905-910] high volatility period, [911] sharp deep trough, [914-920] normal volatility, [916-922] short downward trend, [924-930] normal volatility, [936-952] normal volatility, [951] sharp prominent peak, [956-968] normal volatility, [969] sharp deep trough, [971-984] normal volatility, [986] sharp prominent peak',\n",
       " '<sequence length=1000 events=134> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-14] low volatility period, [3] local trough, [8] sharp prominent peak, [13] sharp deep trough, [18] sharp prominent peak, [19-27] high volatility period, [34] sharp deep trough, [38-46] normal volatility, [41] sharp prominent peak, [47] sharp deep trough, [50-57] normal volatility, [55] sharp prominent peak, [60] local trough, [63-69] low volatility period, [70-94] normal volatility, [81] sharp prominent peak, [89] sharp deep trough, [99-104] short downward trend, [99-109] normal volatility, [110-118] high volatility period, [119] sharp prominent peak, [123-128] high volatility period, [131-138] high volatility period, [143] sharp deep trough, [149] sharp prominent peak, [155] sharp deep trough, [160] sharp prominent peak, [163-176] normal volatility, [177] sharp deep trough, [180-192] normal volatility, [196] sharp prominent peak, [204-211] high volatility period, [211] local trough, [221] sharp prominent peak, [223-231] normal volatility, [225-231] short downward trend, [233-255] normal volatility, [245] sharp deep trough, [253] sharp prominent peak, [256-263] high volatility period, [264-269] sudden volatility spike, [265] sharp deep trough, [271-276] short upward trend, [271-277] sudden volatility spike, [271] sharp prominent peak, [279-290] sudden volatility spike, [283] sharp deep trough, [290-296] short downward trend, [291-310] normal volatility, [308] sharp prominent peak, [311-316] high volatility period, [315-320] short downward trend, [317-322] normal volatility, [330-337] normal volatility, [332] sharp deep trough, [360-369] normal volatility, [366] sharp prominent peak, [374-384] high volatility period, [395-414] normal volatility, [395] sharp deep trough, [414] sharp prominent peak, [415-451] low volatility period, [423] sharp deep trough, [433] sharp prominent peak, [442] sharp deep trough, [456-463] normal volatility, [459] sharp prominent peak, [464-502] low volatility period, [490] sharp deep trough, [495] sharp prominent peak, [502] sharp deep trough, [503-509] normal volatility, [512-521] normal volatility, [512] sharp prominent peak, [522-559] low volatility period, [527] sharp deep trough, [543] sharp prominent peak, [548] sharp deep trough, [560-566] normal volatility, [566] sharp prominent peak, [567-579] sudden volatility spike, [579] local trough, [590-605] normal volatility, [603] sharp prominent peak, [611] sharp deep trough, [613-622] high volatility period, [616] sharp prominent peak, [622] sharp deep trough, [628] sharp prominent peak, [631-637] normal volatility, [638] sharp deep trough, [640-646] short upward trend, [648-657] high volatility period, [648] sharp prominent peak, [663] sharp deep trough, [678] sharp prominent peak, [694-705] normal volatility, [699] sharp deep trough, [705] sharp prominent peak, [706-741] low volatility period, [711] sharp deep trough, [719] sharp prominent peak, [736] sharp deep trough, [742] sharp prominent peak, [745-763] normal volatility, [747] sharp deep trough, [762] sharp prominent peak, [767-774] normal volatility, [775-795] sudden volatility spike, [776] sharp deep trough, [796-806] normal volatility, [809] sharp prominent peak, [814] sharp deep trough, [818-848] normal volatility, [829] sharp prominent peak, [839] sharp deep trough, [844] sharp prominent peak, [850] sharp deep trough, [852-864] low volatility period, [865-884] normal volatility, [865] sharp prominent peak, [880] sharp deep trough, [885-894] low volatility period, [895-903] normal volatility, [905-974] normal volatility, [945] sharp prominent peak, [956] sharp deep trough, [975-987] low volatility period, [981] sharp prominent peak, [988-999] normal volatility, [988] sharp deep trough, [996] sharp prominent peak',\n",
       " '<sequence length=1000 events=147> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-18] low volatility period, [5] sharp deep trough, [10] sharp prominent peak, [17] sharp deep trough, [19-34] normal volatility, [26] sharp prominent peak, [35] sharp deep trough, [37-54] normal volatility, [61-78] normal volatility, [62] sharp prominent peak, [79-89] low volatility period, [83] sharp deep trough, [90-101] normal volatility, [90] sharp prominent peak, [102] sharp deep trough, [107] sharp prominent peak, [114] sharp deep trough, [117-123] short upward trend, [119] sharp prominent peak, [122-133] normal volatility, [124-130] short downward trend, [135] sharp deep trough, [149] sharp prominent peak, [158] sharp deep trough, [162-168] high volatility period, [166-171] short downward trend, [176-188] high volatility period, [176] sharp prominent peak, [189-208] sudden volatility spike, [189] sharp deep trough, [200-206] short upward trend, [201] sharp prominent peak, [210-215] normal volatility, [216-237] low volatility period, [220-225] short downward trend, [224] sharp deep trough, [231] sharp prominent peak, [240-257] normal volatility, [243] sharp deep trough, [248] sharp prominent peak, [258-313] low volatility period, [261] sharp deep trough, [272] sharp prominent peak, [282] sharp deep trough, [314-329] normal volatility, [314] sharp prominent peak, [331] sharp deep trough, [334-340] normal volatility, [354] sharp prominent peak, [359-366] normal volatility, [367-373] high volatility period, [374-380] normal volatility, [385-393] normal volatility, [392] sharp deep trough, [398] sharp prominent peak, [399-406] high volatility period, [408] sharp deep trough, [409-418] normal volatility, [420-432] normal volatility, [433-439] high volatility period, [433] sharp prominent peak, [444] sharp deep trough, [458] sharp prominent peak, [462-483] normal volatility, [472] sharp deep trough, [488] sharp prominent peak, [493-499] short upward trend, [500] sharp deep trough, [512-518] short downward trend, [512-521] normal volatility, [521] sharp prominent peak, [524-541] normal volatility, [526] sharp deep trough, [532] sharp prominent peak, [537] sharp deep trough, [545-551] normal volatility, [549] sharp prominent peak, [552-566] low volatility period, [555-560] short upward trend, [561] sharp deep trough, [569-575] low volatility period, [577] sharp prominent peak, [583-596] sudden volatility spike, [600] sharp deep trough, [606-611] short upward trend, [606] sharp prominent peak, [610-632] normal volatility, [619] sharp deep trough, [636-652] sudden volatility spike, [636] sharp prominent peak, [661] sharp deep trough, [666] sharp prominent peak, [670-691] normal volatility, [671] sharp deep trough, [672-680] short upward trend, [683] sharp prominent peak, [694-705] low volatility period, [701] sharp deep trough, [709-719] normal volatility, [709] sharp prominent peak, [715] sharp deep trough, [720] sharp prominent peak, [724-733] normal volatility, [728] sharp deep trough, [739] sharp prominent peak, [741-758] normal volatility, [746] sharp deep trough, [759-770] low volatility period, [759] sharp prominent peak, [767] sharp deep trough, [771-799] normal volatility, [774] sharp prominent peak, [786] sharp deep trough, [799] sharp prominent peak, [802-808] sudden volatility spike, [809-818] high volatility period, [822-846] low volatility period, [823] sharp deep trough, [836] sharp prominent peak, [847-854] normal volatility, [847] sharp deep trough, [852] sharp prominent peak, [856-862] normal volatility, [867-876] normal volatility, [869] sharp deep trough, [878-887] normal volatility, [878] sharp prominent peak, [881-887] short downward trend, [889-909] normal volatility, [892] sharp deep trough, [899] sharp prominent peak, [910-924] low volatility period, [912] sharp deep trough, [925-932] normal volatility, [925] sharp prominent peak, [933] sharp deep trough, [937-944] sudden volatility spike, [947-953] short upward trend, [953-960] normal volatility, [961-967] low volatility period, [971-979] normal volatility, [980-999] sudden volatility spike, [980] sharp prominent peak, [987] sharp deep trough, [993] sharp prominent peak',\n",
       " '<sequence length=1000 events=142> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-11] low volatility period, [6] sharp prominent peak, [12-31] normal volatility, [12] sharp deep trough, [32-39] low volatility period, [43] sharp prominent peak, [44-58] normal volatility, [58-63] short downward trend, [59] sharp deep trough, [65-73] high volatility period, [65] sharp prominent peak, [74-84] normal volatility, [85-95] low volatility period, [95] sharp deep trough, [96-104] normal volatility, [105] sharp prominent peak, [106-123] normal volatility, [110] sharp deep trough, [117] sharp prominent peak, [124-133] low volatility period, [129-134] short upward trend, [137-143] low volatility period, [144-149] normal volatility, [144] sharp deep trough, [151-171] normal volatility, [158-163] short upward trend, [162] sharp prominent peak, [172] sharp deep trough, [182-197] normal volatility, [191] sharp prominent peak, [197] sharp deep trough, [202-210] normal volatility, [211-218] low volatility period, [222-242] low volatility period, [229-234] short upward trend, [242] sharp prominent peak, [252-260] sudden volatility spike, [275-282] high volatility period, [276] sharp deep trough, [283-288] normal volatility, [290] sharp prominent peak, [294-309] low volatility period, [310-329] normal volatility, [310] sharp deep trough, [323] sharp prominent peak, [330-347] low volatility period, [332] sharp deep trough, [347] sharp prominent peak, [348-395] normal volatility, [359] sharp deep trough, [370] sharp prominent peak, [378] sharp deep trough, [396-425] low volatility period, [397] sharp prominent peak, [420] sharp deep trough, [426-442] normal volatility, [426] sharp prominent peak, [432-438] short downward trend, [441] sharp deep trough, [444-472] normal volatility, [446] sharp prominent peak, [455-460] short downward trend, [464-472] short downward trend, [467] sharp deep trough, [473] sharp prominent peak, [476-485] normal volatility, [478] sharp deep trough, [483] sharp prominent peak, [487-507] normal volatility, [488] sharp deep trough, [496] sharp prominent peak, [508-530] low volatility period, [526] sharp deep trough, [531-544] normal volatility, [545] sharp prominent peak, [546-552] high volatility period, [557-562] short downward trend, [558] sharp deep trough, [559-565] sudden volatility spike, [565] sharp prominent peak, [569-575] high volatility period, [574] sharp deep trough, [589] sharp prominent peak, [590-600] normal volatility, [601] sharp deep trough, [606-621] sudden volatility spike, [608] sharp prominent peak, [621] sharp deep trough, [628-635] normal volatility, [636] sharp prominent peak, [638-651] normal volatility, [654-659] normal volatility, [657] sharp deep trough, [663-671] normal volatility, [672] sharp prominent peak, [682] sharp deep trough, [684-691] sudden volatility spike, [696-703] normal volatility, [704-709] low volatility period, [705] sharp prominent peak, [710-720] normal volatility, [716] sharp deep trough, [730] sharp prominent peak, [733-739] normal volatility, [743] sharp deep trough, [746-756] low volatility period, [757-763] normal volatility, [762] sharp prominent peak, [764-775] high volatility period, [774] sharp deep trough, [776-783] normal volatility, [779] sharp prominent peak, [784-804] low volatility period, [805-827] normal volatility, [814] sharp deep trough, [828-833] high volatility period, [834] sharp prominent peak, [835-848] high volatility period, [845-851] short downward trend, [845] sharp deep trough, [852] sharp prominent peak, [858-871] sudden volatility spike, [866] sharp deep trough, [873-878] normal volatility, [877] sharp prominent peak, [882] sharp deep trough, [884-908] low volatility period, [891] sharp prominent peak, [898] sharp deep trough, [915-932] normal volatility, [923] sharp prominent peak, [934] sharp deep trough, [941] sharp prominent peak, [948-961] sudden volatility spike, [948] sharp deep trough, [972-979] normal volatility, [981-994] sudden volatility spike, [981] sharp prominent peak, [993] sharp deep trough, [994-999] short upward trend',\n",
       " '<sequence length=1000 events=137> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-15] low volatility period, [5] sharp deep trough, [12] sharp prominent peak, [16-34] normal volatility, [24] sharp deep trough, [36-42] sudden volatility spike, [36] sharp prominent peak, [48-54] high volatility period, [48] sharp deep trough, [61-69] normal volatility, [61] sharp prominent peak, [73-85] normal volatility, [86-92] high volatility period, [86] sharp deep trough, [91] sharp prominent peak, [97-102] normal volatility, [100] sharp deep trough, [103-126] low volatility period, [111] sharp prominent peak, [127-132] normal volatility, [137-146] sudden volatility spike, [144] sharp deep trough, [150-189] normal volatility, [163] sharp prominent peak, [170] sharp deep trough, [181] sharp prominent peak, [190-207] low volatility period, [194] sharp deep trough, [208-215] normal volatility, [216-222] high volatility period, [216] sharp prominent peak, [223] sharp deep trough, [227-232] short upward trend, [229-246] low volatility period, [238] sharp prominent peak, [247-256] normal volatility, [252] sharp deep trough, [258-264] normal volatility, [273-284] normal volatility, [282] sharp prominent peak, [285-294] low volatility period, [288] sharp deep trough, [295] sharp prominent peak, [299-311] normal volatility, [312] sharp deep trough, [313-329] normal volatility, [318] sharp prominent peak, [325] sharp deep trough, [330] sharp prominent peak, [332-341] high volatility period, [336] sharp deep trough, [342-348] normal volatility, [344] sharp prominent peak, [351] sharp deep trough, [356-370] sudden volatility spike, [358] sharp prominent peak, [369] sharp deep trough, [388] sharp prominent peak, [394] sharp deep trough, [408-424] normal volatility, [420] sharp prominent peak, [425-464] low volatility period, [434] sharp deep trough, [455] sharp prominent peak, [462] sharp deep trough, [465-470] normal volatility, [473-497] normal volatility, [499-509] normal volatility, [508] sharp prominent peak, [514] sharp deep trough, [515-523] normal volatility, [524] sharp prominent peak, [529-547] normal volatility, [540] sharp deep trough, [547] sharp prominent peak, [552] sharp deep trough, [555-561] normal volatility, [558] sharp prominent peak, [573] sharp deep trough, [575-581] normal volatility, [578] sharp prominent peak, [586-595] normal volatility, [586] sharp deep trough, [596] sharp prominent peak, [599-615] sudden volatility spike, [601] sharp deep trough, [606] sharp prominent peak, [617] sharp deep trough, [619-672] normal volatility, [636] sharp prominent peak, [644] sharp deep trough, [653] sharp prominent peak, [675-684] normal volatility, [675] sharp deep trough, [685] sharp prominent peak, [692-704] sudden volatility spike, [692] sharp deep trough, [708] sharp prominent peak, [713] sharp deep trough, [718-730] normal volatility, [721] sharp prominent peak, [726] sharp deep trough, [731-773] low volatility period, [732] sharp prominent peak, [735-740] short downward trend, [757] sharp deep trough, [771] sharp prominent peak, [774-798] normal volatility, [782] sharp deep trough, [791] sharp prominent peak, [796] sharp deep trough, [799-810] low volatility period, [801] sharp prominent peak, [817] sharp deep trough, [821-828] short upward trend, [833] sharp prominent peak, [842] sharp deep trough, [845-852] high volatility period, [849] sharp prominent peak, [853-858] normal volatility, [855] sharp deep trough, [879-889] low volatility period, [886] sharp prominent peak, [890-897] normal volatility, [891] sharp deep trough, [897] sharp prominent peak, [907-913] low volatility period, [913] sharp deep trough, [926] local peak, [941-946] short downward trend, [946-973] normal volatility, [948] sharp deep trough, [975-988] normal volatility, [975] sharp prominent peak, [991] sharp deep trough',\n",
       " '<sequence length=1000 events=128> [0-999] sideways consolidation regime, [0-19] flat stable segment, [1] sharp prominent peak, [9-24] sudden volatility spike, [36-56] normal volatility, [42] sharp deep trough, [50] sharp prominent peak, [57-97] low volatility period, [70] sharp deep trough, [100-119] sudden volatility spike, [115] sharp prominent peak, [120] sharp deep trough, [121-132] normal volatility, [133] sharp prominent peak, [140] sharp deep trough, [145] sharp prominent peak, [149-158] normal volatility, [155] sharp deep trough, [164-172] sudden volatility spike, [164] sharp prominent peak, [173-183] high volatility period, [180] sharp deep trough, [184-196] normal volatility, [200-207] low volatility period, [207] sharp prominent peak, [213-237] normal volatility, [218] sharp deep trough, [224-229] short downward trend, [234] sharp prominent peak, [238-248] low volatility period, [248] sharp deep trough, [249-264] normal volatility, [266] sharp prominent peak, [274-287] normal volatility, [274] sharp deep trough, [290-302] normal volatility, [302] sharp prominent peak, [304-322] normal volatility, [311] sharp deep trough, [331-344] low volatility period, [346] sharp prominent peak, [347-362] high volatility period, [359] sharp deep trough, [366-381] low volatility period, [374] sharp prominent peak, [379] sharp deep trough, [382-401] normal volatility, [388] sharp prominent peak, [393] sharp deep trough, [405] sharp prominent peak, [412-424] normal volatility, [425-445] low volatility period, [430] sharp deep trough, [441] sharp prominent peak, [446] sharp deep trough, [448-468] low volatility period, [454] sharp prominent peak, [461] sharp deep trough, [466] sharp prominent peak, [469-480] normal volatility, [474] sharp deep trough, [486-500] low volatility period, [499] sharp prominent peak, [506-511] normal volatility, [512-517] low volatility period, [519] sharp deep trough, [524-550] normal volatility, [526] sharp prominent peak, [533-539] short downward trend, [549] sharp deep trough, [558] sharp prominent peak, [562-577] sudden volatility spike, [563] sharp deep trough, [571] sharp prominent peak, [582] sharp deep trough, [586-599] low volatility period, [600] sharp prominent peak, [604-614] normal volatility, [606] sharp deep trough, [615] sharp prominent peak, [620-625] normal volatility, [624] sharp deep trough, [640] sharp prominent peak, [653-658] high volatility period, [659] sharp deep trough, [668] sharp prominent peak, [673] sharp deep trough, [679-685] normal volatility, [684] sharp prominent peak, [688-699] normal volatility, [698] sharp deep trough, [700-705] high volatility period, [714-719] normal volatility, [720] sharp prominent peak, [723-729] short downward trend, [723-731] normal volatility, [733-752] normal volatility, [733] sharp deep trough, [753-771] low volatility period, [772-785] normal volatility, [778] sharp prominent peak, [792-803] normal volatility, [798] sharp deep trough, [804] sharp prominent peak, [806-811] low volatility period, [810] sharp deep trough, [823-849] normal volatility, [823] sharp prominent peak, [831] sharp deep trough, [838] sharp prominent peak, [851-857] normal volatility, [860] sharp deep trough, [866-872] normal volatility, [873-880] high volatility period, [873] sharp prominent peak, [881-892] normal volatility, [897-907] normal volatility, [908] sharp deep trough, [921-927] sudden volatility spike, [921] sharp prominent peak, [928-933] high volatility period, [941-953] normal volatility, [959] sharp deep trough, [970-975] short downward trend, [970-976] high volatility period, [976-981] short upward trend, [986-999] sudden volatility spike, [986] sharp prominent peak',\n",
       " '<sequence length=1000 events=148> [0-999] sideways consolidation regime, [0-19] flat stable segment, [5-11] normal volatility, [13-20] sudden volatility spike, [13] sharp prominent peak, [18] sharp deep trough, [21-30] normal volatility, [25] sharp prominent peak, [31] sharp deep trough, [32-40] high volatility period, [46] sharp prominent peak, [56] sharp deep trough, [66-73] normal volatility, [69] sharp prominent peak, [74] sharp deep trough, [83-88] normal volatility, [83] sharp prominent peak, [96-102] normal volatility, [103-111] low volatility period, [103] sharp deep trough, [112] sharp prominent peak, [114-120] short downward trend, [121-130] short upward trend, [123-130] low volatility period, [131-137] normal volatility, [141-147] normal volatility, [150] sharp deep trough, [165-170] sudden volatility spike, [165] sharp prominent peak, [174-182] normal volatility, [183] sharp deep trough, [186-201] normal volatility, [202] sharp prominent peak, [203-220] normal volatility, [215-220] short upward trend, [222] sharp deep trough, [224-233] sudden volatility spike, [235-241] sudden volatility spike, [238] sharp prominent peak, [245-257] normal volatility, [251] sharp deep trough, [258-265] low volatility period, [266] sharp prominent peak, [276-285] sudden volatility spike, [276] sharp deep trough, [287-295] normal volatility, [293] sharp prominent peak, [301-321] low volatility period, [303-310] short downward trend, [310] local trough, [322-332] normal volatility, [333] sharp prominent peak, [342-351] normal volatility, [352] sharp deep trough, [353-360] normal volatility, [361-380] low volatility period, [371] sharp prominent peak, [376] sharp deep trough, [381-386] normal volatility, [381] sharp prominent peak, [386] sharp deep trough, [387-395] high volatility period, [396-404] normal volatility, [412] sharp prominent peak, [418] sharp deep trough, [421-427] sudden volatility spike, [423] sharp prominent peak, [432-440] normal volatility, [451-456] normal volatility, [461-467] sudden volatility spike, [461] sharp deep trough, [462-467] short upward trend, [469] sharp prominent peak, [470-475] short downward trend, [481-489] normal volatility, [482] sharp deep trough, [489] sharp prominent peak, [498-517] sudden volatility spike, [498] sharp deep trough, [512] sharp prominent peak, [523-550] low volatility period, [526] sharp deep trough, [537] sharp prominent peak, [551-563] normal volatility, [551] sharp deep trough, [564] sharp prominent peak, [570] sharp deep trough, [578-583] normal volatility, [589] sharp prominent peak, [590-638] low volatility period, [602] sharp deep trough, [616] sharp prominent peak, [625-631] short downward trend, [630] sharp deep trough, [636] sharp prominent peak, [639-666] normal volatility, [644] sharp deep trough, [649] sharp prominent peak, [656] sharp deep trough, [665] sharp prominent peak, [670] sharp deep trough, [675-680] high volatility period, [675] sharp prominent peak, [682] sharp deep trough, [688] sharp prominent peak, [699-711] normal volatility, [699] sharp deep trough, [714] sharp prominent peak, [719-729] normal volatility, [730-745] low volatility period, [742] sharp deep trough, [746-756] normal volatility, [761] sharp prominent peak, [762-780] normal volatility, [781-786] low volatility period, [786] sharp deep trough, [787-793] normal volatility, [794-805] high volatility period, [813] sharp prominent peak, [818-829] low volatility period, [821] sharp deep trough, [830-852] normal volatility, [830] sharp prominent peak, [831-837] short downward trend, [844] sharp deep trough, [853-862] low volatility period, [863-884] normal volatility, [882] sharp prominent peak, [885-890] short downward trend, [890-909] normal volatility, [890] sharp deep trough, [906] sharp prominent peak, [919-931] normal volatility, [919] sharp deep trough, [928] sharp prominent peak, [935-940] normal volatility, [943-958] normal volatility, [943-949] short upward trend, [944] sharp deep trough, [952] sharp prominent peak, [959-964] sudden volatility spike, [959] sharp deep trough, [965-978] high volatility period, [978] sharp prominent peak, [983-988] high volatility period, [988-993] short downward trend, [989-999] normal volatility, [995] sharp deep trough',\n",
       " '<sequence length=1000 events=144> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-12] low volatility period, [7] sharp deep trough, [16-24] high volatility period, [38-50] normal volatility, [38] sharp prominent peak, [54] sharp deep trough, [62] sharp prominent peak, [74-84] normal volatility, [75] local trough, [81] sharp prominent peak, [87-104] sudden volatility spike, [96] sharp deep trough, [101] sharp prominent peak, [109] sharp deep trough, [117] sharp prominent peak, [118-129] normal volatility, [125] sharp deep trough, [126-132] short upward trend, [130-139] low volatility period, [137] sharp prominent peak, [140-160] normal volatility, [146] sharp deep trough, [155] sharp prominent peak, [164] sharp deep trough, [170] sharp prominent peak, [173-198] low volatility period, [179] sharp deep trough, [189-196] short downward trend, [203] sharp prominent peak, [215-220] high volatility period, [215] sharp deep trough, [221-233] normal volatility, [222] sharp prominent peak, [234-247] low volatility period, [237] sharp deep trough, [251-259] normal volatility, [260] sharp prominent peak, [267] sharp deep trough, [268-279] normal volatility, [280-318] low volatility period, [284] sharp prominent peak, [296] sharp deep trough, [301] sharp prominent peak, [308] sharp deep trough, [318] sharp prominent peak, [319-324] short downward trend, [319-324] normal volatility, [323] sharp deep trough, [328-347] normal volatility, [328] sharp prominent peak, [335] sharp deep trough, [342] sharp prominent peak, [350-364] normal volatility, [362] sharp deep trough, [366-373] normal volatility, [378] sharp prominent peak, [379-384] normal volatility, [390-403] normal volatility, [404-409] sudden volatility spike, [404] sharp deep trough, [410-417] high volatility period, [424] sharp prominent peak, [432-439] normal volatility, [433] sharp deep trough, [455] sharp prominent peak, [457-463] normal volatility, [464] sharp deep trough, [465-480] normal volatility, [474] sharp prominent peak, [484-506] low volatility period, [488] sharp deep trough, [501] sharp prominent peak, [507] sharp deep trough, [517-527] sudden volatility spike, [517] sharp prominent peak, [539-556] normal volatility, [545] sharp deep trough, [551] sharp prominent peak, [557-562] low volatility period, [565] sharp deep trough, [571] sharp prominent peak, [580-590] sudden volatility spike, [581] sharp deep trough, [592-600] normal volatility, [606-619] normal volatility, [616] sharp prominent peak, [620-625] high volatility period, [626-642] normal volatility, [648] sharp deep trough, [653] sharp prominent peak, [658-667] high volatility period, [660] sharp deep trough, [668-674] normal volatility, [668] sharp prominent peak, [673] sharp deep trough, [675-683] low volatility period, [684-697] normal volatility, [694] sharp prominent peak, [695-700] short downward trend, [704] sharp deep trough, [706-721] sudden volatility spike, [730-735] normal volatility, [736] sharp prominent peak, [740-755] normal volatility, [755-760] short downward trend, [760] sharp deep trough, [766] sharp prominent peak, [772-778] high volatility period, [772] sharp deep trough, [778] sharp prominent peak, [780-790] short downward trend, [783] sharp deep trough, [790] sharp prominent peak, [792-798] high volatility period, [799] sharp deep trough, [803-822] low volatility period, [805] sharp prominent peak, [826] sharp deep trough, [829-839] sudden volatility spike, [831] sharp prominent peak, [852-868] normal volatility, [857] sharp deep trough, [873-881] low volatility period, [879] sharp prominent peak, [882-889] normal volatility, [893-901] normal volatility, [894] sharp deep trough, [899] sharp prominent peak, [907-915] normal volatility, [913] sharp deep trough, [918] sharp prominent peak, [922-937] normal volatility, [926] sharp deep trough, [931] sharp prominent peak, [936] sharp deep trough, [940-948] normal volatility, [941] sharp prominent peak, [949] sharp deep trough, [960-995] normal volatility, [969] sharp prominent peak, [979] sharp deep trough, [989] sharp prominent peak',\n",
       " '<sequence length=1000 events=133> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-32] low volatility period, [30] sharp prominent peak, [33-52] normal volatility, [51] sharp deep trough, [57-74] normal volatility, [57] sharp prominent peak, [69] sharp deep trough, [75-103] low volatility period, [86] sharp prominent peak, [108-121] normal volatility, [112] sharp deep trough, [117] sharp prominent peak, [122] sharp deep trough, [123-141] normal volatility, [147] sharp prominent peak, [159-164] high volatility period, [161] sharp deep trough, [172-177] normal volatility, [172] sharp prominent peak, [179-188] normal volatility, [190-196] normal volatility, [194] sharp deep trough, [197-204] high volatility period, [213] sharp prominent peak, [234] sharp deep trough, [238-244] sudden volatility spike, [242-250] short upward trend, [246-257] sudden volatility spike, [246] sharp prominent peak, [251] sharp deep trough, [259-265] sudden volatility spike, [259] sharp prominent peak, [265] local trough, [277-282] short downward trend, [283] sharp prominent peak, [292-304] high volatility period, [296] sharp deep trough, [305-348] normal volatility, [316] sharp prominent peak, [327] sharp deep trough, [351-372] normal volatility, [364] sharp prominent peak, [369] sharp deep trough, [373-378] low volatility period, [389] sharp prominent peak, [392-403] high volatility period, [407] sharp deep trough, [409-425] normal volatility, [425] sharp prominent peak, [427-445] normal volatility, [433] sharp deep trough, [443] sharp prominent peak, [446-457] low volatility period, [450] sharp deep trough, [458] sharp prominent peak, [462-480] low volatility period, [481-497] normal volatility, [481] sharp deep trough, [492] sharp prominent peak, [499-511] normal volatility, [500] sharp deep trough, [507] sharp prominent peak, [514-527] normal volatility, [514] sharp deep trough, [528-548] low volatility period, [528] sharp prominent peak, [534] sharp deep trough, [540] sharp prominent peak, [545] sharp deep trough, [549-555] normal volatility, [553] sharp prominent peak, [556-562] high volatility period, [563-579] sudden volatility spike, [567] sharp deep trough, [587-607] low volatility period, [595] sharp prominent peak, [608-638] normal volatility, [619] sharp deep trough, [630] sharp prominent peak, [638] sharp deep trough, [642-655] normal volatility, [648] sharp prominent peak, [658-670] normal volatility, [667-672] short downward trend, [674] sharp deep trough, [676-687] low volatility period, [693] sharp prominent peak, [694-701] low volatility period, [708-717] normal volatility, [721] sharp deep trough, [724-731] normal volatility, [732-737] high volatility period, [732] sharp prominent peak, [740] sharp deep trough, [750-765] normal volatility, [766-771] high volatility period, [766] sharp prominent peak, [772-785] sudden volatility spike, [772] sharp deep trough, [784] sharp prominent peak, [786-800] high volatility period, [801-811] normal volatility, [807] sharp deep trough, [814-824] low volatility period, [825-833] normal volatility, [825] sharp prominent peak, [835-841] normal volatility, [840] sharp deep trough, [853] sharp prominent peak, [856-872] sudden volatility spike, [868] sharp deep trough, [877-886] normal volatility, [884-890] short upward trend, [887-893] low volatility period, [888] sharp prominent peak, [894-912] normal volatility, [894] sharp deep trough, [902] sharp prominent peak, [910] sharp deep trough, [912-918] short upward trend, [914-921] normal volatility, [923-931] normal volatility, [926] sharp prominent peak, [932-970] low volatility period, [939] sharp deep trough, [952] sharp prominent peak, [969] sharp deep trough, [971-987] normal volatility, [971-982] short upward trend, [991-999] normal volatility, [992] sharp prominent peak',\n",
       " '<sequence length=1000 events=130> [0-999] sideways consolidation regime, [0-20] flat stable segment, [0-23] low volatility period, [1] sharp deep trough, [16] sharp prominent peak, [23] sharp deep trough, [27-65] normal volatility, [37] sharp prominent peak, [49] sharp deep trough, [66] sharp prominent peak, [72-94] sudden volatility spike, [72] sharp deep trough, [88] sharp prominent peak, [103] sharp deep trough, [108-123] normal volatility, [110] sharp prominent peak, [114-120] short downward trend, [120] sharp deep trough, [128-139] normal volatility, [128] sharp prominent peak, [134] sharp deep trough, [150] sharp prominent peak, [152-161] normal volatility, [163] sharp deep trough, [170-179] low volatility period, [187-220] normal volatility, [187] sharp prominent peak, [207] sharp deep trough, [234-249] normal volatility, [240] sharp prominent peak, [245] sharp deep trough, [251-280] normal volatility, [258] sharp prominent peak, [273] sharp deep trough, [281] sharp prominent peak, [286-291] high volatility period, [292-300] normal volatility, [301-317] low volatility period, [303] sharp deep trough, [318] sharp prominent peak, [321-326] high volatility period, [329-336] high volatility period, [329] sharp deep trough, [336] sharp prominent peak, [338-348] normal volatility, [344] sharp deep trough, [355] sharp prominent peak, [358-375] sudden volatility spike, [377] sharp deep trough, [380-401] low volatility period, [402-407] normal volatility, [402] sharp prominent peak, [408-413] high volatility period, [415-420] high volatility period, [428-435] normal volatility, [436] sharp deep trough, [437-443] short upward trend, [442] sharp prominent peak, [450-455] sudden volatility spike, [450] sharp deep trough, [461] sharp prominent peak, [463-479] normal volatility, [480] sharp deep trough, [481-490] normal volatility, [487] sharp prominent peak, [494-499] normal volatility, [495] sharp deep trough, [500-505] low volatility period, [506] sharp prominent peak, [520-537] normal volatility, [520] sharp deep trough, [531] sharp prominent peak, [545-550] normal volatility, [551-563] low volatility period, [552-557] short downward trend, [564-572] normal volatility, [573-585] sudden volatility spike, [573] sharp deep trough, [584] sharp prominent peak, [595-617] low volatility period, [607] sharp deep trough, [612] sharp prominent peak, [618-644] normal volatility, [629] sharp deep trough, [642] sharp prominent peak, [645-661] high volatility period, [649] sharp deep trough, [659] sharp prominent peak, [662-668] normal volatility, [673] sharp deep trough, [679-696] normal volatility, [679] sharp prominent peak, [699-710] low volatility period, [708] sharp deep trough, [714] sharp prominent peak, [717-725] normal volatility, [723] sharp deep trough, [728-748] normal volatility, [736] sharp prominent peak, [748] sharp deep trough, [751-765] sudden volatility spike, [771-779] low volatility period, [779] sharp prominent peak, [782-795] high volatility period, [796] sharp deep trough, [802-819] normal volatility, [803] sharp prominent peak, [808] local trough, [813] sharp prominent peak, [820] sharp deep trough, [822-839] normal volatility, [823-828] short upward trend, [840-859] low volatility period, [862] sharp prominent peak, [874-881] normal volatility, [882-927] low volatility period, [882] sharp deep trough, [895] sharp prominent peak, [928] sharp deep trough, [930-939] normal volatility, [940] sharp prominent peak, [941-947] sudden volatility spike, [949-954] short downward trend, [950-960] normal volatility, [962-977] normal volatility, [962] sharp deep trough, [981-986] short upward trend, [984-989] high volatility period, [984] sharp prominent peak, [992] sharp deep trough',\n",
       " '<sequence length=1000 events=136> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-9] low volatility period, [2] sharp prominent peak, [10] sharp deep trough, [12-19] high volatility period, [22] sharp prominent peak, [30-36] low volatility period, [35] sharp deep trough, [47] sharp prominent peak, [50-56] normal volatility, [55-60] short downward trend, [64] sharp deep trough, [77-83] normal volatility, [86] sharp prominent peak, [96-105] normal volatility, [106-111] low volatility period, [108] sharp deep trough, [112-139] normal volatility, [124] sharp prominent peak, [134] sharp deep trough, [145-150] high volatility period, [148] sharp prominent peak, [157-163] low volatility period, [160] sharp deep trough, [170-177] normal volatility, [172] sharp prominent peak, [178] sharp deep trough, [183-227] sudden volatility spike, [185] sharp prominent peak, [193] sharp deep trough, [216] sharp prominent peak, [241] sharp deep trough, [247-252] short downward trend, [249-258] normal volatility, [254] sharp prominent peak, [259] sharp deep trough, [260-266] normal volatility, [266] sharp prominent peak, [270-286] normal volatility, [272] sharp deep trough, [277] sharp prominent peak, [287] sharp deep trough, [291-296] high volatility period, [297-306] normal volatility, [307-327] low volatility period, [314] sharp prominent peak, [328] sharp deep trough, [331-341] low volatility period, [342-357] normal volatility, [343] sharp prominent peak, [358] sharp deep trough, [362-367] normal volatility, [368-373] short upward trend, [373] sharp prominent peak, [378] sharp deep trough, [388-397] normal volatility, [398-410] low volatility period, [413-419] low volatility period, [424] sharp prominent peak, [432] sharp deep trough, [440] sharp prominent peak, [444-449] normal volatility, [453] sharp deep trough, [460-473] normal volatility, [463] sharp prominent peak, [475-482] normal volatility, [476] sharp deep trough, [486-492] normal volatility, [493-506] low volatility period, [500] sharp prominent peak, [507-526] normal volatility, [507] sharp deep trough, [513] sharp prominent peak, [520] sharp deep trough, [529-537] normal volatility, [538-544] low volatility period, [540] sharp prominent peak, [545-560] normal volatility, [545] sharp deep trough, [550] sharp prominent peak, [559] sharp deep trough, [562-576] normal volatility, [564] sharp prominent peak, [570] sharp deep trough, [582-607] low volatility period, [587] sharp prominent peak, [601] sharp deep trough, [608-620] normal volatility, [622-627] normal volatility, [623] sharp prominent peak, [628-635] low volatility period, [636] sharp deep trough, [642-652] high volatility period, [662] sharp prominent peak, [666-674] normal volatility, [676-682] short downward trend, [679-699] low volatility period, [682] sharp deep trough, [688] sharp prominent peak, [704-712] normal volatility, [704] sharp deep trough, [723-730] normal volatility, [732-738] short upward trend, [733-739] normal volatility, [733] sharp prominent peak, [740] sharp deep trough, [744-754] sudden volatility spike, [753] sharp prominent peak, [758] sharp deep trough, [765-772] high volatility period, [765] sharp prominent peak, [773-778] normal volatility, [777] sharp deep trough, [781-794] normal volatility, [787] sharp prominent peak, [795] sharp deep trough, [799-810] normal volatility, [811-824] low volatility period, [827-841] normal volatility, [846-856] normal volatility, [847] sharp prominent peak, [858-869] normal volatility, [871-879] normal volatility, [879] sharp deep trough, [880-886] high volatility period, [899-908] normal volatility, [912-918] normal volatility, [915] sharp prominent peak, [920-926] sudden volatility spike, [943] sharp deep trough, [945-958] low volatility period, [959] sharp prominent peak, [981-988] normal volatility, [989-995] low volatility period, [996] sharp deep trough',\n",
       " '<sequence length=1000 events=143> [0-999] sideways consolidation regime, [0-19] flat stable segment, [3] sharp prominent peak, [7-26] sudden volatility spike, [8] sharp deep trough, [24] local peak, [33-40] low volatility period, [41-68] normal volatility, [57-63] short downward trend, [63] sharp deep trough, [69-74] high volatility period, [75-82] normal volatility, [76-83] short upward trend, [78] sharp prominent peak, [83-113] low volatility period, [101] sharp deep trough, [114-120] short upward trend, [114-120] normal volatility, [118] sharp prominent peak, [121-127] low volatility period, [128-146] normal volatility, [128] sharp deep trough, [152] sharp prominent peak, [154-160] high volatility period, [166] sharp deep trough, [172-180] normal volatility, [174] sharp prominent peak, [182-193] normal volatility, [196] sharp deep trough, [202] sharp prominent peak, [217-226] normal volatility, [221] sharp deep trough, [235] local peak, [239-244] sudden volatility spike, [240] sharp deep trough, [246] sharp prominent peak, [250-257] high volatility period, [258-270] normal volatility, [260] sharp deep trough, [266] sharp prominent peak, [271-283] low volatility period, [281-286] short downward trend, [287-300] normal volatility, [301] sharp deep trough, [303-320] normal volatility, [315] sharp prominent peak, [321-341] low volatility period, [342-350] normal volatility, [343] sharp deep trough, [353-361] high volatility period, [353] sharp prominent peak, [359-366] short upward trend, [362-378] normal volatility, [367-372] short downward trend, [368] sharp deep trough, [375] sharp prominent peak, [382] sharp deep trough, [383-394] normal volatility, [395-406] low volatility period, [407] sharp prominent peak, [413-426] high volatility period, [432-459] normal volatility, [432] sharp deep trough, [433-441] short upward trend, [444] sharp prominent peak, [461] sharp deep trough, [466-479] normal volatility, [466] sharp prominent peak, [472] sharp deep trough, [480] sharp prominent peak, [485-499] high volatility period, [485] sharp deep trough, [502] sharp prominent peak, [509-519] sudden volatility spike, [509] sharp deep trough, [514] sharp prominent peak, [522-529] normal volatility, [528] sharp deep trough, [539-547] normal volatility, [539] sharp prominent peak, [549] sharp deep trough, [552-566] normal volatility, [562] sharp prominent peak, [572-578] normal volatility, [582-587] short upward trend, [584-591] normal volatility, [588] sharp deep trough, [593-607] normal volatility, [612] sharp prominent peak, [618-636] low volatility period, [637-650] normal volatility, [637] sharp deep trough, [642] sharp prominent peak, [648] sharp deep trough, [650-656] short upward trend, [655] sharp prominent peak, [665] sharp deep trough, [674] sharp prominent peak, [676-684] normal volatility, [680] sharp deep trough, [685-706] low volatility period, [685] sharp prominent peak, [694] sharp deep trough, [704] sharp prominent peak, [707-715] normal volatility, [718-726] high volatility period, [725] sharp deep trough, [730-749] sudden volatility spike, [741] sharp prominent peak, [751] sharp deep trough, [754-794] normal volatility, [763-768] short upward trend, [786] sharp prominent peak, [800] sharp deep trough, [802-814] sudden volatility spike, [815] sharp prominent peak, [822] sharp deep trough, [824-831] low volatility period, [834-855] normal volatility, [849] sharp prominent peak, [856-863] low volatility period, [864] sharp deep trough, [867-875] low volatility period, [870] sharp prominent peak, [876-881] normal volatility, [876] sharp deep trough, [881] sharp prominent peak, [884-899] normal volatility, [889] sharp deep trough, [900-937] low volatility period, [901-908] short upward trend, [925] sharp prominent peak, [932] sharp deep trough, [938] sharp prominent peak, [944] sharp deep trough, [954] sharp prominent peak, [959] sharp deep trough, [964-972] normal volatility, [964] sharp prominent peak, [973-987] low volatility period, [974] sharp deep trough, [988-999] normal volatility, [996] sharp prominent peak',\n",
       " '<sequence length=1000 events=131> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-15] low volatility period, [3] sharp prominent peak, [16-22] normal volatility, [16] sharp deep trough, [23-28] low volatility period, [29-48] normal volatility, [29] sharp prominent peak, [37] sharp deep trough, [46] sharp prominent peak, [49-67] low volatility period, [55] sharp deep trough, [68-74] normal volatility, [68] sharp prominent peak, [75] sharp deep trough, [79-87] high volatility period, [92-105] normal volatility, [107] sharp prominent peak, [111-125] low volatility period, [118] sharp deep trough, [126-132] short downward trend, [127-135] low volatility period, [136-141] normal volatility, [143] sharp prominent peak, [147-155] high volatility period, [155] sharp deep trough, [156-162] normal volatility, [166-179] normal volatility, [180-199] low volatility period, [188-194] short upward trend, [193] sharp prominent peak, [200-208] normal volatility, [208] sharp deep trough, [211-229] normal volatility, [217] local peak, [223] sharp deep trough, [230] sharp prominent peak, [238-249] sudden volatility spike, [239] sharp deep trough, [251-257] high volatility period, [256] sharp prominent peak, [260-265] normal volatility, [266-275] high volatility period, [279-300] normal volatility, [279] sharp deep trough, [286] sharp prominent peak, [301-327] low volatility period, [312] sharp deep trough, [326-332] short upward trend, [328-341] normal volatility, [331] sharp prominent peak, [344] sharp deep trough, [346-360] normal volatility, [350] sharp prominent peak, [363] sharp deep trough, [364-400] normal volatility, [381] sharp prominent peak, [389] sharp deep trough, [403-416] normal volatility, [413] sharp prominent peak, [418-438] normal volatility, [418] sharp deep trough, [423] sharp prominent peak, [446-452] normal volatility, [451] sharp deep trough, [456] sharp prominent peak, [461] sharp deep trough, [470-489] sudden volatility spike, [470] sharp prominent peak, [475] sharp deep trough, [496-507] low volatility period, [508] sharp prominent peak, [520] sharp deep trough, [528-540] normal volatility, [542] sharp prominent peak, [549-561] normal volatility, [557] sharp deep trough, [562] sharp prominent peak, [575] sharp deep trough, [582-617] low volatility period, [595] sharp prominent peak, [604] sharp deep trough, [618-629] normal volatility, [618] sharp prominent peak, [638-651] low volatility period, [652-669] normal volatility, [652] sharp deep trough, [663] sharp prominent peak, [670] sharp deep trough, [672-691] normal volatility, [679] sharp prominent peak, [691] sharp deep trough, [693-700] normal volatility, [709] sharp prominent peak, [711-723] high volatility period, [716-723] short upward trend, [736-745] sudden volatility spike, [755] sharp deep trough, [756-762] normal volatility, [761] sharp prominent peak, [782] sharp deep trough, [792-801] sudden volatility spike, [792] sharp prominent peak, [804] sharp deep trough, [811] sharp prominent peak, [812-817] normal volatility, [822-829] normal volatility, [822] sharp deep trough, [830] sharp prominent peak, [834-841] sudden volatility spike, [846-851] short downward trend, [851] sharp deep trough, [854-867] normal volatility, [869] sharp prominent peak, [878-884] normal volatility, [890-901] normal volatility, [900] sharp deep trough, [910-921] normal volatility, [915] sharp prominent peak, [920] sharp deep trough, [922-933] low volatility period, [928] sharp prominent peak, [934-947] normal volatility, [934] sharp deep trough, [948-975] low volatility period, [961] sharp prominent peak, [976] sharp deep trough, [981-987] high volatility period, [988-997] sudden volatility spike, [990] sharp prominent peak',\n",
       " '<sequence length=1000 events=146> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-7] low volatility period, [2] sharp deep trough, [9] sharp prominent peak, [11-17] high volatility period, [16] sharp deep trough, [25] sharp prominent peak, [28-33] normal volatility, [36-44] low volatility period, [45-56] normal volatility, [45] sharp deep trough, [57-64] high volatility period, [60] sharp prominent peak, [65-73] normal volatility, [74-87] low volatility period, [77] sharp deep trough, [88-97] normal volatility, [89] sharp prominent peak, [94] sharp deep trough, [99-105] short upward trend, [102-107] high volatility period, [102] sharp prominent peak, [108-113] normal volatility, [114] sharp deep trough, [122-129] high volatility period, [130-135] normal volatility, [135] sharp prominent peak, [136-148] low volatility period, [149-172] normal volatility, [156] sharp deep trough, [173] sharp prominent peak, [185] sharp deep trough, [193-204] normal volatility, [197] sharp prominent peak, [203] sharp deep trough, [205-212] low volatility period, [213-236] normal volatility, [213] sharp prominent peak, [226] sharp deep trough, [237-254] low volatility period, [246] sharp prominent peak, [257] sharp deep trough, [269] sharp prominent peak, [275-282] sudden volatility spike, [284-294] sudden volatility spike, [284] sharp deep trough, [289-294] short upward trend, [291] sharp prominent peak, [295-301] high volatility period, [304] sharp deep trough, [305-318] normal volatility, [306-313] short downward trend, [319] sharp prominent peak, [324-331] high volatility period, [332] sharp deep trough, [337] sharp prominent peak, [343-367] normal volatility, [346-351] short upward trend, [361] sharp deep trough, [372-384] normal volatility, [379] sharp prominent peak, [387-452] low volatility period, [388] sharp deep trough, [401] sharp prominent peak, [415] local trough, [433-439] short upward trend, [434] sharp prominent peak, [443] sharp deep trough, [455-464] normal volatility, [455] sharp prominent peak, [472] sharp deep trough, [477-482] short upward trend, [480-489] normal volatility, [480] sharp prominent peak, [501-521] normal volatility, [501] sharp deep trough, [516] sharp prominent peak, [522-530] low volatility period, [534-543] normal volatility, [534] sharp deep trough, [536-542] short upward trend, [540] sharp prominent peak, [546-553] sudden volatility spike, [546] sharp deep trough, [557] sharp prominent peak, [562-590] normal volatility, [567] sharp deep trough, [575] sharp prominent peak, [586] sharp deep trough, [591] sharp prominent peak, [599-623] sudden volatility spike, [599] sharp deep trough, [604] sharp prominent peak, [606-613] short downward trend, [611] sharp deep trough, [625-630] normal volatility, [627] sharp prominent peak, [631-661] low volatility period, [636] sharp deep trough, [657] sharp prominent peak, [662-681] normal volatility, [687-695] normal volatility, [687] sharp deep trough, [699-710] normal volatility, [712-722] normal volatility, [727-732] high volatility period, [733-745] normal volatility, [734] sharp prominent peak, [746-751] low volatility period, [752] sharp deep trough, [758-763] low volatility period, [764-787] normal volatility, [768] sharp prominent peak, [773] sharp deep trough, [780] sharp prominent peak, [791-801] low volatility period, [795] sharp deep trough, [806-827] normal volatility, [808] sharp prominent peak, [829] sharp deep trough, [836-846] normal volatility, [837] sharp prominent peak, [849-862] low volatility period, [851] sharp deep trough, [863-877] normal volatility, [863] sharp prominent peak, [878-884] high volatility period, [891-897] sudden volatility spike, [891] sharp deep trough, [902-908] high volatility period, [909-915] normal volatility, [917] sharp prominent peak, [922-931] high volatility period, [922] sharp deep trough, [930] sharp prominent peak, [931-936] short downward trend, [938] sharp deep trough, [941-947] high volatility period, [951-958] short upward trend, [956] sharp prominent peak, [958-986] normal volatility, [964] sharp deep trough, [969] sharp prominent peak, [987] sharp deep trough, [988-999] normal volatility',\n",
       " '<sequence length=1000 events=148> [0-999] sideways consolidation regime, [0-19] flat stable segment, [0-19] low volatility period, [20-29] normal volatility, [20] sharp deep trough, [30-35] low volatility period, [40-58] low volatility period, [45] sharp prominent peak, [50] sharp deep trough, [57] sharp prominent peak, [63-98] low volatility period, [74] sharp deep trough, [82] sharp prominent peak, [99] sharp deep trough, [104-112] high volatility period, [104] sharp prominent peak, [113-118] sudden volatility spike, [113] sharp deep trough, [124-133] high volatility period, [124] sharp prominent peak, [134-140] normal volatility, [141] sharp deep trough, [146-160] normal volatility, [163-169] low volatility period, [172-177] short downward trend, [173-181] normal volatility, [182] sharp prominent peak, [187] sharp deep trough, [190-198] high volatility period, [197] sharp prominent peak, [203-211] low volatility period, [213-218] low volatility period, [219-225] normal volatility, [226] sharp deep trough, [228-233] high volatility period, [243-249] high volatility period, [245] sharp prominent peak, [246-252] short downward trend, [250-266] normal volatility, [262] sharp deep trough, [268-284] normal volatility, [268] sharp prominent peak, [284] sharp deep trough, [285-301] sudden volatility spike, [304] sharp prominent peak, [307-322] low volatility period, [309] sharp deep trough, [320] sharp prominent peak, [322-327] short downward trend, [323-334] normal volatility, [325] sharp deep trough, [330] sharp prominent peak, [335] sharp deep trough, [340-354] normal volatility, [348] sharp prominent peak, [355-360] low volatility period, [361] sharp deep trough, [370] sharp prominent peak, [371-380] high volatility period, [385-402] normal volatility, [388] sharp deep trough, [405] sharp prominent peak, [408-420] low volatility period, [415-420] short downward trend, [415] sharp deep trough, [421-430] normal volatility, [421] sharp prominent peak, [431-452] sudden volatility spike, [431] sharp deep trough, [437] sharp prominent peak, [455] sharp deep trough, [459-464] high volatility period, [465-474] normal volatility, [473] sharp prominent peak, [479-489] low volatility period, [490-495] normal volatility, [490] sharp deep trough, [502] sharp prominent peak, [504-512] short upward trend, [513-518] high volatility period, [531-550] normal volatility, [531] sharp deep trough, [546] sharp prominent peak, [551-556] low volatility period, [563-590] normal volatility, [565] sharp deep trough, [584] sharp prominent peak, [592-606] normal volatility, [599] sharp deep trough, [605] sharp prominent peak, [612-618] high volatility period, [612] sharp deep trough, [618] sharp prominent peak, [619-624] normal volatility, [638-665] normal volatility, [640] sharp deep trough, [649] sharp prominent peak, [655] sharp deep trough, [668] sharp prominent peak, [675-695] normal volatility, [676] sharp deep trough, [686] sharp prominent peak, [697-719] normal volatility, [706] sharp deep trough, [725-730] low volatility period, [734] sharp prominent peak, [739-759] sudden volatility spike, [740] sharp deep trough, [750] sharp prominent peak, [760-770] normal volatility, [764] sharp deep trough, [773] sharp prominent peak, [777-808] low volatility period, [779] sharp deep trough, [784] sharp prominent peak, [809-816] normal volatility, [809] sharp deep trough, [816] sharp prominent peak, [821-828] sudden volatility spike, [822] sharp deep trough, [830] sharp prominent peak, [837-844] normal volatility, [843] sharp deep trough, [849] sharp prominent peak, [850-865] normal volatility, [867-872] normal volatility, [867] sharp deep trough, [880] sharp prominent peak, [883-896] normal volatility, [892] sharp deep trough, [897] sharp prominent peak, [898-904] short downward trend, [900-914] normal volatility, [902] sharp deep trough, [914] sharp prominent peak, [916-937] normal volatility, [924-929] short downward trend, [929] sharp deep trough, [935-941] short upward trend, [938] sharp prominent peak, [944] sharp deep trough, [949-957] normal volatility, [957] sharp prominent peak, [958-978] low volatility period, [982] sharp deep trough, [985-990] high volatility period, [991-996] normal volatility, [997] sharp prominent peak']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8b2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_sample = [2.7841883e-001,-1.6410568e-002,-1.2352019e-001,-9.9824528e-001,-9.7530022e-001,-9.6032199e-001,-9.9880719e-001,-9.7491437e-001,-9.5768622e-001,-9.4306751e-001,-5.5785126e-001,-8.1840869e-001,8.4930787e-001,6.8584458e-001,8.2263681e-001,-9.8193011e-001,-9.9999130e-001,-9.9978838e-001,-9.9840537e-001,-9.9915036e-001,-9.7786550e-001,-9.4822478e-001,-7.1489166e-001,-5.0093000e-001,-5.7097906e-001,6.1162716e-001,-3.2954862e-001,2.8421321e-001,2.8459454e-001,1.1570542e-001,-9.0962529e-002,2.9431041e-001,-2.8121057e-001,8.5988430e-002,-2.2152694e-002,-1.6656535e-002,-2.2064350e-001,-1.3428663e-002,-7.2691890e-002,5.7938169e-001,9.6656113e-001,-1.4155127e-001,1.0937881e-001,-9.9741134e-001,-9.8944741e-001,-9.3163868e-001,-9.9788359e-001,-9.8961366e-001,-9.3324040e-001,8.9206031e-001,-1.6134256e-001,1.2258573e-001,9.8452014e-001,-1.1489334e-001,1.0276411e-001,-3.8342955e-001,9.0782890e-001,-9.7058275e-001,-9.7850045e-001,-9.9918838e-001,-9.9002851e-001,-9.4168540e-001,-1.0000000e+000,-1.0000000e+000,-2.1049361e-001,-4.1005552e-001,4.1385634e-001,-4.1756716e-001,4.2132499e-001,-1.9635929e-001,1.2534464e-001,-1.0556772e-001,1.0909013e-001,-8.3388211e-001,8.3427110e-001,-8.3418438e-001,8.3046390e-001,-8.3128389e-001,-8.6571108e-001,9.7438562e-001,7.4006709e-002,5.7711041e-003,2.9376633e-002,-9.9554814e-001,-9.8106363e-001,-9.9184570e-001,-9.9563201e-001,-9.7893801e-001,-9.9127664e-001,-9.9454467e-001,-9.7906823e-001,-9.9225735e-001,9.9257710e-001,9.9180836e-001,9.8853913e-001,-9.9139374e-001,-9.9995974e-001,-9.9963956e-001,-9.9984538e-001,-9.9386273e-001,-9.7943511e-001,-9.9338380e-001,-8.7509640e-001,-6.5536210e-001,-7.6738085e-001,4.8966215e-001,7.0997076e-002,3.6271450e-001,5.2730342e-001,1.4939565e-001,6.2925097e-002,3.7049343e-001,4.1354814e-001,1.2221568e-001,1.8061304e-001,4.7423999e-002,1.6657268e-001,-2.0877218e-001,8.4103799e-002,-2.6855390e-001,-1.6111620e-002,-8.3893777e-002,1.0058429e-001,-9.8311996e-001,-9.8904580e-001,-9.8912123e-001,-9.8689045e-001,-9.8903796e-001,-9.8918458e-001,-8.6490382e-001,-9.5356049e-001,-7.4587000e-001,8.3372106e-001,9.0810964e-001,8.2893499e-001,-9.8061310e-001,-9.9975577e-001,-9.9989731e-001,-9.9982242e-001,-9.9283276e-001,-9.8934472e-001,-9.9024019e-001,7.4693560e-003,-5.3115659e-001,-1.7744455e-001,-3.8768063e-001,1.7913763e-001,2.1078900e-001,-1.4025958e-001,-4.7031809e-002,-6.4949068e-002,1.1768661e-001,8.1691287e-002,4.2364040e-002,-1.4992836e-001,2.9261893e-001,-1.4942935e-001,4.6721243e-002,-2.5692940e-001,1.6939480e-001,-1.1050283e-001,-4.4818731e-002,-5.9242822e-002,-9.8987256e-001,-9.9729260e-001,-9.9385100e-001,-9.8987620e-001,-9.9749168e-001,-9.9377834e-001,-9.9194685e-001,-9.9771714e-001,-9.9492085e-001,9.9048601e-001,9.9712219e-001,9.9450312e-001,-9.9529844e-001,-9.9990775e-001,-9.9998972e-001,-9.9994591e-001,-9.9074179e-001,-9.9730134e-001,-9.9380781e-001,-6.0094453e-001,-7.4824724e-001,-6.0893213e-001,-1.9330757e-001,-6.7406458e-002,1.8561907e-001,4.1521811e-002,7.2352549e-002,-3.5377727e-002,1.7760636e-001,2.7498054e-002,1.8270272e-001,-1.6745740e-001,2.5325103e-001,1.3233386e-001,2.9385535e-001,-1.8075169e-002,-3.4333678e-001,-9.7928915e-001,-9.7605707e-001,-9.7824725e-001,-9.7871147e-001,-9.9533294e-001,-9.7928915e-001,-9.9948803e-001,-9.8124826e-001,-4.4187611e-001,8.1568632e-002,-1.0936606e-001,3.1175771e-001,-4.1167480e-001,-9.7928915e-001,-9.7605707e-001,-9.7824725e-001,-9.7871147e-001,-9.9533294e-001,-9.7928915e-001,-9.9948803e-001,-9.8124826e-001,-4.4187611e-001,8.1568632e-002,-1.0936606e-001,3.1175771e-001,-4.1167480e-001,-9.9125349e-001,-9.9169441e-001,-9.9271603e-001,-9.8866062e-001,-9.9120847e-001,-9.9125349e-001,-9.9984540e-001,-9.9348508e-001,-8.1992830e-001,4.5881205e-001,-2.4494134e-001,5.6139272e-002,-4.5834568e-001,-9.8068314e-001,-9.8375419e-001,-9.8200270e-001,-9.8471460e-001,-9.9155366e-001,-9.8068314e-001,-9.9972466e-001,-9.8285681e-001,-1.9289906e-001,-2.2531738e-001,-1.7059623e-002,1.5577724e-001,8.2575208e-002,-9.9512320e-001,-9.9610164e-001,-9.9583855e-001,-9.9654485e-001,-9.9200604e-001,-9.9512320e-001,-9.9996983e-001,-9.9481921e-001,-7.3072160e-001,2.0933413e-001,-1.7811256e-001,-1.0308433e-001,-4.3823965e-002,-9.9745072e-001,-9.7685173e-001,-9.7352267e-001,-9.9868026e-001,-9.7492981e-001,-9.5543811e-001,-9.9788967e-001,-9.7692389e-001,-9.6837677e-001,-9.9937173e-001,-9.7377026e-001,-9.4877678e-001,-9.9828058e-001,-9.9272090e-001,-9.8951355e-001,-9.8581162e-001,-9.9999084e-001,-9.9944988e-001,-9.9856912e-001,-9.9486488e-001,-9.8078362e-001,-9.8577466e-001,-1.0000000e+000,-9.0474776e-001,-7.5840851e-001,9.6774194e-002,-1.0000000e+000,-1.0000000e+000,2.7130855e-001,4.2863639e-002,-1.4309755e-002,-6.9254090e-001,-9.5404703e-001,-4.9709103e-002,-3.3197386e-001,5.6675367e-002,-2.8900144e-001,-9.9999619e-001,-9.9998175e-001,-9.9994400e-001,-9.9996988e-001,-9.9991885e-001,-9.9986573e-001,-9.9996507e-001,-9.9999945e-001,-9.9999394e-001,-9.9994898e-001,-9.9991401e-001,-9.9997661e-001,-9.9999213e-001,-9.9994590e-001,-9.9941662e-001,-9.9981329e-001,-9.9956858e-001,-9.9987368e-001,-9.9954892e-001,-9.9973714e-001,-9.9956575e-001,-9.9990532e-001,-9.9947352e-001,-9.9955418e-001,-9.9960203e-001,-9.9969530e-001,-9.9944422e-001,-9.9980416e-001,-9.9823460e-001,-9.9976916e-001,-9.9969223e-001,-9.9987487e-001,-9.9966565e-001,-9.9944828e-001,-9.9893018e-001,-9.9875435e-001,-9.9854556e-001,-9.9979176e-001,-9.9963116e-001,-9.9887752e-001,-9.9855336e-001,-9.9982213e-001,-9.9503222e-001,-9.8131147e-001,-9.8973975e-001,-9.9665235e-001,-9.8208394e-001,-9.9262682e-001,-9.9497670e-001,-9.8292946e-001,-9.9164143e-001,-9.9742453e-001,-9.8492321e-001,-9.9318704e-001,-9.9791682e-001,-9.8251860e-001,-9.8683843e-001,-9.8985094e-001,-9.9995965e-001,-9.9963962e-001,-9.9984664e-001,-9.9284336e-001,-9.8522065e-001,-9.9104933e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-3.2000000e-001,-1.2000000e-001,-3.2000000e-001,6.0851352e-001,-5.3675613e-002,6.3148268e-002,-6.3030495e-001,-9.1039449e-001,-4.1442354e-001,-8.5058640e-001,-6.5553468e-001,-9.1598691e-001,-9.9999635e-001,-9.9997967e-001,-9.9994892e-001,-9.9996834e-001,-9.9991010e-001,-9.9981369e-001,-9.9992027e-001,-9.9996071e-001,-9.9998672e-001,-9.9995600e-001,-9.9987671e-001,-9.9991409e-001,-9.9997443e-001,-9.9990582e-001,-9.9986103e-001,-9.9982717e-001,-9.9945649e-001,-9.9983029e-001,-9.9960932e-001,-9.9968546e-001,-9.9957615e-001,-9.9993695e-001,-9.9981738e-001,-9.9953247e-001,-9.9959516e-001,-9.9962567e-001,-9.9962988e-001,-9.9975933e-001,-9.9985891e-001,-9.9984650e-001,-9.9979487e-001,-9.9980092e-001,-9.9981932e-001,-9.9976916e-001,-9.9963701e-001,-9.9995450e-001,-9.9985190e-001,-9.9982733e-001,-9.9980005e-001,-9.9965102e-001,-9.9983501e-001,-9.9982668e-001,-9.7738671e-001,-9.9253003e-001,-9.8960578e-001,-9.8490434e-001,-9.8716807e-001,-9.8978468e-001,-9.7936121e-001,-9.9183683e-001,-9.8796514e-001,-9.8735382e-001,-9.8478644e-001,-9.9015077e-001,-9.8689184e-001,-9.9905355e-001,-9.9441373e-001,-9.8686870e-001,-9.9982491e-001,-9.9991146e-001,-9.9989205e-001,-9.8709935e-001,-9.9556375e-001,-9.8725448e-001,-6.1111189e-001,-7.6460301e-001,-7.5107966e-001,-1.0000000e+000,-1.0000000e+000,-1.0000000e+000,-4.8167435e-002,-4.0160791e-001,-6.8178329e-002,-4.5855331e-001,-7.9701355e-001,3.8756889e-001,1.4866483e-001,-1.5690927e-001,-4.5177589e-001,-9.9985087e-001,-9.9979432e-001,-9.9991309e-001,-9.9991816e-001,-9.9989636e-001,-9.9988528e-001,-9.9978419e-001,-9.9978237e-001,-9.9982986e-001,-9.9989878e-001,-9.9988283e-001,-9.9978339e-001,-9.9982832e-001,-9.9990802e-001,-9.9985638e-001,-9.9998846e-001,-9.9999570e-001,-9.9999416e-001,-9.9998608e-001,-9.9998455e-001,-9.9998002e-001,-9.9999002e-001,-9.9989660e-001,-9.9999447e-001,-9.9998604e-001,-9.9998167e-001,-9.9990259e-001,-9.9999165e-001,-9.9990889e-001,-9.9995940e-001,-9.9992807e-001,-9.9996632e-001,-9.9998549e-001,-9.9992637e-001,-9.9996147e-001,-9.9998312e-001,-9.9990171e-001,-9.9991776e-001,-9.9997539e-001,-9.9997110e-001,-9.9989434e-001,-9.9997104e-001,-9.8085662e-001,-9.7586576e-001,-9.7577688e-001,-9.7822635e-001,-9.8691082e-001,-9.8085662e-001,-9.9947194e-001,-9.8447923e-001,-8.1667357e-001,-1.0000000e+000,-4.4149887e-002,-1.2204037e-001,-4.4952188e-001,-9.9033549e-001,-9.9196029e-001,-9.8973198e-001,-9.9448884e-001,-9.8954882e-001,-9.9033549e-001,-9.9986688e-001,-9.9113389e-001,-1.0000000e+000,-8.4126984e-001,5.3206052e-001,-6.2487099e-001,-9.0015998e-001,-9.8829555e-001,-9.8332192e-001,-9.8265928e-001,-9.8632076e-001,-9.9182878e-001,-9.8829555e-001,-9.9981120e-001,-9.9397851e-001,-7.2068300e-001,-9.4871795e-001,-2.7195846e-001,-3.3631041e-001,-7.2001508e-001,-9.9585386e-001,-9.9639947e-001,-9.9544209e-001,-9.9686602e-001,-9.9443965e-001,-9.9585386e-001,-9.9998065e-001,-9.9454373e-001,-1.0000000e+000,-1.0000000e+000,1.5807454e-001,-5.9505094e-001,-8.6149931e-001,5.3476955e-002,-7.4345661e-003,-7.3262621e-001,7.0351059e-001,-8.4478760e-001,1.8028889e-001,-5.4316717e-002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1d565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list har_sample to 1,len tensor\n",
    "import torch\n",
    "har_tensor = torch.tensor(har_sample).unsqueeze(0)  # Shape: [1, len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b1c8b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 561])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "har_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a9b4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from hierarchical_event_system import HierarchicalEventDataset\n",
    "\n",
    "# Load HAR data\n",
    "train_data = torch.load('/home/AD/sachith/ACL_LITES/LITES/data/HAR/train.pt')\n",
    "X_train = train_data['samples']  # [N, 9, 128]\n",
    "\n",
    "# Reshape: treat each channel as separate univariate sequence\n",
    "X_univariate = X_train.reshape(-1, 128)\n",
    "\n",
    "# Shuffle\n",
    "indices = torch.randperm(X_univariate.shape[0])\n",
    "X_shuffled = X_univariate[indices].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7cc58e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: torch.Size([5881, 9, 128])\n",
      "Univariate: torch.Size([52929, 128])\n",
      "Shuffled: torch.Size([52929, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {X_train.shape}\")\n",
    "print(f\"Univariate: {X_univariate.shape}\")\n",
    "print(f\"Shuffled: {X_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2204b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 52929\n",
      "Length: 128\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 6774912 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/52929...\n",
      "      Processing sequence 50/52929...\n",
      "      Processing sequence 100/52929...\n",
      "      Processing sequence 150/52929...\n",
      "      Processing sequence 200/52929...\n",
      "      Processing sequence 250/52929...\n",
      "      Processing sequence 300/52929...\n",
      "      Processing sequence 350/52929...\n",
      "      Processing sequence 400/52929...\n",
      "      Processing sequence 450/52929...\n",
      "      Processing sequence 500/52929...\n",
      "      Processing sequence 550/52929...\n",
      "      Processing sequence 600/52929...\n",
      "      Processing sequence 650/52929...\n",
      "      Processing sequence 700/52929...\n",
      "      Processing sequence 750/52929...\n",
      "      Processing sequence 800/52929...\n",
      "      Processing sequence 850/52929...\n",
      "      Processing sequence 900/52929...\n",
      "      Processing sequence 950/52929...\n",
      "      Processing sequence 1000/52929...\n",
      "      Processing sequence 1050/52929...\n",
      "      Processing sequence 1100/52929...\n",
      "      Processing sequence 1150/52929...\n",
      "      Processing sequence 1200/52929...\n",
      "      Processing sequence 1250/52929...\n",
      "      Processing sequence 1300/52929...\n",
      "      Processing sequence 1350/52929...\n",
      "      Processing sequence 1400/52929...\n",
      "      Processing sequence 1450/52929...\n",
      "      Processing sequence 1500/52929...\n",
      "      Processing sequence 1550/52929...\n",
      "      Processing sequence 1600/52929...\n",
      "      Processing sequence 1650/52929...\n",
      "      Processing sequence 1700/52929...\n",
      "      Processing sequence 1750/52929...\n",
      "      Processing sequence 1800/52929...\n",
      "      Processing sequence 1850/52929...\n",
      "      Processing sequence 1900/52929...\n",
      "      Processing sequence 1950/52929...\n",
      "      Processing sequence 2000/52929...\n",
      "      Processing sequence 2050/52929...\n",
      "      Processing sequence 2100/52929...\n",
      "      Processing sequence 2150/52929...\n",
      "      Processing sequence 2200/52929...\n",
      "      Processing sequence 2250/52929...\n",
      "      Processing sequence 2300/52929...\n",
      "      Processing sequence 2350/52929...\n",
      "      Processing sequence 2400/52929...\n",
      "      Processing sequence 2450/52929...\n",
      "      Processing sequence 2500/52929...\n",
      "      Processing sequence 2550/52929...\n",
      "      Processing sequence 2600/52929...\n",
      "      Processing sequence 2650/52929...\n",
      "      Processing sequence 2700/52929...\n",
      "      Processing sequence 2750/52929...\n",
      "      Processing sequence 2800/52929...\n",
      "      Processing sequence 2850/52929...\n",
      "      Processing sequence 2900/52929...\n",
      "      Processing sequence 2950/52929...\n",
      "      Processing sequence 3000/52929...\n",
      "      Processing sequence 3050/52929...\n",
      "      Processing sequence 3100/52929...\n",
      "      Processing sequence 3150/52929...\n",
      "      Processing sequence 3200/52929...\n",
      "      Processing sequence 3250/52929...\n",
      "      Processing sequence 3300/52929...\n",
      "      Processing sequence 3350/52929...\n",
      "      Processing sequence 3400/52929...\n",
      "      Processing sequence 3450/52929...\n",
      "      Processing sequence 3500/52929...\n",
      "      Processing sequence 3550/52929...\n",
      "      Processing sequence 3600/52929...\n",
      "      Processing sequence 3650/52929...\n",
      "      Processing sequence 3700/52929...\n",
      "      Processing sequence 3750/52929...\n",
      "      Processing sequence 3800/52929...\n",
      "      Processing sequence 3850/52929...\n",
      "      Processing sequence 3900/52929...\n",
      "      Processing sequence 3950/52929...\n",
      "      Processing sequence 4000/52929...\n",
      "      Processing sequence 4050/52929...\n",
      "      Processing sequence 4100/52929...\n",
      "      Processing sequence 4150/52929...\n",
      "      Processing sequence 4200/52929...\n",
      "      Processing sequence 4250/52929...\n",
      "      Processing sequence 4300/52929...\n",
      "      Processing sequence 4350/52929...\n",
      "      Processing sequence 4400/52929...\n",
      "      Processing sequence 4450/52929...\n",
      "      Processing sequence 4500/52929...\n",
      "      Processing sequence 4550/52929...\n",
      "      Processing sequence 4600/52929...\n",
      "      Processing sequence 4650/52929...\n",
      "      Processing sequence 4700/52929...\n",
      "      Processing sequence 4750/52929...\n",
      "      Processing sequence 4800/52929...\n",
      "      Processing sequence 4850/52929...\n",
      "      Processing sequence 4900/52929...\n",
      "      Processing sequence 4950/52929...\n",
      "      Processing sequence 5000/52929...\n",
      "      Processing sequence 5050/52929...\n",
      "      Processing sequence 5100/52929...\n",
      "      Processing sequence 5150/52929...\n",
      "      Processing sequence 5200/52929...\n",
      "      Processing sequence 5250/52929...\n",
      "      Processing sequence 5300/52929...\n",
      "      Processing sequence 5350/52929...\n",
      "      Processing sequence 5400/52929...\n",
      "      Processing sequence 5450/52929...\n",
      "      Processing sequence 5500/52929...\n",
      "      Processing sequence 5550/52929...\n",
      "      Processing sequence 5600/52929...\n",
      "      Processing sequence 5650/52929...\n",
      "      Processing sequence 5700/52929...\n",
      "      Processing sequence 5750/52929...\n",
      "      Processing sequence 5800/52929...\n",
      "      Processing sequence 5850/52929...\n",
      "      Processing sequence 5900/52929...\n",
      "      Processing sequence 5950/52929...\n",
      "      Processing sequence 6000/52929...\n",
      "      Processing sequence 6050/52929...\n",
      "      Processing sequence 6100/52929...\n",
      "      Processing sequence 6150/52929...\n",
      "      Processing sequence 6200/52929...\n",
      "      Processing sequence 6250/52929...\n",
      "      Processing sequence 6300/52929...\n",
      "      Processing sequence 6350/52929...\n",
      "      Processing sequence 6400/52929...\n",
      "      Processing sequence 6450/52929...\n",
      "      Processing sequence 6500/52929...\n",
      "      Processing sequence 6550/52929...\n",
      "      Processing sequence 6600/52929...\n",
      "      Processing sequence 6650/52929...\n",
      "      Processing sequence 6700/52929...\n",
      "      Processing sequence 6750/52929...\n",
      "      Processing sequence 6800/52929...\n",
      "      Processing sequence 6850/52929...\n",
      "      Processing sequence 6900/52929...\n",
      "      Processing sequence 6950/52929...\n",
      "      Processing sequence 7000/52929...\n",
      "      Processing sequence 7050/52929...\n",
      "      Processing sequence 7100/52929...\n",
      "      Processing sequence 7150/52929...\n",
      "      Processing sequence 7200/52929...\n",
      "      Processing sequence 7250/52929...\n",
      "      Processing sequence 7300/52929...\n",
      "      Processing sequence 7350/52929...\n",
      "      Processing sequence 7400/52929...\n",
      "      Processing sequence 7450/52929...\n",
      "      Processing sequence 7500/52929...\n",
      "      Processing sequence 7550/52929...\n",
      "      Processing sequence 7600/52929...\n",
      "      Processing sequence 7650/52929...\n",
      "      Processing sequence 7700/52929...\n",
      "      Processing sequence 7750/52929...\n",
      "      Processing sequence 7800/52929...\n",
      "      Processing sequence 7850/52929...\n",
      "      Processing sequence 7900/52929...\n",
      "      Processing sequence 7950/52929...\n",
      "      Processing sequence 8000/52929...\n",
      "      Processing sequence 8050/52929...\n",
      "      Processing sequence 8100/52929...\n",
      "      Processing sequence 8150/52929...\n",
      "      Processing sequence 8200/52929...\n",
      "      Processing sequence 8250/52929...\n",
      "      Processing sequence 8300/52929...\n",
      "      Processing sequence 8350/52929...\n",
      "      Processing sequence 8400/52929...\n",
      "      Processing sequence 8450/52929...\n",
      "      Processing sequence 8500/52929...\n",
      "      Processing sequence 8550/52929...\n",
      "      Processing sequence 8600/52929...\n",
      "      Processing sequence 8650/52929...\n",
      "      Processing sequence 8700/52929...\n",
      "      Processing sequence 8750/52929...\n",
      "      Processing sequence 8800/52929...\n",
      "      Processing sequence 8850/52929...\n",
      "      Processing sequence 8900/52929...\n",
      "      Processing sequence 8950/52929...\n",
      "      Processing sequence 9000/52929...\n",
      "      Processing sequence 9050/52929...\n",
      "      Processing sequence 9100/52929...\n",
      "      Processing sequence 9150/52929...\n",
      "      Processing sequence 9200/52929...\n",
      "      Processing sequence 9250/52929...\n",
      "      Processing sequence 9300/52929...\n",
      "      Processing sequence 9350/52929...\n",
      "      Processing sequence 9400/52929...\n",
      "      Processing sequence 9450/52929...\n",
      "      Processing sequence 9500/52929...\n",
      "      Processing sequence 9550/52929...\n",
      "      Processing sequence 9600/52929...\n",
      "      Processing sequence 9650/52929...\n",
      "      Processing sequence 9700/52929...\n",
      "      Processing sequence 9750/52929...\n",
      "      Processing sequence 9800/52929...\n",
      "      Processing sequence 9850/52929...\n",
      "      Processing sequence 9900/52929...\n",
      "      Processing sequence 9950/52929...\n",
      "      Processing sequence 10000/52929...\n",
      "      Processing sequence 10050/52929...\n",
      "      Processing sequence 10100/52929...\n",
      "      Processing sequence 10150/52929...\n",
      "      Processing sequence 10200/52929...\n",
      "      Processing sequence 10250/52929...\n",
      "      Processing sequence 10300/52929...\n",
      "      Processing sequence 10350/52929...\n",
      "      Processing sequence 10400/52929...\n",
      "      Processing sequence 10450/52929...\n",
      "      Processing sequence 10500/52929...\n",
      "      Processing sequence 10550/52929...\n",
      "      Processing sequence 10600/52929...\n",
      "      Processing sequence 10650/52929...\n",
      "      Processing sequence 10700/52929...\n",
      "      Processing sequence 10750/52929...\n",
      "      Processing sequence 10800/52929...\n",
      "      Processing sequence 10850/52929...\n",
      "      Processing sequence 10900/52929...\n",
      "      Processing sequence 10950/52929...\n",
      "      Processing sequence 11000/52929...\n",
      "      Processing sequence 11050/52929...\n",
      "      Processing sequence 11100/52929...\n",
      "      Processing sequence 11150/52929...\n",
      "      Processing sequence 11200/52929...\n",
      "      Processing sequence 11250/52929...\n",
      "      Processing sequence 11300/52929...\n",
      "      Processing sequence 11350/52929...\n",
      "      Processing sequence 11400/52929...\n",
      "      Processing sequence 11450/52929...\n",
      "      Processing sequence 11500/52929...\n",
      "      Processing sequence 11550/52929...\n",
      "      Processing sequence 11600/52929...\n",
      "      Processing sequence 11650/52929...\n",
      "      Processing sequence 11700/52929...\n",
      "      Processing sequence 11750/52929...\n",
      "      Processing sequence 11800/52929...\n",
      "      Processing sequence 11850/52929...\n",
      "      Processing sequence 11900/52929...\n",
      "      Processing sequence 11950/52929...\n",
      "      Processing sequence 12000/52929...\n",
      "      Processing sequence 12050/52929...\n",
      "      Processing sequence 12100/52929...\n",
      "      Processing sequence 12150/52929...\n",
      "      Processing sequence 12200/52929...\n",
      "      Processing sequence 12250/52929...\n",
      "      Processing sequence 12300/52929...\n",
      "      Processing sequence 12350/52929...\n",
      "      Processing sequence 12400/52929...\n",
      "      Processing sequence 12450/52929...\n",
      "      Processing sequence 12500/52929...\n",
      "      Processing sequence 12550/52929...\n",
      "      Processing sequence 12600/52929...\n",
      "      Processing sequence 12650/52929...\n",
      "      Processing sequence 12700/52929...\n",
      "      Processing sequence 12750/52929...\n",
      "      Processing sequence 12800/52929...\n",
      "      Processing sequence 12850/52929...\n",
      "      Processing sequence 12900/52929...\n",
      "      Processing sequence 12950/52929...\n",
      "      Processing sequence 13000/52929...\n",
      "      Processing sequence 13050/52929...\n",
      "      Processing sequence 13100/52929...\n",
      "      Processing sequence 13150/52929...\n",
      "      Processing sequence 13200/52929...\n",
      "      Processing sequence 13250/52929...\n",
      "      Processing sequence 13300/52929...\n",
      "      Processing sequence 13350/52929...\n",
      "      Processing sequence 13400/52929...\n",
      "      Processing sequence 13450/52929...\n",
      "      Processing sequence 13500/52929...\n",
      "      Processing sequence 13550/52929...\n",
      "      Processing sequence 13600/52929...\n",
      "      Processing sequence 13650/52929...\n",
      "      Processing sequence 13700/52929...\n",
      "      Processing sequence 13750/52929...\n",
      "      Processing sequence 13800/52929...\n",
      "      Processing sequence 13850/52929...\n",
      "      Processing sequence 13900/52929...\n",
      "      Processing sequence 13950/52929...\n",
      "      Processing sequence 14000/52929...\n",
      "      Processing sequence 14050/52929...\n",
      "      Processing sequence 14100/52929...\n",
      "      Processing sequence 14150/52929...\n",
      "      Processing sequence 14200/52929...\n",
      "      Processing sequence 14250/52929...\n",
      "      Processing sequence 14300/52929...\n",
      "      Processing sequence 14350/52929...\n",
      "      Processing sequence 14400/52929...\n",
      "      Processing sequence 14450/52929...\n",
      "      Processing sequence 14500/52929...\n",
      "      Processing sequence 14550/52929...\n",
      "      Processing sequence 14600/52929...\n",
      "      Processing sequence 14650/52929...\n",
      "      Processing sequence 14700/52929...\n",
      "      Processing sequence 14750/52929...\n",
      "      Processing sequence 14800/52929...\n",
      "      Processing sequence 14850/52929...\n",
      "      Processing sequence 14900/52929...\n",
      "      Processing sequence 14950/52929...\n",
      "      Processing sequence 15000/52929...\n",
      "      Processing sequence 15050/52929...\n",
      "      Processing sequence 15100/52929...\n",
      "      Processing sequence 15150/52929...\n",
      "      Processing sequence 15200/52929...\n",
      "      Processing sequence 15250/52929...\n",
      "      Processing sequence 15300/52929...\n",
      "      Processing sequence 15350/52929...\n",
      "      Processing sequence 15400/52929...\n",
      "      Processing sequence 15450/52929...\n",
      "      Processing sequence 15500/52929...\n",
      "      Processing sequence 15550/52929...\n",
      "      Processing sequence 15600/52929...\n",
      "      Processing sequence 15650/52929...\n",
      "      Processing sequence 15700/52929...\n",
      "      Processing sequence 15750/52929...\n",
      "      Processing sequence 15800/52929...\n",
      "      Processing sequence 15850/52929...\n",
      "      Processing sequence 15900/52929...\n",
      "      Processing sequence 15950/52929...\n",
      "      Processing sequence 16000/52929...\n",
      "      Processing sequence 16050/52929...\n",
      "      Processing sequence 16100/52929...\n",
      "      Processing sequence 16150/52929...\n",
      "      Processing sequence 16200/52929...\n",
      "      Processing sequence 16250/52929...\n",
      "      Processing sequence 16300/52929...\n",
      "      Processing sequence 16350/52929...\n",
      "      Processing sequence 16400/52929...\n",
      "      Processing sequence 16450/52929...\n",
      "      Processing sequence 16500/52929...\n",
      "      Processing sequence 16550/52929...\n",
      "      Processing sequence 16600/52929...\n",
      "      Processing sequence 16650/52929...\n",
      "      Processing sequence 16700/52929...\n",
      "      Processing sequence 16750/52929...\n",
      "      Processing sequence 16800/52929...\n",
      "      Processing sequence 16850/52929...\n",
      "      Processing sequence 16900/52929...\n",
      "      Processing sequence 16950/52929...\n",
      "      Processing sequence 17000/52929...\n",
      "      Processing sequence 17050/52929...\n",
      "      Processing sequence 17100/52929...\n",
      "      Processing sequence 17150/52929...\n",
      "      Processing sequence 17200/52929...\n",
      "      Processing sequence 17250/52929...\n",
      "      Processing sequence 17300/52929...\n",
      "      Processing sequence 17350/52929...\n",
      "      Processing sequence 17400/52929...\n",
      "      Processing sequence 17450/52929...\n",
      "      Processing sequence 17500/52929...\n",
      "      Processing sequence 17550/52929...\n",
      "      Processing sequence 17600/52929...\n",
      "      Processing sequence 17650/52929...\n",
      "      Processing sequence 17700/52929...\n",
      "      Processing sequence 17750/52929...\n",
      "      Processing sequence 17800/52929...\n",
      "      Processing sequence 17850/52929...\n",
      "      Processing sequence 17900/52929...\n",
      "      Processing sequence 17950/52929...\n",
      "      Processing sequence 18000/52929...\n",
      "      Processing sequence 18050/52929...\n",
      "      Processing sequence 18100/52929...\n",
      "      Processing sequence 18150/52929...\n",
      "      Processing sequence 18200/52929...\n",
      "      Processing sequence 18250/52929...\n",
      "      Processing sequence 18300/52929...\n",
      "      Processing sequence 18350/52929...\n",
      "      Processing sequence 18400/52929...\n",
      "      Processing sequence 18450/52929...\n",
      "      Processing sequence 18500/52929...\n",
      "      Processing sequence 18550/52929...\n",
      "      Processing sequence 18600/52929...\n",
      "      Processing sequence 18650/52929...\n",
      "      Processing sequence 18700/52929...\n",
      "      Processing sequence 18750/52929...\n",
      "      Processing sequence 18800/52929...\n",
      "      Processing sequence 18850/52929...\n",
      "      Processing sequence 18900/52929...\n",
      "      Processing sequence 18950/52929...\n",
      "      Processing sequence 19000/52929...\n",
      "      Processing sequence 19050/52929...\n",
      "      Processing sequence 19100/52929...\n",
      "      Processing sequence 19150/52929...\n",
      "      Processing sequence 19200/52929...\n",
      "      Processing sequence 19250/52929...\n",
      "      Processing sequence 19300/52929...\n",
      "      Processing sequence 19350/52929...\n",
      "      Processing sequence 19400/52929...\n",
      "      Processing sequence 19450/52929...\n",
      "      Processing sequence 19500/52929...\n",
      "      Processing sequence 19550/52929...\n",
      "      Processing sequence 19600/52929...\n",
      "      Processing sequence 19650/52929...\n",
      "      Processing sequence 19700/52929...\n",
      "      Processing sequence 19750/52929...\n",
      "      Processing sequence 19800/52929...\n",
      "      Processing sequence 19850/52929...\n",
      "      Processing sequence 19900/52929...\n",
      "      Processing sequence 19950/52929...\n",
      "      Processing sequence 20000/52929...\n",
      "      Processing sequence 20050/52929...\n",
      "      Processing sequence 20100/52929...\n",
      "      Processing sequence 20150/52929...\n",
      "      Processing sequence 20200/52929...\n",
      "      Processing sequence 20250/52929...\n",
      "      Processing sequence 20300/52929...\n",
      "      Processing sequence 20350/52929...\n",
      "      Processing sequence 20400/52929...\n",
      "      Processing sequence 20450/52929...\n",
      "      Processing sequence 20500/52929...\n",
      "      Processing sequence 20550/52929...\n",
      "      Processing sequence 20600/52929...\n",
      "      Processing sequence 20650/52929...\n",
      "      Processing sequence 20700/52929...\n",
      "      Processing sequence 20750/52929...\n",
      "      Processing sequence 20800/52929...\n",
      "      Processing sequence 20850/52929...\n",
      "      Processing sequence 20900/52929...\n",
      "      Processing sequence 20950/52929...\n",
      "      Processing sequence 21000/52929...\n",
      "      Processing sequence 21050/52929...\n",
      "      Processing sequence 21100/52929...\n",
      "      Processing sequence 21150/52929...\n",
      "      Processing sequence 21200/52929...\n",
      "      Processing sequence 21250/52929...\n",
      "      Processing sequence 21300/52929...\n",
      "      Processing sequence 21350/52929...\n",
      "      Processing sequence 21400/52929...\n",
      "      Processing sequence 21450/52929...\n",
      "      Processing sequence 21500/52929...\n",
      "      Processing sequence 21550/52929...\n",
      "      Processing sequence 21600/52929...\n",
      "      Processing sequence 21650/52929...\n",
      "      Processing sequence 21700/52929...\n",
      "      Processing sequence 21750/52929...\n",
      "      Processing sequence 21800/52929...\n",
      "      Processing sequence 21850/52929...\n",
      "      Processing sequence 21900/52929...\n",
      "      Processing sequence 21950/52929...\n",
      "      Processing sequence 22000/52929...\n",
      "      Processing sequence 22050/52929...\n",
      "      Processing sequence 22100/52929...\n",
      "      Processing sequence 22150/52929...\n",
      "      Processing sequence 22200/52929...\n",
      "      Processing sequence 22250/52929...\n",
      "      Processing sequence 22300/52929...\n",
      "      Processing sequence 22350/52929...\n",
      "      Processing sequence 22400/52929...\n",
      "      Processing sequence 22450/52929...\n",
      "      Processing sequence 22500/52929...\n",
      "      Processing sequence 22550/52929...\n",
      "      Processing sequence 22600/52929...\n",
      "      Processing sequence 22650/52929...\n",
      "      Processing sequence 22700/52929...\n",
      "      Processing sequence 22750/52929...\n",
      "      Processing sequence 22800/52929...\n",
      "      Processing sequence 22850/52929...\n",
      "      Processing sequence 22900/52929...\n",
      "      Processing sequence 22950/52929...\n",
      "      Processing sequence 23000/52929...\n",
      "      Processing sequence 23050/52929...\n",
      "      Processing sequence 23100/52929...\n",
      "      Processing sequence 23150/52929...\n",
      "      Processing sequence 23200/52929...\n",
      "      Processing sequence 23250/52929...\n",
      "      Processing sequence 23300/52929...\n",
      "      Processing sequence 23350/52929...\n",
      "      Processing sequence 23400/52929...\n",
      "      Processing sequence 23450/52929...\n",
      "      Processing sequence 23500/52929...\n",
      "      Processing sequence 23550/52929...\n",
      "      Processing sequence 23600/52929...\n",
      "      Processing sequence 23650/52929...\n",
      "      Processing sequence 23700/52929...\n",
      "      Processing sequence 23750/52929...\n",
      "      Processing sequence 23800/52929...\n",
      "      Processing sequence 23850/52929...\n",
      "      Processing sequence 23900/52929...\n",
      "      Processing sequence 23950/52929...\n",
      "      Processing sequence 24000/52929...\n",
      "      Processing sequence 24050/52929...\n",
      "      Processing sequence 24100/52929...\n",
      "      Processing sequence 24150/52929...\n",
      "      Processing sequence 24200/52929...\n",
      "      Processing sequence 24250/52929...\n",
      "      Processing sequence 24300/52929...\n",
      "      Processing sequence 24350/52929...\n",
      "      Processing sequence 24400/52929...\n",
      "      Processing sequence 24450/52929...\n",
      "      Processing sequence 24500/52929...\n",
      "      Processing sequence 24550/52929...\n",
      "      Processing sequence 24600/52929...\n",
      "      Processing sequence 24650/52929...\n",
      "      Processing sequence 24700/52929...\n",
      "      Processing sequence 24750/52929...\n",
      "      Processing sequence 24800/52929...\n",
      "      Processing sequence 24850/52929...\n",
      "      Processing sequence 24900/52929...\n",
      "      Processing sequence 24950/52929...\n",
      "      Processing sequence 25000/52929...\n",
      "      Processing sequence 25050/52929...\n",
      "      Processing sequence 25100/52929...\n",
      "      Processing sequence 25150/52929...\n",
      "      Processing sequence 25200/52929...\n",
      "      Processing sequence 25250/52929...\n",
      "      Processing sequence 25300/52929...\n",
      "      Processing sequence 25350/52929...\n",
      "      Processing sequence 25400/52929...\n",
      "      Processing sequence 25450/52929...\n",
      "      Processing sequence 25500/52929...\n",
      "      Processing sequence 25550/52929...\n",
      "      Processing sequence 25600/52929...\n",
      "      Processing sequence 25650/52929...\n",
      "      Processing sequence 25700/52929...\n",
      "      Processing sequence 25750/52929...\n",
      "      Processing sequence 25800/52929...\n",
      "      Processing sequence 25850/52929...\n",
      "      Processing sequence 25900/52929...\n",
      "      Processing sequence 25950/52929...\n",
      "      Processing sequence 26000/52929...\n",
      "      Processing sequence 26050/52929...\n",
      "      Processing sequence 26100/52929...\n",
      "      Processing sequence 26150/52929...\n",
      "      Processing sequence 26200/52929...\n",
      "      Processing sequence 26250/52929...\n",
      "      Processing sequence 26300/52929...\n",
      "      Processing sequence 26350/52929...\n",
      "      Processing sequence 26400/52929...\n",
      "      Processing sequence 26450/52929...\n",
      "      Processing sequence 26500/52929...\n",
      "      Processing sequence 26550/52929...\n",
      "      Processing sequence 26600/52929...\n",
      "      Processing sequence 26650/52929...\n",
      "      Processing sequence 26700/52929...\n",
      "      Processing sequence 26750/52929...\n",
      "      Processing sequence 26800/52929...\n",
      "      Processing sequence 26850/52929...\n",
      "      Processing sequence 26900/52929...\n",
      "      Processing sequence 26950/52929...\n",
      "      Processing sequence 27000/52929...\n",
      "      Processing sequence 27050/52929...\n",
      "      Processing sequence 27100/52929...\n",
      "      Processing sequence 27150/52929...\n",
      "      Processing sequence 27200/52929...\n",
      "      Processing sequence 27250/52929...\n",
      "      Processing sequence 27300/52929...\n",
      "      Processing sequence 27350/52929...\n",
      "      Processing sequence 27400/52929...\n",
      "      Processing sequence 27450/52929...\n",
      "      Processing sequence 27500/52929...\n",
      "      Processing sequence 27550/52929...\n",
      "      Processing sequence 27600/52929...\n",
      "      Processing sequence 27650/52929...\n",
      "      Processing sequence 27700/52929...\n",
      "      Processing sequence 27750/52929...\n",
      "      Processing sequence 27800/52929...\n",
      "      Processing sequence 27850/52929...\n",
      "      Processing sequence 27900/52929...\n",
      "      Processing sequence 27950/52929...\n",
      "      Processing sequence 28000/52929...\n",
      "      Processing sequence 28050/52929...\n",
      "      Processing sequence 28100/52929...\n",
      "      Processing sequence 28150/52929...\n",
      "      Processing sequence 28200/52929...\n",
      "      Processing sequence 28250/52929...\n",
      "      Processing sequence 28300/52929...\n",
      "      Processing sequence 28350/52929...\n",
      "      Processing sequence 28400/52929...\n",
      "      Processing sequence 28450/52929...\n",
      "      Processing sequence 28500/52929...\n",
      "      Processing sequence 28550/52929...\n",
      "      Processing sequence 28600/52929...\n",
      "      Processing sequence 28650/52929...\n",
      "      Processing sequence 28700/52929...\n",
      "      Processing sequence 28750/52929...\n",
      "      Processing sequence 28800/52929...\n",
      "      Processing sequence 28850/52929...\n",
      "      Processing sequence 28900/52929...\n",
      "      Processing sequence 28950/52929...\n",
      "      Processing sequence 29000/52929...\n",
      "      Processing sequence 29050/52929...\n",
      "      Processing sequence 29100/52929...\n",
      "      Processing sequence 29150/52929...\n",
      "      Processing sequence 29200/52929...\n",
      "      Processing sequence 29250/52929...\n",
      "      Processing sequence 29300/52929...\n",
      "      Processing sequence 29350/52929...\n",
      "      Processing sequence 29400/52929...\n",
      "      Processing sequence 29450/52929...\n",
      "      Processing sequence 29500/52929...\n",
      "      Processing sequence 29550/52929...\n",
      "      Processing sequence 29600/52929...\n",
      "      Processing sequence 29650/52929...\n",
      "      Processing sequence 29700/52929...\n",
      "      Processing sequence 29750/52929...\n",
      "      Processing sequence 29800/52929...\n",
      "      Processing sequence 29850/52929...\n",
      "      Processing sequence 29900/52929...\n",
      "      Processing sequence 29950/52929...\n",
      "      Processing sequence 30000/52929...\n",
      "      Processing sequence 30050/52929...\n",
      "      Processing sequence 30100/52929...\n",
      "      Processing sequence 30150/52929...\n",
      "      Processing sequence 30200/52929...\n",
      "      Processing sequence 30250/52929...\n",
      "      Processing sequence 30300/52929...\n",
      "      Processing sequence 30350/52929...\n",
      "      Processing sequence 30400/52929...\n",
      "      Processing sequence 30450/52929...\n",
      "      Processing sequence 30500/52929...\n",
      "      Processing sequence 30550/52929...\n",
      "      Processing sequence 30600/52929...\n",
      "      Processing sequence 30650/52929...\n",
      "      Processing sequence 30700/52929...\n",
      "      Processing sequence 30750/52929...\n",
      "      Processing sequence 30800/52929...\n",
      "      Processing sequence 30850/52929...\n",
      "      Processing sequence 30900/52929...\n",
      "      Processing sequence 30950/52929...\n",
      "      Processing sequence 31000/52929...\n",
      "      Processing sequence 31050/52929...\n",
      "      Processing sequence 31100/52929...\n",
      "      Processing sequence 31150/52929...\n",
      "      Processing sequence 31200/52929...\n",
      "      Processing sequence 31250/52929...\n",
      "      Processing sequence 31300/52929...\n",
      "      Processing sequence 31350/52929...\n",
      "      Processing sequence 31400/52929...\n",
      "      Processing sequence 31450/52929...\n",
      "      Processing sequence 31500/52929...\n",
      "      Processing sequence 31550/52929...\n",
      "      Processing sequence 31600/52929...\n",
      "      Processing sequence 31650/52929...\n",
      "      Processing sequence 31700/52929...\n",
      "      Processing sequence 31750/52929...\n",
      "      Processing sequence 31800/52929...\n",
      "      Processing sequence 31850/52929...\n",
      "      Processing sequence 31900/52929...\n",
      "      Processing sequence 31950/52929...\n",
      "      Processing sequence 32000/52929...\n",
      "      Processing sequence 32050/52929...\n",
      "      Processing sequence 32100/52929...\n",
      "      Processing sequence 32150/52929...\n",
      "      Processing sequence 32200/52929...\n",
      "      Processing sequence 32250/52929...\n",
      "      Processing sequence 32300/52929...\n",
      "      Processing sequence 32350/52929...\n",
      "      Processing sequence 32400/52929...\n",
      "      Processing sequence 32450/52929...\n",
      "      Processing sequence 32500/52929...\n",
      "      Processing sequence 32550/52929...\n",
      "      Processing sequence 32600/52929...\n",
      "      Processing sequence 32650/52929...\n",
      "      Processing sequence 32700/52929...\n",
      "      Processing sequence 32750/52929...\n",
      "      Processing sequence 32800/52929...\n",
      "      Processing sequence 32850/52929...\n",
      "      Processing sequence 32900/52929...\n",
      "      Processing sequence 32950/52929...\n",
      "      Processing sequence 33000/52929...\n",
      "      Processing sequence 33050/52929...\n",
      "      Processing sequence 33100/52929...\n",
      "      Processing sequence 33150/52929...\n",
      "      Processing sequence 33200/52929...\n",
      "      Processing sequence 33250/52929...\n",
      "      Processing sequence 33300/52929...\n",
      "      Processing sequence 33350/52929...\n",
      "      Processing sequence 33400/52929...\n",
      "      Processing sequence 33450/52929...\n",
      "      Processing sequence 33500/52929...\n",
      "      Processing sequence 33550/52929...\n",
      "      Processing sequence 33600/52929...\n",
      "      Processing sequence 33650/52929...\n",
      "      Processing sequence 33700/52929...\n",
      "      Processing sequence 33750/52929...\n",
      "      Processing sequence 33800/52929...\n",
      "      Processing sequence 33850/52929...\n",
      "      Processing sequence 33900/52929...\n",
      "      Processing sequence 33950/52929...\n",
      "      Processing sequence 34000/52929...\n",
      "      Processing sequence 34050/52929...\n",
      "      Processing sequence 34100/52929...\n",
      "      Processing sequence 34150/52929...\n",
      "      Processing sequence 34200/52929...\n",
      "      Processing sequence 34250/52929...\n",
      "      Processing sequence 34300/52929...\n",
      "      Processing sequence 34350/52929...\n",
      "      Processing sequence 34400/52929...\n",
      "      Processing sequence 34450/52929...\n",
      "      Processing sequence 34500/52929...\n",
      "      Processing sequence 34550/52929...\n",
      "      Processing sequence 34600/52929...\n",
      "      Processing sequence 34650/52929...\n",
      "      Processing sequence 34700/52929...\n",
      "      Processing sequence 34750/52929...\n",
      "      Processing sequence 34800/52929...\n",
      "      Processing sequence 34850/52929...\n",
      "      Processing sequence 34900/52929...\n",
      "      Processing sequence 34950/52929...\n",
      "      Processing sequence 35000/52929...\n",
      "      Processing sequence 35050/52929...\n",
      "      Processing sequence 35100/52929...\n",
      "      Processing sequence 35150/52929...\n",
      "      Processing sequence 35200/52929...\n",
      "      Processing sequence 35250/52929...\n",
      "      Processing sequence 35300/52929...\n",
      "      Processing sequence 35350/52929...\n",
      "      Processing sequence 35400/52929...\n",
      "      Processing sequence 35450/52929...\n",
      "      Processing sequence 35500/52929...\n",
      "      Processing sequence 35550/52929...\n",
      "      Processing sequence 35600/52929...\n",
      "      Processing sequence 35650/52929...\n",
      "      Processing sequence 35700/52929...\n",
      "      Processing sequence 35750/52929...\n",
      "      Processing sequence 35800/52929...\n",
      "      Processing sequence 35850/52929...\n",
      "      Processing sequence 35900/52929...\n",
      "      Processing sequence 35950/52929...\n",
      "      Processing sequence 36000/52929...\n",
      "      Processing sequence 36050/52929...\n",
      "      Processing sequence 36100/52929...\n",
      "      Processing sequence 36150/52929...\n",
      "      Processing sequence 36200/52929...\n",
      "      Processing sequence 36250/52929...\n",
      "      Processing sequence 36300/52929...\n",
      "      Processing sequence 36350/52929...\n",
      "      Processing sequence 36400/52929...\n",
      "      Processing sequence 36450/52929...\n",
      "      Processing sequence 36500/52929...\n",
      "      Processing sequence 36550/52929...\n",
      "      Processing sequence 36600/52929...\n",
      "      Processing sequence 36650/52929...\n",
      "      Processing sequence 36700/52929...\n",
      "      Processing sequence 36750/52929...\n",
      "      Processing sequence 36800/52929...\n",
      "      Processing sequence 36850/52929...\n",
      "      Processing sequence 36900/52929...\n",
      "      Processing sequence 36950/52929...\n",
      "      Processing sequence 37000/52929...\n",
      "      Processing sequence 37050/52929...\n",
      "      Processing sequence 37100/52929...\n",
      "      Processing sequence 37150/52929...\n",
      "      Processing sequence 37200/52929...\n",
      "      Processing sequence 37250/52929...\n",
      "      Processing sequence 37300/52929...\n",
      "      Processing sequence 37350/52929...\n",
      "      Processing sequence 37400/52929...\n",
      "      Processing sequence 37450/52929...\n",
      "      Processing sequence 37500/52929...\n",
      "      Processing sequence 37550/52929...\n",
      "      Processing sequence 37600/52929...\n",
      "      Processing sequence 37650/52929...\n",
      "      Processing sequence 37700/52929...\n",
      "      Processing sequence 37750/52929...\n",
      "      Processing sequence 37800/52929...\n",
      "      Processing sequence 37850/52929...\n",
      "      Processing sequence 37900/52929...\n",
      "      Processing sequence 37950/52929...\n",
      "      Processing sequence 38000/52929...\n",
      "      Processing sequence 38050/52929...\n",
      "      Processing sequence 38100/52929...\n",
      "      Processing sequence 38150/52929...\n",
      "      Processing sequence 38200/52929...\n",
      "      Processing sequence 38250/52929...\n",
      "      Processing sequence 38300/52929...\n",
      "      Processing sequence 38350/52929...\n",
      "      Processing sequence 38400/52929...\n",
      "      Processing sequence 38450/52929...\n",
      "      Processing sequence 38500/52929...\n",
      "      Processing sequence 38550/52929...\n",
      "      Processing sequence 38600/52929...\n",
      "      Processing sequence 38650/52929...\n",
      "      Processing sequence 38700/52929...\n",
      "      Processing sequence 38750/52929...\n",
      "      Processing sequence 38800/52929...\n",
      "      Processing sequence 38850/52929...\n",
      "      Processing sequence 38900/52929...\n",
      "      Processing sequence 38950/52929...\n",
      "      Processing sequence 39000/52929...\n",
      "      Processing sequence 39050/52929...\n",
      "      Processing sequence 39100/52929...\n",
      "      Processing sequence 39150/52929...\n",
      "      Processing sequence 39200/52929...\n",
      "      Processing sequence 39250/52929...\n",
      "      Processing sequence 39300/52929...\n",
      "      Processing sequence 39350/52929...\n",
      "      Processing sequence 39400/52929...\n",
      "      Processing sequence 39450/52929...\n",
      "      Processing sequence 39500/52929...\n",
      "      Processing sequence 39550/52929...\n",
      "      Processing sequence 39600/52929...\n",
      "      Processing sequence 39650/52929...\n",
      "      Processing sequence 39700/52929...\n",
      "      Processing sequence 39750/52929...\n",
      "      Processing sequence 39800/52929...\n",
      "      Processing sequence 39850/52929...\n",
      "      Processing sequence 39900/52929...\n",
      "      Processing sequence 39950/52929...\n",
      "      Processing sequence 40000/52929...\n",
      "      Processing sequence 40050/52929...\n",
      "      Processing sequence 40100/52929...\n",
      "      Processing sequence 40150/52929...\n",
      "      Processing sequence 40200/52929...\n",
      "      Processing sequence 40250/52929...\n",
      "      Processing sequence 40300/52929...\n",
      "      Processing sequence 40350/52929...\n",
      "      Processing sequence 40400/52929...\n",
      "      Processing sequence 40450/52929...\n",
      "      Processing sequence 40500/52929...\n",
      "      Processing sequence 40550/52929...\n",
      "      Processing sequence 40600/52929...\n",
      "      Processing sequence 40650/52929...\n",
      "      Processing sequence 40700/52929...\n",
      "      Processing sequence 40750/52929...\n",
      "      Processing sequence 40800/52929...\n",
      "      Processing sequence 40850/52929...\n",
      "      Processing sequence 40900/52929...\n",
      "      Processing sequence 40950/52929...\n",
      "      Processing sequence 41000/52929...\n",
      "      Processing sequence 41050/52929...\n",
      "      Processing sequence 41100/52929...\n",
      "      Processing sequence 41150/52929...\n",
      "      Processing sequence 41200/52929...\n",
      "      Processing sequence 41250/52929...\n",
      "      Processing sequence 41300/52929...\n",
      "      Processing sequence 41350/52929...\n",
      "      Processing sequence 41400/52929...\n",
      "      Processing sequence 41450/52929...\n",
      "      Processing sequence 41500/52929...\n",
      "      Processing sequence 41550/52929...\n",
      "      Processing sequence 41600/52929...\n",
      "      Processing sequence 41650/52929...\n",
      "      Processing sequence 41700/52929...\n",
      "      Processing sequence 41750/52929...\n",
      "      Processing sequence 41800/52929...\n",
      "      Processing sequence 41850/52929...\n",
      "      Processing sequence 41900/52929...\n",
      "      Processing sequence 41950/52929...\n",
      "      Processing sequence 42000/52929...\n",
      "      Processing sequence 42050/52929...\n",
      "      Processing sequence 42100/52929...\n",
      "      Processing sequence 42150/52929...\n",
      "      Processing sequence 42200/52929...\n",
      "      Processing sequence 42250/52929...\n",
      "      Processing sequence 42300/52929...\n",
      "      Processing sequence 42350/52929...\n",
      "      Processing sequence 42400/52929...\n",
      "      Processing sequence 42450/52929...\n",
      "      Processing sequence 42500/52929...\n",
      "      Processing sequence 42550/52929...\n",
      "      Processing sequence 42600/52929...\n",
      "      Processing sequence 42650/52929...\n",
      "      Processing sequence 42700/52929...\n",
      "      Processing sequence 42750/52929...\n",
      "      Processing sequence 42800/52929...\n",
      "      Processing sequence 42850/52929...\n",
      "      Processing sequence 42900/52929...\n",
      "      Processing sequence 42950/52929...\n",
      "      Processing sequence 43000/52929...\n",
      "      Processing sequence 43050/52929...\n",
      "      Processing sequence 43100/52929...\n",
      "      Processing sequence 43150/52929...\n",
      "      Processing sequence 43200/52929...\n",
      "      Processing sequence 43250/52929...\n",
      "      Processing sequence 43300/52929...\n",
      "      Processing sequence 43350/52929...\n",
      "      Processing sequence 43400/52929...\n",
      "      Processing sequence 43450/52929...\n",
      "      Processing sequence 43500/52929...\n",
      "      Processing sequence 43550/52929...\n",
      "      Processing sequence 43600/52929...\n",
      "      Processing sequence 43650/52929...\n",
      "      Processing sequence 43700/52929...\n",
      "      Processing sequence 43750/52929...\n",
      "      Processing sequence 43800/52929...\n",
      "      Processing sequence 43850/52929...\n",
      "      Processing sequence 43900/52929...\n",
      "      Processing sequence 43950/52929...\n",
      "      Processing sequence 44000/52929...\n",
      "      Processing sequence 44050/52929...\n",
      "      Processing sequence 44100/52929...\n",
      "      Processing sequence 44150/52929...\n",
      "      Processing sequence 44200/52929...\n",
      "      Processing sequence 44250/52929...\n",
      "      Processing sequence 44300/52929...\n",
      "      Processing sequence 44350/52929...\n",
      "      Processing sequence 44400/52929...\n",
      "      Processing sequence 44450/52929...\n",
      "      Processing sequence 44500/52929...\n",
      "      Processing sequence 44550/52929...\n",
      "      Processing sequence 44600/52929...\n",
      "      Processing sequence 44650/52929...\n",
      "      Processing sequence 44700/52929...\n",
      "      Processing sequence 44750/52929...\n",
      "      Processing sequence 44800/52929...\n",
      "      Processing sequence 44850/52929...\n",
      "      Processing sequence 44900/52929...\n",
      "      Processing sequence 44950/52929...\n",
      "      Processing sequence 45000/52929...\n",
      "      Processing sequence 45050/52929...\n",
      "      Processing sequence 45100/52929...\n",
      "      Processing sequence 45150/52929...\n",
      "      Processing sequence 45200/52929...\n",
      "      Processing sequence 45250/52929...\n",
      "      Processing sequence 45300/52929...\n",
      "      Processing sequence 45350/52929...\n",
      "      Processing sequence 45400/52929...\n",
      "      Processing sequence 45450/52929...\n",
      "      Processing sequence 45500/52929...\n",
      "      Processing sequence 45550/52929...\n",
      "      Processing sequence 45600/52929...\n",
      "      Processing sequence 45650/52929...\n",
      "      Processing sequence 45700/52929...\n",
      "      Processing sequence 45750/52929...\n",
      "      Processing sequence 45800/52929...\n",
      "      Processing sequence 45850/52929...\n",
      "      Processing sequence 45900/52929...\n",
      "      Processing sequence 45950/52929...\n",
      "      Processing sequence 46000/52929...\n",
      "      Processing sequence 46050/52929...\n",
      "      Processing sequence 46100/52929...\n",
      "      Processing sequence 46150/52929...\n",
      "      Processing sequence 46200/52929...\n",
      "      Processing sequence 46250/52929...\n",
      "      Processing sequence 46300/52929...\n",
      "      Processing sequence 46350/52929...\n",
      "      Processing sequence 46400/52929...\n",
      "      Processing sequence 46450/52929...\n",
      "      Processing sequence 46500/52929...\n",
      "      Processing sequence 46550/52929...\n",
      "      Processing sequence 46600/52929...\n",
      "      Processing sequence 46650/52929...\n",
      "      Processing sequence 46700/52929...\n",
      "      Processing sequence 46750/52929...\n",
      "      Processing sequence 46800/52929...\n",
      "      Processing sequence 46850/52929...\n",
      "      Processing sequence 46900/52929...\n",
      "      Processing sequence 46950/52929...\n",
      "      Processing sequence 47000/52929...\n",
      "      Processing sequence 47050/52929...\n",
      "      Processing sequence 47100/52929...\n",
      "      Processing sequence 47150/52929...\n",
      "      Processing sequence 47200/52929...\n",
      "      Processing sequence 47250/52929...\n",
      "      Processing sequence 47300/52929...\n",
      "      Processing sequence 47350/52929...\n",
      "      Processing sequence 47400/52929...\n",
      "      Processing sequence 47450/52929...\n",
      "      Processing sequence 47500/52929...\n",
      "      Processing sequence 47550/52929...\n",
      "      Processing sequence 47600/52929...\n",
      "      Processing sequence 47650/52929...\n",
      "      Processing sequence 47700/52929...\n",
      "      Processing sequence 47750/52929...\n",
      "      Processing sequence 47800/52929...\n",
      "      Processing sequence 47850/52929...\n",
      "      Processing sequence 47900/52929...\n",
      "      Processing sequence 47950/52929...\n",
      "      Processing sequence 48000/52929...\n",
      "      Processing sequence 48050/52929...\n",
      "      Processing sequence 48100/52929...\n",
      "      Processing sequence 48150/52929...\n",
      "      Processing sequence 48200/52929...\n",
      "      Processing sequence 48250/52929...\n",
      "      Processing sequence 48300/52929...\n",
      "      Processing sequence 48350/52929...\n",
      "      Processing sequence 48400/52929...\n",
      "      Processing sequence 48450/52929...\n",
      "      Processing sequence 48500/52929...\n",
      "      Processing sequence 48550/52929...\n",
      "      Processing sequence 48600/52929...\n",
      "      Processing sequence 48650/52929...\n",
      "      Processing sequence 48700/52929...\n",
      "      Processing sequence 48750/52929...\n",
      "      Processing sequence 48800/52929...\n",
      "      Processing sequence 48850/52929...\n",
      "      Processing sequence 48900/52929...\n",
      "      Processing sequence 48950/52929...\n",
      "      Processing sequence 49000/52929...\n",
      "      Processing sequence 49050/52929...\n",
      "      Processing sequence 49100/52929...\n",
      "      Processing sequence 49150/52929...\n",
      "      Processing sequence 49200/52929...\n",
      "      Processing sequence 49250/52929...\n",
      "      Processing sequence 49300/52929...\n",
      "      Processing sequence 49350/52929...\n",
      "      Processing sequence 49400/52929...\n",
      "      Processing sequence 49450/52929...\n",
      "      Processing sequence 49500/52929...\n",
      "      Processing sequence 49550/52929...\n",
      "      Processing sequence 49600/52929...\n",
      "      Processing sequence 49650/52929...\n",
      "      Processing sequence 49700/52929...\n",
      "      Processing sequence 49750/52929...\n",
      "      Processing sequence 49800/52929...\n",
      "      Processing sequence 49850/52929...\n",
      "      Processing sequence 49900/52929...\n",
      "      Processing sequence 49950/52929...\n",
      "      Processing sequence 50000/52929...\n",
      "      Processing sequence 50050/52929...\n",
      "      Processing sequence 50100/52929...\n",
      "      Processing sequence 50150/52929...\n",
      "      Processing sequence 50200/52929...\n",
      "      Processing sequence 50250/52929...\n",
      "      Processing sequence 50300/52929...\n",
      "      Processing sequence 50350/52929...\n",
      "      Processing sequence 50400/52929...\n",
      "      Processing sequence 50450/52929...\n",
      "      Processing sequence 50500/52929...\n",
      "      Processing sequence 50550/52929...\n",
      "      Processing sequence 50600/52929...\n",
      "      Processing sequence 50650/52929...\n",
      "      Processing sequence 50700/52929...\n",
      "      Processing sequence 50750/52929...\n",
      "      Processing sequence 50800/52929...\n",
      "      Processing sequence 50850/52929...\n",
      "      Processing sequence 50900/52929...\n",
      "      Processing sequence 50950/52929...\n",
      "      Processing sequence 51000/52929...\n",
      "      Processing sequence 51050/52929...\n",
      "      Processing sequence 51100/52929...\n",
      "      Processing sequence 51150/52929...\n",
      "      Processing sequence 51200/52929...\n",
      "      Processing sequence 51250/52929...\n",
      "      Processing sequence 51300/52929...\n",
      "      Processing sequence 51350/52929...\n",
      "      Processing sequence 51400/52929...\n",
      "      Processing sequence 51450/52929...\n",
      "      Processing sequence 51500/52929...\n",
      "      Processing sequence 51550/52929...\n",
      "      Processing sequence 51600/52929...\n",
      "      Processing sequence 51650/52929...\n",
      "      Processing sequence 51700/52929...\n",
      "      Processing sequence 51750/52929...\n",
      "      Processing sequence 51800/52929...\n",
      "      Processing sequence 51850/52929...\n",
      "      Processing sequence 51900/52929...\n",
      "      Processing sequence 51950/52929...\n",
      "      Processing sequence 52000/52929...\n",
      "      Processing sequence 52050/52929...\n",
      "      Processing sequence 52100/52929...\n",
      "      Processing sequence 52150/52929...\n",
      "      Processing sequence 52200/52929...\n",
      "      Processing sequence 52250/52929...\n",
      "      Processing sequence 52300/52929...\n",
      "      Processing sequence 52350/52929...\n",
      "      Processing sequence 52400/52929...\n",
      "      Processing sequence 52450/52929...\n",
      "      Processing sequence 52500/52929...\n",
      "      Processing sequence 52550/52929...\n",
      "      Processing sequence 52600/52929...\n",
      "      Processing sequence 52650/52929...\n",
      "      Processing sequence 52700/52929...\n",
      "      Processing sequence 52750/52929...\n",
      "      Processing sequence 52800/52929...\n",
      "      Processing sequence 52850/52929...\n",
      "      Processing sequence 52900/52929...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 904805\n",
      "      Avg per sequence: 17.1\n",
      "      By scale:\n",
      "        MICRO.......    4.0 per sequence\n",
      "        MINI........    8.8 per sequence\n",
      "        MESO........    2.8 per sequence\n",
      "        MACRO.......    1.6 per sequence\n",
      "        GLOBAL......    0.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "Total tokens: 4,616,094\n"
     ]
    }
   ],
   "source": [
    "from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "\n",
    "# Your data\n",
    "dataset = HierarchicalEventDataset(X_shuffled)\n",
    "\n",
    "# Generate Qwen-optimized corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset)\n",
    "\n",
    "# Save for training\n",
    "generator.save_corpus(corpus, 'qwen_training.txt')\n",
    "\n",
    "# Get statistics\n",
    "stats = generator.estimate_tokens(corpus)\n",
    "print(f\"Total tokens: {stats['estimated_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9602d691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2486, -0.4019, -0.4792,  ...,  0.0221, -0.1104, -0.2543],\n",
       "        [ 0.0211,  0.0350,  0.0369,  ..., -0.0455,  0.0193,  0.0633],\n",
       "        [ 0.0880,  0.0757,  0.0149,  ...,  0.0203, -0.0656, -0.0789],\n",
       "        ...,\n",
       "        [ 0.7907,  0.7895,  0.7871,  ...,  0.7914,  0.7915,  0.7910],\n",
       "        [ 0.4151,  0.4157,  0.4186,  ...,  0.4144,  0.4125,  0.4122],\n",
       "        [ 0.4614,  0.4617,  0.4639,  ...,  0.4710,  0.4733,  0.4705]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_univariate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a893455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts2seq.backbone import TimeSeriesEventModel\n",
    "\n",
    "# Parameters\n",
    "batch_size = 4\n",
    "seq_length = 128\n",
    "vocab_size = 184\n",
    "d_model = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c5f75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "  Parameters: 5,368,248\n",
      "  Encoder layers: 4\n",
      "  Decoder layers: 4\n",
      "  Model dimension: 256\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = TimeSeriesEventModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,  # Smaller for demo\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "model = model.cuda()\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Parameters: {model.count_parameters():,}\")\n",
    "print(f\"  Encoder layers: 4\")\n",
    "print(f\"  Decoder layers: 4\")\n",
    "print(f\"  Model dimension: {d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1cea182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input shapes:\n",
      "  Time series: torch.Size([4, 128])\n",
      "  Target text: torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "# Dummy data\n",
    "src = torch.randn(batch_size, seq_length).cuda()  # Time series\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, 50)).cuda()  # Target text\n",
    "\n",
    "print(f\"\\nInput shapes:\")\n",
    "print(f\"  Time series: {src.shape}\")\n",
    "print(f\"  Target text: {tgt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a59efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output logits: torch.Size([4, 50, 184])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "logits = model(src, tgt)\n",
    "print(f\"\\nOutput logits: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0cad69e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "# Generation\n",
    "generated = model.generate(src, max_length=50)\n",
    "print(f\"Generated text: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1bd19",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: GENERATE EVENT CORPUS\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create hierarchical event dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m HierarchicalEventDataset(\u001b[43mX_subset\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate text corpus\u001b[39;00m\n\u001b[1;32m     13\u001b[0m generator \u001b[38;5;241m=\u001b[39m QwenCorpusGenerator(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnatural_flat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: GENERATE EVENT CORPUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: GENERATE EVENT CORPUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create hierarchical event dataset\n",
    "dataset = HierarchicalEventDataset(X_univariate, verbose=True)\n",
    "\n",
    "# Generate text corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset, add_metadata=True)\n",
    "\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Documents: {len(corpus)}\")\n",
    "print(f\"  Example: {corpus[0][:150]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: BUILD TOKENIZER AND DETERMINE VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: BUILD TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = EventTextTokenizer(\n",
    "    max_position=512,      # Support positions 0-511\n",
    "    max_vocab_size=3000    # Upper bound (won't reach this)\n",
    ")\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "tokenizer.build_vocab(corpus, min_freq=2)\n",
    "\n",
    "# Get actual vocabulary size\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print(f\"\\nActual vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: DECIDE MODEL VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DECIDE MODEL VOCAB SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Use exact size (most efficient)\n",
    "vocab_size_exact = actual_vocab_size\n",
    "\n",
    "# Option 2: Round up to power of 2 (hardware-friendly)\n",
    "import math\n",
    "vocab_size_pow2 = 2 ** math.ceil(math.log2(actual_vocab_size))\n",
    "\n",
    "# Option 3: Round up to nearest 256 (common practice)\n",
    "vocab_size_256 = ((actual_vocab_size + 255) // 256) * 256\n",
    "\n",
    "print(f\"\\nVocabulary size options:\")\n",
    "print(f\"  Exact: {vocab_size_exact}\")\n",
    "print(f\"  Power of 2: {vocab_size_pow2}\")\n",
    "print(f\"  Nearest 256: {vocab_size_256}\")\n",
    "\n",
    "# Choose option (I recommend power of 2)\n",
    "vocab_size = vocab_size_pow2\n",
    "\n",
    "print(f\"\\n✓ Selected vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fe6d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: PREPARE DATA\n",
      "================================================================================\n",
      "Data shape: torch.Size([52929, 128])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete workflow: Generate corpus → Build tokenizer → Create model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from hierarchical_event_labeling import HierarchicalEventDataset\n",
    "from qwen_text_generator import QwenCorpusGenerator\n",
    "from ts2seq.tokenizer import EventTextTokenizer\n",
    "from ts2seq.backbone import TimeSeriesEventModel\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: PREPARE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load HAR data\n",
    "train_data = torch.load('data/HAR/train.pt')\n",
    "X_train = train_data['samples']  # [5881, 9, 128]\n",
    "\n",
    "# Flatten to univariate\n",
    "X_univariate = X_train.reshape(-1, 128).float()  # [52929, 128]\n",
    "\n",
    "# Shuffle\n",
    "indices = torch.randperm(X_univariate.shape[0])\n",
    "X_shuffled = X_univariate[indices]\n",
    "\n",
    "print(f\"Data shape: {X_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de6bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: GENERATE EVENT CORPUS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL EVENT DATASET\n",
      "================================================================================\n",
      "Sequences: 52929\n",
      "Length: 128\n",
      "\n",
      "[1/4] Extracting multi-scale features...\n",
      "      ✓ Computed 14 feature types\n",
      "[2/4] Encoding step-wise labels...\n",
      "      ✓ Encoded 6774912 timesteps\n",
      "[3/4] Detecting events and building hierarchy...\n",
      "      Processing sequence 0/52929...\n",
      "      Processing sequence 50/52929...\n",
      "      Processing sequence 100/52929...\n",
      "      Processing sequence 150/52929...\n",
      "      Processing sequence 200/52929...\n",
      "      Processing sequence 250/52929...\n",
      "      Processing sequence 300/52929...\n",
      "      Processing sequence 350/52929...\n",
      "      Processing sequence 400/52929...\n",
      "      Processing sequence 450/52929...\n",
      "      Processing sequence 500/52929...\n",
      "      Processing sequence 550/52929...\n",
      "      Processing sequence 600/52929...\n",
      "      Processing sequence 650/52929...\n",
      "      Processing sequence 700/52929...\n",
      "      Processing sequence 750/52929...\n",
      "      Processing sequence 800/52929...\n",
      "      Processing sequence 850/52929...\n",
      "      Processing sequence 900/52929...\n",
      "      Processing sequence 950/52929...\n",
      "      Processing sequence 1000/52929...\n",
      "      Processing sequence 1050/52929...\n",
      "      Processing sequence 1100/52929...\n",
      "      Processing sequence 1150/52929...\n",
      "      Processing sequence 1200/52929...\n",
      "      Processing sequence 1250/52929...\n",
      "      Processing sequence 1300/52929...\n",
      "      Processing sequence 1350/52929...\n",
      "      Processing sequence 1400/52929...\n",
      "      Processing sequence 1450/52929...\n",
      "      Processing sequence 1500/52929...\n",
      "      Processing sequence 1550/52929...\n",
      "      Processing sequence 1600/52929...\n",
      "      Processing sequence 1650/52929...\n",
      "      Processing sequence 1700/52929...\n",
      "      Processing sequence 1750/52929...\n",
      "      Processing sequence 1800/52929...\n",
      "      Processing sequence 1850/52929...\n",
      "      Processing sequence 1900/52929...\n",
      "      Processing sequence 1950/52929...\n",
      "      Processing sequence 2000/52929...\n",
      "      Processing sequence 2050/52929...\n",
      "      Processing sequence 2100/52929...\n",
      "      Processing sequence 2150/52929...\n",
      "      Processing sequence 2200/52929...\n",
      "      Processing sequence 2250/52929...\n",
      "      Processing sequence 2300/52929...\n",
      "      Processing sequence 2350/52929...\n",
      "      Processing sequence 2400/52929...\n",
      "      Processing sequence 2450/52929...\n",
      "      Processing sequence 2500/52929...\n",
      "      Processing sequence 2550/52929...\n",
      "      Processing sequence 2600/52929...\n",
      "      Processing sequence 2650/52929...\n",
      "      Processing sequence 2700/52929...\n",
      "      Processing sequence 2750/52929...\n",
      "      Processing sequence 2800/52929...\n",
      "      Processing sequence 2850/52929...\n",
      "      Processing sequence 2900/52929...\n",
      "      Processing sequence 2950/52929...\n",
      "      Processing sequence 3000/52929...\n",
      "      Processing sequence 3050/52929...\n",
      "      Processing sequence 3100/52929...\n",
      "      Processing sequence 3150/52929...\n",
      "      Processing sequence 3200/52929...\n",
      "      Processing sequence 3250/52929...\n",
      "      Processing sequence 3300/52929...\n",
      "      Processing sequence 3350/52929...\n",
      "      Processing sequence 3400/52929...\n",
      "      Processing sequence 3450/52929...\n",
      "      Processing sequence 3500/52929...\n",
      "      Processing sequence 3550/52929...\n",
      "      Processing sequence 3600/52929...\n",
      "      Processing sequence 3650/52929...\n",
      "      Processing sequence 3700/52929...\n",
      "      Processing sequence 3750/52929...\n",
      "      Processing sequence 3800/52929...\n",
      "      Processing sequence 3850/52929...\n",
      "      Processing sequence 3900/52929...\n",
      "      Processing sequence 3950/52929...\n",
      "      Processing sequence 4000/52929...\n",
      "      Processing sequence 4050/52929...\n",
      "      Processing sequence 4100/52929...\n",
      "      Processing sequence 4150/52929...\n",
      "      Processing sequence 4200/52929...\n",
      "      Processing sequence 4250/52929...\n",
      "      Processing sequence 4300/52929...\n",
      "      Processing sequence 4350/52929...\n",
      "      Processing sequence 4400/52929...\n",
      "      Processing sequence 4450/52929...\n",
      "      Processing sequence 4500/52929...\n",
      "      Processing sequence 4550/52929...\n",
      "      Processing sequence 4600/52929...\n",
      "      Processing sequence 4650/52929...\n",
      "      Processing sequence 4700/52929...\n",
      "      Processing sequence 4750/52929...\n",
      "      Processing sequence 4800/52929...\n",
      "      Processing sequence 4850/52929...\n",
      "      Processing sequence 4900/52929...\n",
      "      Processing sequence 4950/52929...\n",
      "      Processing sequence 5000/52929...\n",
      "      Processing sequence 5050/52929...\n",
      "      Processing sequence 5100/52929...\n",
      "      Processing sequence 5150/52929...\n",
      "      Processing sequence 5200/52929...\n",
      "      Processing sequence 5250/52929...\n",
      "      Processing sequence 5300/52929...\n",
      "      Processing sequence 5350/52929...\n",
      "      Processing sequence 5400/52929...\n",
      "      Processing sequence 5450/52929...\n",
      "      Processing sequence 5500/52929...\n",
      "      Processing sequence 5550/52929...\n",
      "      Processing sequence 5600/52929...\n",
      "      Processing sequence 5650/52929...\n",
      "      Processing sequence 5700/52929...\n",
      "      Processing sequence 5750/52929...\n",
      "      Processing sequence 5800/52929...\n",
      "      Processing sequence 5850/52929...\n",
      "      Processing sequence 5900/52929...\n",
      "      Processing sequence 5950/52929...\n",
      "      Processing sequence 6000/52929...\n",
      "      Processing sequence 6050/52929...\n",
      "      Processing sequence 6100/52929...\n",
      "      Processing sequence 6150/52929...\n",
      "      Processing sequence 6200/52929...\n",
      "      Processing sequence 6250/52929...\n",
      "      Processing sequence 6300/52929...\n",
      "      Processing sequence 6350/52929...\n",
      "      Processing sequence 6400/52929...\n",
      "      Processing sequence 6450/52929...\n",
      "      Processing sequence 6500/52929...\n",
      "      Processing sequence 6550/52929...\n",
      "      Processing sequence 6600/52929...\n",
      "      Processing sequence 6650/52929...\n",
      "      Processing sequence 6700/52929...\n",
      "      Processing sequence 6750/52929...\n",
      "      Processing sequence 6800/52929...\n",
      "      Processing sequence 6850/52929...\n",
      "      Processing sequence 6900/52929...\n",
      "      Processing sequence 6950/52929...\n",
      "      Processing sequence 7000/52929...\n",
      "      Processing sequence 7050/52929...\n",
      "      Processing sequence 7100/52929...\n",
      "      Processing sequence 7150/52929...\n",
      "      Processing sequence 7200/52929...\n",
      "      Processing sequence 7250/52929...\n",
      "      Processing sequence 7300/52929...\n",
      "      Processing sequence 7350/52929...\n",
      "      Processing sequence 7400/52929...\n",
      "      Processing sequence 7450/52929...\n",
      "      Processing sequence 7500/52929...\n",
      "      Processing sequence 7550/52929...\n",
      "      Processing sequence 7600/52929...\n",
      "      Processing sequence 7650/52929...\n",
      "      Processing sequence 7700/52929...\n",
      "      Processing sequence 7750/52929...\n",
      "      Processing sequence 7800/52929...\n",
      "      Processing sequence 7850/52929...\n",
      "      Processing sequence 7900/52929...\n",
      "      Processing sequence 7950/52929...\n",
      "      Processing sequence 8000/52929...\n",
      "      Processing sequence 8050/52929...\n",
      "      Processing sequence 8100/52929...\n",
      "      Processing sequence 8150/52929...\n",
      "      Processing sequence 8200/52929...\n",
      "      Processing sequence 8250/52929...\n",
      "      Processing sequence 8300/52929...\n",
      "      Processing sequence 8350/52929...\n",
      "      Processing sequence 8400/52929...\n",
      "      Processing sequence 8450/52929...\n",
      "      Processing sequence 8500/52929...\n",
      "      Processing sequence 8550/52929...\n",
      "      Processing sequence 8600/52929...\n",
      "      Processing sequence 8650/52929...\n",
      "      Processing sequence 8700/52929...\n",
      "      Processing sequence 8750/52929...\n",
      "      Processing sequence 8800/52929...\n",
      "      Processing sequence 8850/52929...\n",
      "      Processing sequence 8900/52929...\n",
      "      Processing sequence 8950/52929...\n",
      "      Processing sequence 9000/52929...\n",
      "      Processing sequence 9050/52929...\n",
      "      Processing sequence 9100/52929...\n",
      "      Processing sequence 9150/52929...\n",
      "      Processing sequence 9200/52929...\n",
      "      Processing sequence 9250/52929...\n",
      "      Processing sequence 9300/52929...\n",
      "      Processing sequence 9350/52929...\n",
      "      Processing sequence 9400/52929...\n",
      "      Processing sequence 9450/52929...\n",
      "      Processing sequence 9500/52929...\n",
      "      Processing sequence 9550/52929...\n",
      "      Processing sequence 9600/52929...\n",
      "      Processing sequence 9650/52929...\n",
      "      Processing sequence 9700/52929...\n",
      "      Processing sequence 9750/52929...\n",
      "      Processing sequence 9800/52929...\n",
      "      Processing sequence 9850/52929...\n",
      "      Processing sequence 9900/52929...\n",
      "      Processing sequence 9950/52929...\n",
      "      Processing sequence 10000/52929...\n",
      "      Processing sequence 10050/52929...\n",
      "      Processing sequence 10100/52929...\n",
      "      Processing sequence 10150/52929...\n",
      "      Processing sequence 10200/52929...\n",
      "      Processing sequence 10250/52929...\n",
      "      Processing sequence 10300/52929...\n",
      "      Processing sequence 10350/52929...\n",
      "      Processing sequence 10400/52929...\n",
      "      Processing sequence 10450/52929...\n",
      "      Processing sequence 10500/52929...\n",
      "      Processing sequence 10550/52929...\n",
      "      Processing sequence 10600/52929...\n",
      "      Processing sequence 10650/52929...\n",
      "      Processing sequence 10700/52929...\n",
      "      Processing sequence 10750/52929...\n",
      "      Processing sequence 10800/52929...\n",
      "      Processing sequence 10850/52929...\n",
      "      Processing sequence 10900/52929...\n",
      "      Processing sequence 10950/52929...\n",
      "      Processing sequence 11000/52929...\n",
      "      Processing sequence 11050/52929...\n",
      "      Processing sequence 11100/52929...\n",
      "      Processing sequence 11150/52929...\n",
      "      Processing sequence 11200/52929...\n",
      "      Processing sequence 11250/52929...\n",
      "      Processing sequence 11300/52929...\n",
      "      Processing sequence 11350/52929...\n",
      "      Processing sequence 11400/52929...\n",
      "      Processing sequence 11450/52929...\n",
      "      Processing sequence 11500/52929...\n",
      "      Processing sequence 11550/52929...\n",
      "      Processing sequence 11600/52929...\n",
      "      Processing sequence 11650/52929...\n",
      "      Processing sequence 11700/52929...\n",
      "      Processing sequence 11750/52929...\n",
      "      Processing sequence 11800/52929...\n",
      "      Processing sequence 11850/52929...\n",
      "      Processing sequence 11900/52929...\n",
      "      Processing sequence 11950/52929...\n",
      "      Processing sequence 12000/52929...\n",
      "      Processing sequence 12050/52929...\n",
      "      Processing sequence 12100/52929...\n",
      "      Processing sequence 12150/52929...\n",
      "      Processing sequence 12200/52929...\n",
      "      Processing sequence 12250/52929...\n",
      "      Processing sequence 12300/52929...\n",
      "      Processing sequence 12350/52929...\n",
      "      Processing sequence 12400/52929...\n",
      "      Processing sequence 12450/52929...\n",
      "      Processing sequence 12500/52929...\n",
      "      Processing sequence 12550/52929...\n",
      "      Processing sequence 12600/52929...\n",
      "      Processing sequence 12650/52929...\n",
      "      Processing sequence 12700/52929...\n",
      "      Processing sequence 12750/52929...\n",
      "      Processing sequence 12800/52929...\n",
      "      Processing sequence 12850/52929...\n",
      "      Processing sequence 12900/52929...\n",
      "      Processing sequence 12950/52929...\n",
      "      Processing sequence 13000/52929...\n",
      "      Processing sequence 13050/52929...\n",
      "      Processing sequence 13100/52929...\n",
      "      Processing sequence 13150/52929...\n",
      "      Processing sequence 13200/52929...\n",
      "      Processing sequence 13250/52929...\n",
      "      Processing sequence 13300/52929...\n",
      "      Processing sequence 13350/52929...\n",
      "      Processing sequence 13400/52929...\n",
      "      Processing sequence 13450/52929...\n",
      "      Processing sequence 13500/52929...\n",
      "      Processing sequence 13550/52929...\n",
      "      Processing sequence 13600/52929...\n",
      "      Processing sequence 13650/52929...\n",
      "      Processing sequence 13700/52929...\n",
      "      Processing sequence 13750/52929...\n",
      "      Processing sequence 13800/52929...\n",
      "      Processing sequence 13850/52929...\n",
      "      Processing sequence 13900/52929...\n",
      "      Processing sequence 13950/52929...\n",
      "      Processing sequence 14000/52929...\n",
      "      Processing sequence 14050/52929...\n",
      "      Processing sequence 14100/52929...\n",
      "      Processing sequence 14150/52929...\n",
      "      Processing sequence 14200/52929...\n",
      "      Processing sequence 14250/52929...\n",
      "      Processing sequence 14300/52929...\n",
      "      Processing sequence 14350/52929...\n",
      "      Processing sequence 14400/52929...\n",
      "      Processing sequence 14450/52929...\n",
      "      Processing sequence 14500/52929...\n",
      "      Processing sequence 14550/52929...\n",
      "      Processing sequence 14600/52929...\n",
      "      Processing sequence 14650/52929...\n",
      "      Processing sequence 14700/52929...\n",
      "      Processing sequence 14750/52929...\n",
      "      Processing sequence 14800/52929...\n",
      "      Processing sequence 14850/52929...\n",
      "      Processing sequence 14900/52929...\n",
      "      Processing sequence 14950/52929...\n",
      "      Processing sequence 15000/52929...\n",
      "      Processing sequence 15050/52929...\n",
      "      Processing sequence 15100/52929...\n",
      "      Processing sequence 15150/52929...\n",
      "      Processing sequence 15200/52929...\n",
      "      Processing sequence 15250/52929...\n",
      "      Processing sequence 15300/52929...\n",
      "      Processing sequence 15350/52929...\n",
      "      Processing sequence 15400/52929...\n",
      "      Processing sequence 15450/52929...\n",
      "      Processing sequence 15500/52929...\n",
      "      Processing sequence 15550/52929...\n",
      "      Processing sequence 15600/52929...\n",
      "      Processing sequence 15650/52929...\n",
      "      Processing sequence 15700/52929...\n",
      "      Processing sequence 15750/52929...\n",
      "      Processing sequence 15800/52929...\n",
      "      Processing sequence 15850/52929...\n",
      "      Processing sequence 15900/52929...\n",
      "      Processing sequence 15950/52929...\n",
      "      Processing sequence 16000/52929...\n",
      "      Processing sequence 16050/52929...\n",
      "      Processing sequence 16100/52929...\n",
      "      Processing sequence 16150/52929...\n",
      "      Processing sequence 16200/52929...\n",
      "      Processing sequence 16250/52929...\n",
      "      Processing sequence 16300/52929...\n",
      "      Processing sequence 16350/52929...\n",
      "      Processing sequence 16400/52929...\n",
      "      Processing sequence 16450/52929...\n",
      "      Processing sequence 16500/52929...\n",
      "      Processing sequence 16550/52929...\n",
      "      Processing sequence 16600/52929...\n",
      "      Processing sequence 16650/52929...\n",
      "      Processing sequence 16700/52929...\n",
      "      Processing sequence 16750/52929...\n",
      "      Processing sequence 16800/52929...\n",
      "      Processing sequence 16850/52929...\n",
      "      Processing sequence 16900/52929...\n",
      "      Processing sequence 16950/52929...\n",
      "      Processing sequence 17000/52929...\n",
      "      Processing sequence 17050/52929...\n",
      "      Processing sequence 17100/52929...\n",
      "      Processing sequence 17150/52929...\n",
      "      Processing sequence 17200/52929...\n",
      "      Processing sequence 17250/52929...\n",
      "      Processing sequence 17300/52929...\n",
      "      Processing sequence 17350/52929...\n",
      "      Processing sequence 17400/52929...\n",
      "      Processing sequence 17450/52929...\n",
      "      Processing sequence 17500/52929...\n",
      "      Processing sequence 17550/52929...\n",
      "      Processing sequence 17600/52929...\n",
      "      Processing sequence 17650/52929...\n",
      "      Processing sequence 17700/52929...\n",
      "      Processing sequence 17750/52929...\n",
      "      Processing sequence 17800/52929...\n",
      "      Processing sequence 17850/52929...\n",
      "      Processing sequence 17900/52929...\n",
      "      Processing sequence 17950/52929...\n",
      "      Processing sequence 18000/52929...\n",
      "      Processing sequence 18050/52929...\n",
      "      Processing sequence 18100/52929...\n",
      "      Processing sequence 18150/52929...\n",
      "      Processing sequence 18200/52929...\n",
      "      Processing sequence 18250/52929...\n",
      "      Processing sequence 18300/52929...\n",
      "      Processing sequence 18350/52929...\n",
      "      Processing sequence 18400/52929...\n",
      "      Processing sequence 18450/52929...\n",
      "      Processing sequence 18500/52929...\n",
      "      Processing sequence 18550/52929...\n",
      "      Processing sequence 18600/52929...\n",
      "      Processing sequence 18650/52929...\n",
      "      Processing sequence 18700/52929...\n",
      "      Processing sequence 18750/52929...\n",
      "      Processing sequence 18800/52929...\n",
      "      Processing sequence 18850/52929...\n",
      "      Processing sequence 18900/52929...\n",
      "      Processing sequence 18950/52929...\n",
      "      Processing sequence 19000/52929...\n",
      "      Processing sequence 19050/52929...\n",
      "      Processing sequence 19100/52929...\n",
      "      Processing sequence 19150/52929...\n",
      "      Processing sequence 19200/52929...\n",
      "      Processing sequence 19250/52929...\n",
      "      Processing sequence 19300/52929...\n",
      "      Processing sequence 19350/52929...\n",
      "      Processing sequence 19400/52929...\n",
      "      Processing sequence 19450/52929...\n",
      "      Processing sequence 19500/52929...\n",
      "      Processing sequence 19550/52929...\n",
      "      Processing sequence 19600/52929...\n",
      "      Processing sequence 19650/52929...\n",
      "      Processing sequence 19700/52929...\n",
      "      Processing sequence 19750/52929...\n",
      "      Processing sequence 19800/52929...\n",
      "      Processing sequence 19850/52929...\n",
      "      Processing sequence 19900/52929...\n",
      "      Processing sequence 19950/52929...\n",
      "      Processing sequence 20000/52929...\n",
      "      Processing sequence 20050/52929...\n",
      "      Processing sequence 20100/52929...\n",
      "      Processing sequence 20150/52929...\n",
      "      Processing sequence 20200/52929...\n",
      "      Processing sequence 20250/52929...\n",
      "      Processing sequence 20300/52929...\n",
      "      Processing sequence 20350/52929...\n",
      "      Processing sequence 20400/52929...\n",
      "      Processing sequence 20450/52929...\n",
      "      Processing sequence 20500/52929...\n",
      "      Processing sequence 20550/52929...\n",
      "      Processing sequence 20600/52929...\n",
      "      Processing sequence 20650/52929...\n",
      "      Processing sequence 20700/52929...\n",
      "      Processing sequence 20750/52929...\n",
      "      Processing sequence 20800/52929...\n",
      "      Processing sequence 20850/52929...\n",
      "      Processing sequence 20900/52929...\n",
      "      Processing sequence 20950/52929...\n",
      "      Processing sequence 21000/52929...\n",
      "      Processing sequence 21050/52929...\n",
      "      Processing sequence 21100/52929...\n",
      "      Processing sequence 21150/52929...\n",
      "      Processing sequence 21200/52929...\n",
      "      Processing sequence 21250/52929...\n",
      "      Processing sequence 21300/52929...\n",
      "      Processing sequence 21350/52929...\n",
      "      Processing sequence 21400/52929...\n",
      "      Processing sequence 21450/52929...\n",
      "      Processing sequence 21500/52929...\n",
      "      Processing sequence 21550/52929...\n",
      "      Processing sequence 21600/52929...\n",
      "      Processing sequence 21650/52929...\n",
      "      Processing sequence 21700/52929...\n",
      "      Processing sequence 21750/52929...\n",
      "      Processing sequence 21800/52929...\n",
      "      Processing sequence 21850/52929...\n",
      "      Processing sequence 21900/52929...\n",
      "      Processing sequence 21950/52929...\n",
      "      Processing sequence 22000/52929...\n",
      "      Processing sequence 22050/52929...\n",
      "      Processing sequence 22100/52929...\n",
      "      Processing sequence 22150/52929...\n",
      "      Processing sequence 22200/52929...\n",
      "      Processing sequence 22250/52929...\n",
      "      Processing sequence 22300/52929...\n",
      "      Processing sequence 22350/52929...\n",
      "      Processing sequence 22400/52929...\n",
      "      Processing sequence 22450/52929...\n",
      "      Processing sequence 22500/52929...\n",
      "      Processing sequence 22550/52929...\n",
      "      Processing sequence 22600/52929...\n",
      "      Processing sequence 22650/52929...\n",
      "      Processing sequence 22700/52929...\n",
      "      Processing sequence 22750/52929...\n",
      "      Processing sequence 22800/52929...\n",
      "      Processing sequence 22850/52929...\n",
      "      Processing sequence 22900/52929...\n",
      "      Processing sequence 22950/52929...\n",
      "      Processing sequence 23000/52929...\n",
      "      Processing sequence 23050/52929...\n",
      "      Processing sequence 23100/52929...\n",
      "      Processing sequence 23150/52929...\n",
      "      Processing sequence 23200/52929...\n",
      "      Processing sequence 23250/52929...\n",
      "      Processing sequence 23300/52929...\n",
      "      Processing sequence 23350/52929...\n",
      "      Processing sequence 23400/52929...\n",
      "      Processing sequence 23450/52929...\n",
      "      Processing sequence 23500/52929...\n",
      "      Processing sequence 23550/52929...\n",
      "      Processing sequence 23600/52929...\n",
      "      Processing sequence 23650/52929...\n",
      "      Processing sequence 23700/52929...\n",
      "      Processing sequence 23750/52929...\n",
      "      Processing sequence 23800/52929...\n",
      "      Processing sequence 23850/52929...\n",
      "      Processing sequence 23900/52929...\n",
      "      Processing sequence 23950/52929...\n",
      "      Processing sequence 24000/52929...\n",
      "      Processing sequence 24050/52929...\n",
      "      Processing sequence 24100/52929...\n",
      "      Processing sequence 24150/52929...\n",
      "      Processing sequence 24200/52929...\n",
      "      Processing sequence 24250/52929...\n",
      "      Processing sequence 24300/52929...\n",
      "      Processing sequence 24350/52929...\n",
      "      Processing sequence 24400/52929...\n",
      "      Processing sequence 24450/52929...\n",
      "      Processing sequence 24500/52929...\n",
      "      Processing sequence 24550/52929...\n",
      "      Processing sequence 24600/52929...\n",
      "      Processing sequence 24650/52929...\n",
      "      Processing sequence 24700/52929...\n",
      "      Processing sequence 24750/52929...\n",
      "      Processing sequence 24800/52929...\n",
      "      Processing sequence 24850/52929...\n",
      "      Processing sequence 24900/52929...\n",
      "      Processing sequence 24950/52929...\n",
      "      Processing sequence 25000/52929...\n",
      "      Processing sequence 25050/52929...\n",
      "      Processing sequence 25100/52929...\n",
      "      Processing sequence 25150/52929...\n",
      "      Processing sequence 25200/52929...\n",
      "      Processing sequence 25250/52929...\n",
      "      Processing sequence 25300/52929...\n",
      "      Processing sequence 25350/52929...\n",
      "      Processing sequence 25400/52929...\n",
      "      Processing sequence 25450/52929...\n",
      "      Processing sequence 25500/52929...\n",
      "      Processing sequence 25550/52929...\n",
      "      Processing sequence 25600/52929...\n",
      "      Processing sequence 25650/52929...\n",
      "      Processing sequence 25700/52929...\n",
      "      Processing sequence 25750/52929...\n",
      "      Processing sequence 25800/52929...\n",
      "      Processing sequence 25850/52929...\n",
      "      Processing sequence 25900/52929...\n",
      "      Processing sequence 25950/52929...\n",
      "      Processing sequence 26000/52929...\n",
      "      Processing sequence 26050/52929...\n",
      "      Processing sequence 26100/52929...\n",
      "      Processing sequence 26150/52929...\n",
      "      Processing sequence 26200/52929...\n",
      "      Processing sequence 26250/52929...\n",
      "      Processing sequence 26300/52929...\n",
      "      Processing sequence 26350/52929...\n",
      "      Processing sequence 26400/52929...\n",
      "      Processing sequence 26450/52929...\n",
      "      Processing sequence 26500/52929...\n",
      "      Processing sequence 26550/52929...\n",
      "      Processing sequence 26600/52929...\n",
      "      Processing sequence 26650/52929...\n",
      "      Processing sequence 26700/52929...\n",
      "      Processing sequence 26750/52929...\n",
      "      Processing sequence 26800/52929...\n",
      "      Processing sequence 26850/52929...\n",
      "      Processing sequence 26900/52929...\n",
      "      Processing sequence 26950/52929...\n",
      "      Processing sequence 27000/52929...\n",
      "      Processing sequence 27050/52929...\n",
      "      Processing sequence 27100/52929...\n",
      "      Processing sequence 27150/52929...\n",
      "      Processing sequence 27200/52929...\n",
      "      Processing sequence 27250/52929...\n",
      "      Processing sequence 27300/52929...\n",
      "      Processing sequence 27350/52929...\n",
      "      Processing sequence 27400/52929...\n",
      "      Processing sequence 27450/52929...\n",
      "      Processing sequence 27500/52929...\n",
      "      Processing sequence 27550/52929...\n",
      "      Processing sequence 27600/52929...\n",
      "      Processing sequence 27650/52929...\n",
      "      Processing sequence 27700/52929...\n",
      "      Processing sequence 27750/52929...\n",
      "      Processing sequence 27800/52929...\n",
      "      Processing sequence 27850/52929...\n",
      "      Processing sequence 27900/52929...\n",
      "      Processing sequence 27950/52929...\n",
      "      Processing sequence 28000/52929...\n",
      "      Processing sequence 28050/52929...\n",
      "      Processing sequence 28100/52929...\n",
      "      Processing sequence 28150/52929...\n",
      "      Processing sequence 28200/52929...\n",
      "      Processing sequence 28250/52929...\n",
      "      Processing sequence 28300/52929...\n",
      "      Processing sequence 28350/52929...\n",
      "      Processing sequence 28400/52929...\n",
      "      Processing sequence 28450/52929...\n",
      "      Processing sequence 28500/52929...\n",
      "      Processing sequence 28550/52929...\n",
      "      Processing sequence 28600/52929...\n",
      "      Processing sequence 28650/52929...\n",
      "      Processing sequence 28700/52929...\n",
      "      Processing sequence 28750/52929...\n",
      "      Processing sequence 28800/52929...\n",
      "      Processing sequence 28850/52929...\n",
      "      Processing sequence 28900/52929...\n",
      "      Processing sequence 28950/52929...\n",
      "      Processing sequence 29000/52929...\n",
      "      Processing sequence 29050/52929...\n",
      "      Processing sequence 29100/52929...\n",
      "      Processing sequence 29150/52929...\n",
      "      Processing sequence 29200/52929...\n",
      "      Processing sequence 29250/52929...\n",
      "      Processing sequence 29300/52929...\n",
      "      Processing sequence 29350/52929...\n",
      "      Processing sequence 29400/52929...\n",
      "      Processing sequence 29450/52929...\n",
      "      Processing sequence 29500/52929...\n",
      "      Processing sequence 29550/52929...\n",
      "      Processing sequence 29600/52929...\n",
      "      Processing sequence 29650/52929...\n",
      "      Processing sequence 29700/52929...\n",
      "      Processing sequence 29750/52929...\n",
      "      Processing sequence 29800/52929...\n",
      "      Processing sequence 29850/52929...\n",
      "      Processing sequence 29900/52929...\n",
      "      Processing sequence 29950/52929...\n",
      "      Processing sequence 30000/52929...\n",
      "      Processing sequence 30050/52929...\n",
      "      Processing sequence 30100/52929...\n",
      "      Processing sequence 30150/52929...\n",
      "      Processing sequence 30200/52929...\n",
      "      Processing sequence 30250/52929...\n",
      "      Processing sequence 30300/52929...\n",
      "      Processing sequence 30350/52929...\n",
      "      Processing sequence 30400/52929...\n",
      "      Processing sequence 30450/52929...\n",
      "      Processing sequence 30500/52929...\n",
      "      Processing sequence 30550/52929...\n",
      "      Processing sequence 30600/52929...\n",
      "      Processing sequence 30650/52929...\n",
      "      Processing sequence 30700/52929...\n",
      "      Processing sequence 30750/52929...\n",
      "      Processing sequence 30800/52929...\n",
      "      Processing sequence 30850/52929...\n",
      "      Processing sequence 30900/52929...\n",
      "      Processing sequence 30950/52929...\n",
      "      Processing sequence 31000/52929...\n",
      "      Processing sequence 31050/52929...\n",
      "      Processing sequence 31100/52929...\n",
      "      Processing sequence 31150/52929...\n",
      "      Processing sequence 31200/52929...\n",
      "      Processing sequence 31250/52929...\n",
      "      Processing sequence 31300/52929...\n",
      "      Processing sequence 31350/52929...\n",
      "      Processing sequence 31400/52929...\n",
      "      Processing sequence 31450/52929...\n",
      "      Processing sequence 31500/52929...\n",
      "      Processing sequence 31550/52929...\n",
      "      Processing sequence 31600/52929...\n",
      "      Processing sequence 31650/52929...\n",
      "      Processing sequence 31700/52929...\n",
      "      Processing sequence 31750/52929...\n",
      "      Processing sequence 31800/52929...\n",
      "      Processing sequence 31850/52929...\n",
      "      Processing sequence 31900/52929...\n",
      "      Processing sequence 31950/52929...\n",
      "      Processing sequence 32000/52929...\n",
      "      Processing sequence 32050/52929...\n",
      "      Processing sequence 32100/52929...\n",
      "      Processing sequence 32150/52929...\n",
      "      Processing sequence 32200/52929...\n",
      "      Processing sequence 32250/52929...\n",
      "      Processing sequence 32300/52929...\n",
      "      Processing sequence 32350/52929...\n",
      "      Processing sequence 32400/52929...\n",
      "      Processing sequence 32450/52929...\n",
      "      Processing sequence 32500/52929...\n",
      "      Processing sequence 32550/52929...\n",
      "      Processing sequence 32600/52929...\n",
      "      Processing sequence 32650/52929...\n",
      "      Processing sequence 32700/52929...\n",
      "      Processing sequence 32750/52929...\n",
      "      Processing sequence 32800/52929...\n",
      "      Processing sequence 32850/52929...\n",
      "      Processing sequence 32900/52929...\n",
      "      Processing sequence 32950/52929...\n",
      "      Processing sequence 33000/52929...\n",
      "      Processing sequence 33050/52929...\n",
      "      Processing sequence 33100/52929...\n",
      "      Processing sequence 33150/52929...\n",
      "      Processing sequence 33200/52929...\n",
      "      Processing sequence 33250/52929...\n",
      "      Processing sequence 33300/52929...\n",
      "      Processing sequence 33350/52929...\n",
      "      Processing sequence 33400/52929...\n",
      "      Processing sequence 33450/52929...\n",
      "      Processing sequence 33500/52929...\n",
      "      Processing sequence 33550/52929...\n",
      "      Processing sequence 33600/52929...\n",
      "      Processing sequence 33650/52929...\n",
      "      Processing sequence 33700/52929...\n",
      "      Processing sequence 33750/52929...\n",
      "      Processing sequence 33800/52929...\n",
      "      Processing sequence 33850/52929...\n",
      "      Processing sequence 33900/52929...\n",
      "      Processing sequence 33950/52929...\n",
      "      Processing sequence 34000/52929...\n",
      "      Processing sequence 34050/52929...\n",
      "      Processing sequence 34100/52929...\n",
      "      Processing sequence 34150/52929...\n",
      "      Processing sequence 34200/52929...\n",
      "      Processing sequence 34250/52929...\n",
      "      Processing sequence 34300/52929...\n",
      "      Processing sequence 34350/52929...\n",
      "      Processing sequence 34400/52929...\n",
      "      Processing sequence 34450/52929...\n",
      "      Processing sequence 34500/52929...\n",
      "      Processing sequence 34550/52929...\n",
      "      Processing sequence 34600/52929...\n",
      "      Processing sequence 34650/52929...\n",
      "      Processing sequence 34700/52929...\n",
      "      Processing sequence 34750/52929...\n",
      "      Processing sequence 34800/52929...\n",
      "      Processing sequence 34850/52929...\n",
      "      Processing sequence 34900/52929...\n",
      "      Processing sequence 34950/52929...\n",
      "      Processing sequence 35000/52929...\n",
      "      Processing sequence 35050/52929...\n",
      "      Processing sequence 35100/52929...\n",
      "      Processing sequence 35150/52929...\n",
      "      Processing sequence 35200/52929...\n",
      "      Processing sequence 35250/52929...\n",
      "      Processing sequence 35300/52929...\n",
      "      Processing sequence 35350/52929...\n",
      "      Processing sequence 35400/52929...\n",
      "      Processing sequence 35450/52929...\n",
      "      Processing sequence 35500/52929...\n",
      "      Processing sequence 35550/52929...\n",
      "      Processing sequence 35600/52929...\n",
      "      Processing sequence 35650/52929...\n",
      "      Processing sequence 35700/52929...\n",
      "      Processing sequence 35750/52929...\n",
      "      Processing sequence 35800/52929...\n",
      "      Processing sequence 35850/52929...\n",
      "      Processing sequence 35900/52929...\n",
      "      Processing sequence 35950/52929...\n",
      "      Processing sequence 36000/52929...\n",
      "      Processing sequence 36050/52929...\n",
      "      Processing sequence 36100/52929...\n",
      "      Processing sequence 36150/52929...\n",
      "      Processing sequence 36200/52929...\n",
      "      Processing sequence 36250/52929...\n",
      "      Processing sequence 36300/52929...\n",
      "      Processing sequence 36350/52929...\n",
      "      Processing sequence 36400/52929...\n",
      "      Processing sequence 36450/52929...\n",
      "      Processing sequence 36500/52929...\n",
      "      Processing sequence 36550/52929...\n",
      "      Processing sequence 36600/52929...\n",
      "      Processing sequence 36650/52929...\n",
      "      Processing sequence 36700/52929...\n",
      "      Processing sequence 36750/52929...\n",
      "      Processing sequence 36800/52929...\n",
      "      Processing sequence 36850/52929...\n",
      "      Processing sequence 36900/52929...\n",
      "      Processing sequence 36950/52929...\n",
      "      Processing sequence 37000/52929...\n",
      "      Processing sequence 37050/52929...\n",
      "      Processing sequence 37100/52929...\n",
      "      Processing sequence 37150/52929...\n",
      "      Processing sequence 37200/52929...\n",
      "      Processing sequence 37250/52929...\n",
      "      Processing sequence 37300/52929...\n",
      "      Processing sequence 37350/52929...\n",
      "      Processing sequence 37400/52929...\n",
      "      Processing sequence 37450/52929...\n",
      "      Processing sequence 37500/52929...\n",
      "      Processing sequence 37550/52929...\n",
      "      Processing sequence 37600/52929...\n",
      "      Processing sequence 37650/52929...\n",
      "      Processing sequence 37700/52929...\n",
      "      Processing sequence 37750/52929...\n",
      "      Processing sequence 37800/52929...\n",
      "      Processing sequence 37850/52929...\n",
      "      Processing sequence 37900/52929...\n",
      "      Processing sequence 37950/52929...\n",
      "      Processing sequence 38000/52929...\n",
      "      Processing sequence 38050/52929...\n",
      "      Processing sequence 38100/52929...\n",
      "      Processing sequence 38150/52929...\n",
      "      Processing sequence 38200/52929...\n",
      "      Processing sequence 38250/52929...\n",
      "      Processing sequence 38300/52929...\n",
      "      Processing sequence 38350/52929...\n",
      "      Processing sequence 38400/52929...\n",
      "      Processing sequence 38450/52929...\n",
      "      Processing sequence 38500/52929...\n",
      "      Processing sequence 38550/52929...\n",
      "      Processing sequence 38600/52929...\n",
      "      Processing sequence 38650/52929...\n",
      "      Processing sequence 38700/52929...\n",
      "      Processing sequence 38750/52929...\n",
      "      Processing sequence 38800/52929...\n",
      "      Processing sequence 38850/52929...\n",
      "      Processing sequence 38900/52929...\n",
      "      Processing sequence 38950/52929...\n",
      "      Processing sequence 39000/52929...\n",
      "      Processing sequence 39050/52929...\n",
      "      Processing sequence 39100/52929...\n",
      "      Processing sequence 39150/52929...\n",
      "      Processing sequence 39200/52929...\n",
      "      Processing sequence 39250/52929...\n",
      "      Processing sequence 39300/52929...\n",
      "      Processing sequence 39350/52929...\n",
      "      Processing sequence 39400/52929...\n",
      "      Processing sequence 39450/52929...\n",
      "      Processing sequence 39500/52929...\n",
      "      Processing sequence 39550/52929...\n",
      "      Processing sequence 39600/52929...\n",
      "      Processing sequence 39650/52929...\n",
      "      Processing sequence 39700/52929...\n",
      "      Processing sequence 39750/52929...\n",
      "      Processing sequence 39800/52929...\n",
      "      Processing sequence 39850/52929...\n",
      "      Processing sequence 39900/52929...\n",
      "      Processing sequence 39950/52929...\n",
      "      Processing sequence 40000/52929...\n",
      "      Processing sequence 40050/52929...\n",
      "      Processing sequence 40100/52929...\n",
      "      Processing sequence 40150/52929...\n",
      "      Processing sequence 40200/52929...\n",
      "      Processing sequence 40250/52929...\n",
      "      Processing sequence 40300/52929...\n",
      "      Processing sequence 40350/52929...\n",
      "      Processing sequence 40400/52929...\n",
      "      Processing sequence 40450/52929...\n",
      "      Processing sequence 40500/52929...\n",
      "      Processing sequence 40550/52929...\n",
      "      Processing sequence 40600/52929...\n",
      "      Processing sequence 40650/52929...\n",
      "      Processing sequence 40700/52929...\n",
      "      Processing sequence 40750/52929...\n",
      "      Processing sequence 40800/52929...\n",
      "      Processing sequence 40850/52929...\n",
      "      Processing sequence 40900/52929...\n",
      "      Processing sequence 40950/52929...\n",
      "      Processing sequence 41000/52929...\n",
      "      Processing sequence 41050/52929...\n",
      "      Processing sequence 41100/52929...\n",
      "      Processing sequence 41150/52929...\n",
      "      Processing sequence 41200/52929...\n",
      "      Processing sequence 41250/52929...\n",
      "      Processing sequence 41300/52929...\n",
      "      Processing sequence 41350/52929...\n",
      "      Processing sequence 41400/52929...\n",
      "      Processing sequence 41450/52929...\n",
      "      Processing sequence 41500/52929...\n",
      "      Processing sequence 41550/52929...\n",
      "      Processing sequence 41600/52929...\n",
      "      Processing sequence 41650/52929...\n",
      "      Processing sequence 41700/52929...\n",
      "      Processing sequence 41750/52929...\n",
      "      Processing sequence 41800/52929...\n",
      "      Processing sequence 41850/52929...\n",
      "      Processing sequence 41900/52929...\n",
      "      Processing sequence 41950/52929...\n",
      "      Processing sequence 42000/52929...\n",
      "      Processing sequence 42050/52929...\n",
      "      Processing sequence 42100/52929...\n",
      "      Processing sequence 42150/52929...\n",
      "      Processing sequence 42200/52929...\n",
      "      Processing sequence 42250/52929...\n",
      "      Processing sequence 42300/52929...\n",
      "      Processing sequence 42350/52929...\n",
      "      Processing sequence 42400/52929...\n",
      "      Processing sequence 42450/52929...\n",
      "      Processing sequence 42500/52929...\n",
      "      Processing sequence 42550/52929...\n",
      "      Processing sequence 42600/52929...\n",
      "      Processing sequence 42650/52929...\n",
      "      Processing sequence 42700/52929...\n",
      "      Processing sequence 42750/52929...\n",
      "      Processing sequence 42800/52929...\n",
      "      Processing sequence 42850/52929...\n",
      "      Processing sequence 42900/52929...\n",
      "      Processing sequence 42950/52929...\n",
      "      Processing sequence 43000/52929...\n",
      "      Processing sequence 43050/52929...\n",
      "      Processing sequence 43100/52929...\n",
      "      Processing sequence 43150/52929...\n",
      "      Processing sequence 43200/52929...\n",
      "      Processing sequence 43250/52929...\n",
      "      Processing sequence 43300/52929...\n",
      "      Processing sequence 43350/52929...\n",
      "      Processing sequence 43400/52929...\n",
      "      Processing sequence 43450/52929...\n",
      "      Processing sequence 43500/52929...\n",
      "      Processing sequence 43550/52929...\n",
      "      Processing sequence 43600/52929...\n",
      "      Processing sequence 43650/52929...\n",
      "      Processing sequence 43700/52929...\n",
      "      Processing sequence 43750/52929...\n",
      "      Processing sequence 43800/52929...\n",
      "      Processing sequence 43850/52929...\n",
      "      Processing sequence 43900/52929...\n",
      "      Processing sequence 43950/52929...\n",
      "      Processing sequence 44000/52929...\n",
      "      Processing sequence 44050/52929...\n",
      "      Processing sequence 44100/52929...\n",
      "      Processing sequence 44150/52929...\n",
      "      Processing sequence 44200/52929...\n",
      "      Processing sequence 44250/52929...\n",
      "      Processing sequence 44300/52929...\n",
      "      Processing sequence 44350/52929...\n",
      "      Processing sequence 44400/52929...\n",
      "      Processing sequence 44450/52929...\n",
      "      Processing sequence 44500/52929...\n",
      "      Processing sequence 44550/52929...\n",
      "      Processing sequence 44600/52929...\n",
      "      Processing sequence 44650/52929...\n",
      "      Processing sequence 44700/52929...\n",
      "      Processing sequence 44750/52929...\n",
      "      Processing sequence 44800/52929...\n",
      "      Processing sequence 44850/52929...\n",
      "      Processing sequence 44900/52929...\n",
      "      Processing sequence 44950/52929...\n",
      "      Processing sequence 45000/52929...\n",
      "      Processing sequence 45050/52929...\n",
      "      Processing sequence 45100/52929...\n",
      "      Processing sequence 45150/52929...\n",
      "      Processing sequence 45200/52929...\n",
      "      Processing sequence 45250/52929...\n",
      "      Processing sequence 45300/52929...\n",
      "      Processing sequence 45350/52929...\n",
      "      Processing sequence 45400/52929...\n",
      "      Processing sequence 45450/52929...\n",
      "      Processing sequence 45500/52929...\n",
      "      Processing sequence 45550/52929...\n",
      "      Processing sequence 45600/52929...\n",
      "      Processing sequence 45650/52929...\n",
      "      Processing sequence 45700/52929...\n",
      "      Processing sequence 45750/52929...\n",
      "      Processing sequence 45800/52929...\n",
      "      Processing sequence 45850/52929...\n",
      "      Processing sequence 45900/52929...\n",
      "      Processing sequence 45950/52929...\n",
      "      Processing sequence 46000/52929...\n",
      "      Processing sequence 46050/52929...\n",
      "      Processing sequence 46100/52929...\n",
      "      Processing sequence 46150/52929...\n",
      "      Processing sequence 46200/52929...\n",
      "      Processing sequence 46250/52929...\n",
      "      Processing sequence 46300/52929...\n",
      "      Processing sequence 46350/52929...\n",
      "      Processing sequence 46400/52929...\n",
      "      Processing sequence 46450/52929...\n",
      "      Processing sequence 46500/52929...\n",
      "      Processing sequence 46550/52929...\n",
      "      Processing sequence 46600/52929...\n",
      "      Processing sequence 46650/52929...\n",
      "      Processing sequence 46700/52929...\n",
      "      Processing sequence 46750/52929...\n",
      "      Processing sequence 46800/52929...\n",
      "      Processing sequence 46850/52929...\n",
      "      Processing sequence 46900/52929...\n",
      "      Processing sequence 46950/52929...\n",
      "      Processing sequence 47000/52929...\n",
      "      Processing sequence 47050/52929...\n",
      "      Processing sequence 47100/52929...\n",
      "      Processing sequence 47150/52929...\n",
      "      Processing sequence 47200/52929...\n",
      "      Processing sequence 47250/52929...\n",
      "      Processing sequence 47300/52929...\n",
      "      Processing sequence 47350/52929...\n",
      "      Processing sequence 47400/52929...\n",
      "      Processing sequence 47450/52929...\n",
      "      Processing sequence 47500/52929...\n",
      "      Processing sequence 47550/52929...\n",
      "      Processing sequence 47600/52929...\n",
      "      Processing sequence 47650/52929...\n",
      "      Processing sequence 47700/52929...\n",
      "      Processing sequence 47750/52929...\n",
      "      Processing sequence 47800/52929...\n",
      "      Processing sequence 47850/52929...\n",
      "      Processing sequence 47900/52929...\n",
      "      Processing sequence 47950/52929...\n",
      "      Processing sequence 48000/52929...\n",
      "      Processing sequence 48050/52929...\n",
      "      Processing sequence 48100/52929...\n",
      "      Processing sequence 48150/52929...\n",
      "      Processing sequence 48200/52929...\n",
      "      Processing sequence 48250/52929...\n",
      "      Processing sequence 48300/52929...\n",
      "      Processing sequence 48350/52929...\n",
      "      Processing sequence 48400/52929...\n",
      "      Processing sequence 48450/52929...\n",
      "      Processing sequence 48500/52929...\n",
      "      Processing sequence 48550/52929...\n",
      "      Processing sequence 48600/52929...\n",
      "      Processing sequence 48650/52929...\n",
      "      Processing sequence 48700/52929...\n",
      "      Processing sequence 48750/52929...\n",
      "      Processing sequence 48800/52929...\n",
      "      Processing sequence 48850/52929...\n",
      "      Processing sequence 48900/52929...\n",
      "      Processing sequence 48950/52929...\n",
      "      Processing sequence 49000/52929...\n",
      "      Processing sequence 49050/52929...\n",
      "      Processing sequence 49100/52929...\n",
      "      Processing sequence 49150/52929...\n",
      "      Processing sequence 49200/52929...\n",
      "      Processing sequence 49250/52929...\n",
      "      Processing sequence 49300/52929...\n",
      "      Processing sequence 49350/52929...\n",
      "      Processing sequence 49400/52929...\n",
      "      Processing sequence 49450/52929...\n",
      "      Processing sequence 49500/52929...\n",
      "      Processing sequence 49550/52929...\n",
      "      Processing sequence 49600/52929...\n",
      "      Processing sequence 49650/52929...\n",
      "      Processing sequence 49700/52929...\n",
      "      Processing sequence 49750/52929...\n",
      "      Processing sequence 49800/52929...\n",
      "      Processing sequence 49850/52929...\n",
      "      Processing sequence 49900/52929...\n",
      "      Processing sequence 49950/52929...\n",
      "      Processing sequence 50000/52929...\n",
      "      Processing sequence 50050/52929...\n",
      "      Processing sequence 50100/52929...\n",
      "      Processing sequence 50150/52929...\n",
      "      Processing sequence 50200/52929...\n",
      "      Processing sequence 50250/52929...\n",
      "      Processing sequence 50300/52929...\n",
      "      Processing sequence 50350/52929...\n",
      "      Processing sequence 50400/52929...\n",
      "      Processing sequence 50450/52929...\n",
      "      Processing sequence 50500/52929...\n",
      "      Processing sequence 50550/52929...\n",
      "      Processing sequence 50600/52929...\n",
      "      Processing sequence 50650/52929...\n",
      "      Processing sequence 50700/52929...\n",
      "      Processing sequence 50750/52929...\n",
      "      Processing sequence 50800/52929...\n",
      "      Processing sequence 50850/52929...\n",
      "      Processing sequence 50900/52929...\n",
      "      Processing sequence 50950/52929...\n",
      "      Processing sequence 51000/52929...\n",
      "      Processing sequence 51050/52929...\n",
      "      Processing sequence 51100/52929...\n",
      "      Processing sequence 51150/52929...\n",
      "      Processing sequence 51200/52929...\n",
      "      Processing sequence 51250/52929...\n",
      "      Processing sequence 51300/52929...\n",
      "      Processing sequence 51350/52929...\n",
      "      Processing sequence 51400/52929...\n",
      "      Processing sequence 51450/52929...\n",
      "      Processing sequence 51500/52929...\n",
      "      Processing sequence 51550/52929...\n",
      "      Processing sequence 51600/52929...\n",
      "      Processing sequence 51650/52929...\n",
      "      Processing sequence 51700/52929...\n",
      "      Processing sequence 51750/52929...\n",
      "      Processing sequence 51800/52929...\n",
      "      Processing sequence 51850/52929...\n",
      "      Processing sequence 51900/52929...\n",
      "      Processing sequence 51950/52929...\n",
      "      Processing sequence 52000/52929...\n",
      "      Processing sequence 52050/52929...\n",
      "      Processing sequence 52100/52929...\n",
      "      Processing sequence 52150/52929...\n",
      "      Processing sequence 52200/52929...\n",
      "      Processing sequence 52250/52929...\n",
      "      Processing sequence 52300/52929...\n",
      "      Processing sequence 52350/52929...\n",
      "      Processing sequence 52400/52929...\n",
      "      Processing sequence 52450/52929...\n",
      "      Processing sequence 52500/52929...\n",
      "      Processing sequence 52550/52929...\n",
      "      Processing sequence 52600/52929...\n",
      "      Processing sequence 52650/52929...\n",
      "      Processing sequence 52700/52929...\n",
      "      Processing sequence 52750/52929...\n",
      "      Processing sequence 52800/52929...\n",
      "      Processing sequence 52850/52929...\n",
      "      Processing sequence 52900/52929...\n",
      "[4/4] Computing statistics...\n",
      "      Total events: 904805\n",
      "      Avg per sequence: 17.1\n",
      "      By scale:\n",
      "        MICRO.......    4.0 per sequence\n",
      "        MINI........    8.8 per sequence\n",
      "        MESO........    2.8 per sequence\n",
      "        MACRO.......    1.6 per sequence\n",
      "        GLOBAL......    0.0 per sequence\n",
      "\n",
      "================================================================================\n",
      "✓ DATASET READY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Corpus statistics:\n",
      "  Documents: 52929\n",
      "  Example: <sequence length=128 events=9> [0-127] flat stable segment, [0-127] sideways consolidation regime, [0-23] low volatility period, [25-32] low volatilit...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: GENERATE EVENT CORPUS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: GENERATE EVENT CORPUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create hierarchical event dataset\n",
    "dataset = HierarchicalEventDataset(X_shuffled, verbose=True)\n",
    "\n",
    "# Generate text corpus\n",
    "generator = QwenCorpusGenerator(strategy='natural_flat')\n",
    "corpus = generator.generate_corpus(dataset, add_metadata=True)\n",
    "\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Documents: {len(corpus)}\")\n",
    "print(f\"  Example: {corpus[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d98f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: BUILD TOKENIZER\n",
      "================================================================================\n",
      "Fixed vocabulary initialized: 184 tokens\n",
      "  Special tokens: 6\n",
      "  Structural: 4\n",
      "  Positions (0-127): 128\n",
      "  Core event vocab: 42\n",
      "\n",
      "Building vocabulary from 52929 documents...\n",
      "  Found 1 new unique tokens\n",
      "  Final vocabulary size: 185\n",
      "    Fixed: 184\n",
      "    Variable: 1\n",
      "  Coverage: 100.00%\n",
      "\n",
      "Actual vocabulary size: 185\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: BUILD TOKENIZER AND DETERMINE VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: BUILD TOKENIZER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = EventTextTokenizer(\n",
    "    max_position=128,      # Support positions 0-511\n",
    "    max_vocab_size=3000    # Upper bound (won't reach this)\n",
    ")\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "tokenizer.build_vocab(corpus, min_freq=2)\n",
    "\n",
    "# Get actual vocabulary size\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print(f\"\\nActual vocabulary size: {actual_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2375347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: DECIDE MODEL VOCAB SIZE\n",
      "================================================================================\n",
      "\n",
      "Vocabulary size options:\n",
      "  Exact: 185\n",
      "  Power of 2: 256\n",
      "  Nearest 256: 256\n",
      "\n",
      "✓ Selected vocab_size: 256\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: DECIDE MODEL VOCAB SIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: DECIDE MODEL VOCAB SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Use exact size (most efficient)\n",
    "vocab_size_exact = actual_vocab_size\n",
    "\n",
    "# Option 2: Round up to power of 2 (hardware-friendly)\n",
    "import math\n",
    "vocab_size_pow2 = 2 ** math.ceil(math.log2(actual_vocab_size))\n",
    "\n",
    "# Option 3: Round up to nearest 256 (common practice)\n",
    "vocab_size_256 = ((actual_vocab_size + 255) // 256) * 256\n",
    "\n",
    "print(f\"\\nVocabulary size options:\")\n",
    "print(f\"  Exact: {vocab_size_exact}\")\n",
    "print(f\"  Power of 2: {vocab_size_pow2}\")\n",
    "print(f\"  Nearest 256: {vocab_size_256}\")\n",
    "\n",
    "# Choose option (I recommend power of 2)\n",
    "vocab_size = vocab_size_pow2\n",
    "\n",
    "print(f\"\\n✓ Selected vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59cd434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: CREATE MODEL\n",
      "================================================================================\n",
      "\n",
      "Model created:\n",
      "  Vocabulary size: 256\n",
      "  Model dimension: 256\n",
      "  Total parameters: 500,544\n",
      "\n",
      "Memory footprint:\n",
      "  Embedding layer: 0.25 MB\n",
      "  Total model: 1.91 MB\n",
      "  Embedding %: 13.1%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: CREATE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: CREATE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = TimeSeriesEventModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nModel created:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Model dimension: 256\")\n",
    "print(f\"  Total parameters: {model.count_parameters():,}\")\n",
    "\n",
    "# Calculate embedding layer size\n",
    "embedding_params = vocab_size * 256  # d_model\n",
    "embedding_memory = embedding_params * 4 / (1024**2)  # MB\n",
    "total_memory = model.count_parameters() * 4 / (1024**2)  # MB\n",
    "\n",
    "print(f\"\\nMemory footprint:\")\n",
    "print(f\"  Embedding layer: {embedding_memory:.2f} MB\")\n",
    "print(f\"  Total model: {total_memory:.2f} MB\")\n",
    "print(f\"  Embedding %: {embedding_memory/total_memory*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4fde613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: SAVE ARTIFACTS\n",
      "================================================================================\n",
      "Tokenizer saved to event_tokenizer.json\n",
      "✓ Tokenizer saved\n",
      "✓ Model config saved\n",
      "\n",
      "================================================================================\n",
      "✓ SETUP COMPLETE - READY FOR TRAINING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: SAVE TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: SAVE ARTIFACTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tokenizer.save('event_tokenizer.json')\n",
    "print(f\"✓ Tokenizer saved\")\n",
    "\n",
    "# Save model config for reproducibility\n",
    "import json\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'total_parameters': model.count_parameters()\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "print(f\"✓ Model config saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SETUP COMPLETE - READY FOR TRAINING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca13dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward pass successful!\n",
      "Input: src=torch.Size([4, 128]), tgt=torch.Size([4, 50])\n",
      "Output: logits=torch.Size([4, 50, 256])\n"
     ]
    }
   ],
   "source": [
    "# Quick test - just run forward pass\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 4\n",
    "src = torch.randn(batch_size, 128)  # Time series\n",
    "tgt = torch.randint(0, 128, (batch_size, 50))  # Text tokens\n",
    "\n",
    "# Forward pass\n",
    "logits = model(src, tgt)\n",
    "\n",
    "print(f\"✓ Forward pass successful!\")\n",
    "print(f\"Input: src={src.shape}, tgt={tgt.shape}\")\n",
    "print(f\"Output: logits={logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a61e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entrope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
